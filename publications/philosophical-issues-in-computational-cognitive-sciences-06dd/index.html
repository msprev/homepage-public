<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Philosophical issues in computational cognitive sciences | Mark Sprevak</title>
        <meta name="description" content="">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="stylesheet" href="https://marksprevak.com/kube/css/kube.min.css" />
<link rel="stylesheet" href="https://marksprevak.com/css-customisations/sprevak.css" />
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<title>Mark Sprevak</title>
<base href="https://marksprevak.com/">
<link rel="canonical" href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/">

<link href="https://fonts.googleapis.com/css?family=Roboto:400,700%7CLato:400,700" rel="stylesheet">

    </head>
    <body>
        <div class="page wrapper">

            <header class="header">
                <div class="is-navbar-container" style="padding-bottom: 6px; padding-top: 0px; margin-bottom: 12px; border-bottom: 1px solid; border-color: rgba(0, 0, 0, 0.3);">
    <div class="is-brand">
        <div class="titlebar"><a href="https://marksprevak.com/">Mark&nbsp;Sprevak</a></div>
        
        <a href="#"
                style="color: rgba(0, 0, 0, 0.8); text-decoration: none; border-bottom: none; font-size:18px;"
                class="is-hidden-print nav-toggle is-push-right-mobile is-shown-mobile icon-kube-menu"
                data-kube="toggle"
                data-target="#top-navbar"></a>
    </div>
    <div id="top-navbar" class="is-navbar is-hidden-print is-hidden-mobile">
        <nav class="is-push-right">
            <ul style="text-align: right;">
                
                
                
                
                
                <li  >
                    <a href="https://marksprevak.com/publications/" style="text-decoration: none; border-bottom: none;">Publications</a>
                </li>
                
                
                
                <li  >
                    <a href="https://marksprevak.com/talks/" style="text-decoration: none; border-bottom: none;">Talks</a>
                </li>
                
                
                
                <li  >
                    <a href="https://marksprevak.com/outreach/" style="text-decoration: none; border-bottom: none;">Outreach</a>
                </li>
                
                
                
                <li  >
                    <a href="https://marksprevak.com/cv/" style="text-decoration: none; border-bottom: none;">CV</a>
                </li>
                
                
                
                <li  >
                    <a href="https://marksprevak.com/phds/" style="text-decoration: none; border-bottom: none;">PhD study</a>
                </li>
                
                
                
                <li  >
                    <a href="https://marksprevak.com/mscs/" style="text-decoration: none; border-bottom: none;">MSc study</a>
                </li>
                
                
                
                <li  >
                    <a href="https://marksprevak.com/teaching/" style="text-decoration: none; border-bottom: none;">Teaching</a>
                </li>
                
                
            </ul>
        </nav>
    </div>
</div>

            </header>

            <main class="main">
                <div class="is-row">

                    <div class="is-col is-67">     

                        <div style="padding-bottom: 30px;">
                            <div style="margin-bottom: 10px;">
                                <h1 class="is-color-black" style="margin-top: 0px; margin-bottom: 0px;">Philosophical issues in computational cognitive sciences</h1>
                                
                                <p class="is-muted" style="margin-top: 10px;">
                                    
                                        under review  &nbsp;
                                    
                                    <em>Cambridge Handbook on Computational Psychology</em> (edited by Ron Sun), Cambridge University Press: Cambridge
                                </p>
                                <p class="is-small" style="margin-top: 10px;">
                                    <span>Last updated 19 May 2021</span>
                                    
                                </p>
                            </div>
                            <div class="is-hidden-print">
                                
<a href="https://marksprevak.com/pdf/paper/Sprevak--Philosophical-Issues-in-Computational-Cognitive-Sciences.pdf" target="_blank" class="label is-primary is-focus" style="margin-left: 0px; margin-right:5px;">
    <i class="far fa-file-pdf" style="font-size: 12px;"></i>
    &nbsp;PDF
</a>




                            </div>
                        </div>

                        <div class="is-hidden-mobile">
                            
                        </div>
                        <div class="is-shown-mobile">
                            
                            
                            <div class="is-muted is-smaller is-hidden-print">
                                Abstract:
                            </div>
                            <div class="article-style" style="margin-bottom: 30px;">
                                
                            </div>
                            
                        </div>

                        <div>
                            
                            <div class="is-shown-mobile">
                                
                                    <h1 style="margin-top: 0px;" id="internal-mds-toc">Contents</h1>
                                    <ul class="is-unstyled">
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#introduction"><span style="visibility: visible;">1</span> &nbsp;  Introduction</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#overview-of-chapter"><span style="visibility: visible;">1.1</span> &nbsp;  Overview of chapter</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#semantic-content-searles-chinese-room-argument"><span style="visibility: visible;">2</span> &nbsp;  Semantic content &#x2013; Searle&#x2019;s Chinese room argument</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-chinese-room-argument"><span style="visibility: visible;">2.1</span> &nbsp;  The Chinese room argument</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-problem-of-semantic-content"><span style="visibility: visible;">2.2</span> &nbsp;  The problem of semantic content</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#theories-of-content"><span style="visibility: visible;">2.3</span> &nbsp;  Theories of content</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#content-and-physical-computation"><span style="visibility: visible;">2.4</span> &nbsp;  Content and physical computation</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#consciousness-the-hard-problem"><span style="visibility: visible;">3</span> &nbsp;  Consciousness &#x2013; The hard problem</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-conceivability-argument-against-physicalism"><span style="visibility: visible;">3.1</span> &nbsp;  The conceivability argument against physicalism</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-conceivability-argument-against-computational-functionalism"><span style="visibility: visible;">3.2</span> &nbsp;  The conceivability argument against computational functionalism</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#naturalistic-dualism"><span style="visibility: visible;">3.3</span> &nbsp;  Naturalistic dualism</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#eliminativism-and-related-replies"><span style="visibility: visible;">3.4</span> &nbsp;  Eliminativism and related replies</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#central-reasoning-the-frame-problem"><span style="visibility: visible;">4</span> &nbsp;  Central reasoning &#x2013; The frame problem</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-frame-problem"><span style="visibility: visible;">4.1</span> &nbsp;  The frame problem</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#dreyfuss-argument"><span style="visibility: visible;">4.2</span> &nbsp;  Dreyfus&#x2019;s argument</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#fodors-argument"><span style="visibility: visible;">4.3</span> &nbsp;  Fodor&#x2019;s argument</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#responses-to-the-problem"><span style="visibility: visible;">4.4</span> &nbsp;  Responses to the problem</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#conclusion"><span style="visibility: visible;">5</span> &nbsp;  Conclusion</a>
            
        </span>
        
    </li>
    
</ul>

                                
                                
                            </div>
                            <div class="article-style">
                                <div>
<h1 data-number="1" id="introduction"><span class="header-section-number">1</span> Introduction</h1>
<p>In 1962, Wilfred Sellars wrote: ‘The aim of philosophy, abstractly formulated, is to understand how things in the broadest possible sense of the term hang together in the broadest possible sense of the term’ <span class="citation" data-cites="Sellars62">(Sellars 1962 p. 35)</span>. Sellars was concerned to explain the relationship between philosophy and the sciences. In my view, he got it exactly right. Philosophical issues are marked out not by having some uniquely philosophical subject matter, but in terms of the overall scope of the enquiry. When one turns to philosophical issues, what one is doing is taking a step back from some of the details and considering broadly how matters hang together relative to the ambitions and goals that motivated the scientific enquiry in the first place. In the case of the computational cognitive sciences, this may involve asking such questions as: Are there aspects of cognition or behaviour that are not amenable to computational modelling? How do distinct computational models of cognition and behaviour fit together to tell a coherent story about cognition and behaviour? What exactly does a specific computational model tell (or fail to tell) us about cognition and behaviour? What distinguishes computational models from alternative approaches to modelling cognition and behaviour? How does a computational model connect to, and help to answer, our pre-theoretical questions about what minds are and how they work?</p>
<p>Progress in answering these questions may come from any or all sides. It would be a mistake to think the philosophical issues are somehow only the domain of philosophers. Anyone who takes computational modelling seriously as an attempt to study cognition is likely to want to know the answer to these questions and is liable to be able to contribute to the project of answering them. What philosophers bring to the project is a set of conceptual tools and approaches that have been developed in other domains to address structurally similar issues. They also have the luxury of being allowed to think and write about the big questions.</p>
<p>Sellars had a relatively narrow conception of what it meant to understand how things hang together. He interpreted this as an attempt to reconcile two distinct images of the world: the <em>scientific image</em> (which describes relationships between the posits of the sciences – e.g. cells, molecules, atoms, forces) and the <em>manifest image</em> (which describes relationships between the posits of human common-sense understanding of the world – e.g. persons, thoughts, feelings, ideas) <span class="citation" data-cites="Sellars62">(Sellars 1962)</span>. This chapter adopts a somewhat looser and broader interpretation of the project. Models in the computational cognitive sciences are often partial, provisional, and one of many possible alternatives that consistent with the data. It would be misleading to assume that current computational cognitive science contains a single, coherent account or embodies ‘the’ scientific image of cognition. Similar concerns could be raised about our manifest image in light of work on cross-cultural variation in human folk understanding and conceptualisations of the world <span class="citation" data-cites="Nisbett03 HenrichHeineNorenzayan10 Barrett20">(Barrett 2020; Henrich et al. 2010; Nisbett 2003)</span>. The view adopted in this chapter is that one’s goal should be to understand how the many, diverse current approaches to computational modelling of cognition hang together, both with each other, with work in the other sciences (including neuroscience, cellular biology, evolutionary biology, and the social sciences), and with the various pre-theoretical folk questions and insights we have regarding the mind. Here, there is no prior commitment to a single, well-defined scientific image or manifest image, but rather the ambition to understand how the various perspectives we have on cognition and behaviour cohere and allow us to understand what minds are and how they work <span class="citation" data-cites="Sprevak16">(for more on this point, see Sprevak 2016)</span>.</p>
<h2 data-number="1.1" id="overview-of-chapter"><span class="header-section-number">1.1</span> Overview of chapter</h2>
<p>Researchers generally aim to build a computational model of some limited domain within cognition or behaviour (e.g. face recognition, cheater detection, word segmentation, or depth perception). Splitting up human cognition and behaviour into various smaller domains raises questions about <em>how</em> we should do it. In the language of philosophers, this is about how we should <em>individuate</em> our cognitive capacities and behaviour <span class="citation" data-cites="Machery18 BarrettKurzban06 Anderson14">(M. L. Anderson 2014; Barrett &amp; Kurzban 2006; Machery forthcoming)</span>. Modelling cognition and behaviour in this way also raises questions about how the models of individual capacities we hope to obtain will eventually be woven together to create a coherent, integrated model of cognition. This concerns the issue of how we <em>unify</em> models of distinct cognitive domains <span class="citation" data-cites="Danks14 Eliasmith13 ColomboHartmann17">(Colombo &amp; Hartmann 2017; Danks 2014; Eliasmith 2013)</span>.</p>
<p>This chapter focuses on a separate set of foundational issues that are related, but posterior to, the two just mentioned. These concern possible <em>gaps</em> left by this strategy for modelling cognition. If the strategy were completed, would there be any cognitive capacities that would be missing from the final picture? Are there any aspects of cognition for which we should expect <em>not</em> to obtain a computational model? Are there certain cognitive domains that are, for some reason, ‘no go’ areas for computational modelling? The chapter examines three possible candidates: <em>semantic content</em> (Section 2), <em>phenomenal consciousness</em> (Section 3), and <em>central reasoning</em> (Section 4). In each case, philosophers have argued that there are good reasons to believe that we cannot obtain an adequate computational model of the domain in question.</p>
<h1 data-number="2" id="semantic-content-searles-chinese-room-argument"><span class="header-section-number">2</span> Semantic content – Searle’s Chinese room argument</h1>
<p>John Searle’s Chinese room argument is one of the oldest and most notorious ‘no go’ arguments for computational modelling of cognition. The precise extent of its target has been liable to shift between different presentations of the argument. Searle has claimed in various contexts that the argument shows that <em>understanding</em>, <em>semantic content</em>, <em>intentionality</em>, and <em>consciousness</em> cannot adequately be captured by a computational model <span class="citation" data-cites="Searle92">(according to Searle, all these properties are linked, see Searle 1992 pp. 127–97)</span>. In his original formulation, Searle’s target was <em>understanding</em>, and specifically, our ability to understand simple stories. He considered whether a computational model would adequately be able to account for this cognitive capacity. More precisely, he considered whether such a model would be able to explain the difference between understanding and not understanding a simple story <span class="citation" data-cites="Searle80 SchankAbelson77 Winograd72">(Searle 1980; cf. models of understanding in Schank &amp; Abelson 1977; Winograd 1972)</span>.</p>
<h2 data-number="2.1" id="the-chinese-room-argument"><span class="header-section-number">2.1</span> The Chinese room argument</h2>
<p>Searle’s argument consisted in a thought experiment concerning implementation of the computation. Imagine a monolingual English speaker inside a room with a rule-book and sheets of paper. The rule-book contains instructions in English on what to do if presented with Chinese symbols. The instructions might take the form: ‘If you see Chinese symbol X on one sheet of paper and Chinese symbol Y on another, then write down Chinese symbol Z on a third sheet of paper’. Pieces of paper with Chinese writing are passed into the room and the person inside follows the rules and passes pieces of paper out. Chinese speakers outside the room label the sheets that are passed in ‘story’ and ‘questions’ respectively, and the sheets that come out ‘answers to questions’. Imagine that the rule-book is as sophisticated as you like, and certainly sophisticated enough that the responses that the person gives are indistinguishable from those of a native Chinese speaker. Does the person inside the room thereby understand Chinese? Searle claims that they do not <span class="citation" data-cites="Block80 Maudlin89 Wakefield03">(for discussion of the reliability of his intuition here, see Block 1980; Maudlin 1989; Wakefield 2003)</span>.</p>
<p>Searle observes that the Chinese room is a computer, and he identifies the rule-book with the (symbolic) computation that it performs. He then reminds us that the thought experiment does not depend on the particular rule-book used: it does not matter how sophisticated the rule-book, the person inside the room would still be shuffling Chinese symbols without understanding what they mean. Since any symbolic computational process can be described by some rule-book, the thought experiment shows that the person inside the Chinese room will not understand the meaning of the Chinese expressions they manipulate no matter which symbolic computation they perform. Therefore, we can conclude that the performance of a symbolic computation is insufficient, by itself, to account for the difference between the system performing the computation understanding and not understanding what the Chinese expressions mean. Searle infers from this that any attempt to model understanding purely in terms of a formal, symbolic computation is doomed to failure. According to Searle, the reason why is that a formal computational model cannot induce <em>semantic</em> properties, which are essential to accounting for a semantically laden cognitive process like understanding <span class="citation" data-cites="Searle80">(Searle 1980 p. 422)</span>.</p>
<h2 data-number="2.2" id="the-problem-of-semantic-content"><span class="header-section-number">2.2</span> The problem of semantic content</h2>
<p>Many objections have been raised to Searle’s Chinese room argument <span class="citation" data-cites="Cole20">(for a summary, see Cole 2020)</span>. However, it is notable that despite the argument’s many defects, the basic conclusion that Searle drew has been left largely unchallenged by subsequent attacks. This is that <em>manipulation of formal symbols</em> is insufficient to generate the semantic properties associated with cognitive processes like understanding. In Searle’s terms, the Chinese room thought experiment is an illustration of a general principle that ‘syntax is not sufficient for semantics’ <span class="citation" data-cites="Searle84b">(Searle 1984)</span>. Note that ‘syntax’ here does not refer to the static grammatical properties of symbols or well-formedness of linguistic expressions, but to the algorithmic rules by which symbolic expressions are manipulated or transformed during a computation. ‘Semantics’ refers specifically to the denotational aspects of the meaning associated with symbolic expressions – their intentional properties, or what they refer to in the world.</p>
<p>Searle is not alone in making this kind of claim. <span class="citation" data-cites="Putnam81">Putnam (1981)</span> argued that manipulating symbols (mere ‘syntactic play’) cannot determine what a computation’s symbols refer to, or whether they carry any referential semantic content at all (pp. 10–11). <span class="citation" data-cites="Burge86">Burge (1986)</span>, building on earlier work by Putnam and himself on referring terms in natural language, noted that a physical duplicate of a computer placed in a different environment may undergo exactly the same formal transitions, but have entirely different meaning attached to its symbolic expressions based on its causal relationship to different environmental properties. <span class="citation" data-cites="Fodor78">Fodor (1978)</span> described two physically identical devices that undergo the same symbol-manipulation procedures, one of which runs a simulation of the Six-Day War (with symbols referring to tank divisions, jet planes, and infantry units) and the other runs a simulation of a chess game (with symbols referring to knights, bishops, and pawns). <span class="citation" data-cites="Harnad90">Harnad (1990)</span> argued that all computational models based on symbol processing face a ‘symbol grounding’ problem: although some of their symbols may have their semantic content determined by their formal relationship to other symbols, that sort of process has to bottom out somewhere with symbolic expressions that have their meaning determined in some other way (e.g. by their non-formal relationship to external objects in the environment in perception or motor control).</p>
<p>These considerations are also not confined to symbolic computational models of cognition. Similar arguments could be made for computational models that are defined over numerical values or probabilities. Consider artificial neural network models. These models consist in collections of formal nodes and connections that chain together a long sequence of mathematical operations on numerical activation values or connection weights (adding, multiplying, thresholding values). What do these numerical activation values or connection weights mean? How do are they relate to distal properties and objects in the environment? As outside observers, we might <em>interpret</em> the numerical values inside an artificial neural network as referring to certain things (just as outside observers we might interpret certain symbolic expressions in a classical, symbolic computation as referring to certain things). Independent of our interpreting attitudes, however, the mathematical rules that define a connectionist model do not fix this semantic content. The rules associated with an artificial neural network describe how numerical values are transformed during a computation (during inference or learning), but they do not say what those numbers (either individually or taken in combination) represent in the world. The numerical rules that define an artificial neural network no more imbue it with semantic content than the symbolic rules that operate over expressions do for a classical, symbolic computation. Models that operate over probabilities or probability distributions face a similar kind of problem. These computational models are typically defined in terms of numerical operations on probability distributions (understood as ensembles of values that satisfy the requirements for a measure of probability). These distributions might be interpreted by us, as external observers, as probabilities of certain distal events occurring, but the mathematical rules governing the transformation of these distributions do not usually, by themselves, determine what those distal events are.</p>
<p>It is worth emphasising that there is no suggestion here that computation and semantic content are entirely independent factors in human cognition. Arguably, some symbolic expressions can get their meaning fixed through their formal computational role (most plausibly, this is the case for expressions that represent the logical connectives like AND and OR). However, even the most enthusiastic proponents of conceptual role semantics or procedural semantics do not, or at least do not normally, suggest that <em>all</em> semantic content is determined in this way. An adequate account of semantic content will need to include, not only formal relationships among computational states, but also non-formal relationships between those computational states and distal states in the external environment <span class="citation" data-cites="Block86 JohnsonLaird78 Harman87">(for discussion of this point, see Block 1986; Harman 1987; Johnson-Laird 1978)</span>.</p>
<h2 data-number="2.3" id="theories-of-content"><span class="header-section-number">2.3</span> Theories of content</h2>
<p>A lesson that philosophers have absorbed from this is that a computational model of cognition will need to be supplemented by another kind of model in order to adequately account for cognition’s semantic properties. Modelling cognition should therefore be seen as a project with at least two branches. One branch consists in describing the formal computational transitions or functions associated with a cognitive or behavioural capacity. The other branch connects the abstract symbols or numerical values described in the first branch to distal objects in the environment via semantic relations <span class="citation" data-cites="Chalmers93a">(see Chalmers 2012 pp. 334–5)</span>. This two-pronged approach is perhaps best represented by the research programme of Jerry Fodor. Fodor argued that one should sharply distinguish between one’s <em>computational theory</em> (which describes the dynamics of abstract computational vehicles in cognition) and one’s <em>theory of content</em> (which describes how those vehicles get associated with specific distal representational content). It would be a mistake to think that one’s computational theory can determine one’s semantic properties (or vice versa) <span class="citation" data-cites="Fodor98">(for a helpful summary of this research programme, see Fodor 1998 pp. 9–12)</span>. (Fodor makes exactly this point in response to the Chinese room argument, see Fodor <span class="citation" data-cites="Fodor80b">(1980)</span>).</p>
<p>What does a theory of content look like? Fodor argued that it should aim to answer two questions: (S1) How do computational states get their semantic properties? (S2) Which specific semantic content do they have? Fodor suggested any answers suitable to cognitive science should be <em>naturalistic</em>. By this, he meant that answers to questions S1 and S2 should appeal to properties and relations that are not themselves semantic or intentional. They should explain how semantic content arises, and how specific semantic contents get paired with computational states, in terms of the kinds of non-semantic properties that are typically invoked in the natural sciences (e.g. physical relations between the brain and its environment). A theory of content should not, for example, attempt to answer S1 or S2 by appeal to further semantic or mental properties, such as the interpreting attitudes of us as external observers or the intentional mental states of the subject themself <span class="citation" data-cites="Fodor90f Loewer17">(Fodor 1990 p. 32; Loewer 2017)</span>.</p>
<p>Fodor developed his own naturalistic theory of content, called the ‘asymmetric dependency theory’. This theory of content claimed that semantic content is determined by certain law-like relationships between symbols in the cognitive agent and current environmental stimuli <span class="citation" data-cites="Fodor90f">(Fodor 1990)</span>. Teleological theories of content attempt to naturalise content by appeal, not to conditions involving current environmental stimuli, but to conditions that were rewarded during past learning, or that were selected for in the cognitive agent’s evolutionary history <span class="citation" data-cites="Dretske95 Millikan04a Papineau87 Ryder04">(Dretske 1995; Millikan 2004; Papineau 1987; Ryder 2004)</span>. Use-based theories of content attempt to naturalise content by appeal to structural isomorphisms between multiple computational states in the cognitive agent and states of the world, claiming that their structural correspondence accounts for how the computational states represent <span class="citation" data-cites="Ramsey07 Shagrir12 Swoyer91">(Ramsey 2007; Shagrir 2012; Swoyer 1991)</span>. Information-theoretic theories of content attempt to naturalise content by appeal to Shannon information <span class="citation" data-cites="Dretske81">(Dretske 1981)</span>; recent variants propose that semantic content is determined by whatever distal state of affairs maximises a measure of mutual Shannon information between that distal content and the computational state <span class="citation" data-cites="Skyrms10 Isaac17 Usher01">(Isaac 2019; Skyrms 2010; Usher 2001)</span> – this echoes statistical methods used in cognitive neuroscience by external observers to assign content to neural responses in the sensory or motor systems <span class="citation" data-cites="Usher01 Eliasmith05 RollsTreves11">(Eliasmith 2005; Rolls &amp; Treves 2011; Usher 2001)</span>. <span class="citation" data-cites="Shea18">Shea (2018)</span> provides a powerful new naturalistic theory of content which weaves together elements of all the approaches above, arguing that semantic content is determined by different sorts of naturalistic conditions in different contexts.</p>
<p>Naturalising semantic content is an aspiration rather than a completed goal and serious difficulties confront all current approaches. Common challenges that face current theories include allowing for the possibility of misrepresentation; avoiding unacceptably large amounts of indeterminacy in the ascribed semantic content; and providing a sufficiently general account that covers not only representations involved in perception and motor control, but also non-sensorimotor representations like DEMOCRACY, TIMETABLE, or QUARK <span class="citation" data-cites="AdamsAizawa21 NeanderSchulte21 Shea13">(for an overview of the problems, see Adams &amp; Aizawa 2021; Neander &amp; Schulte 2021; Shea 2013)</span>.</p>
<p>Some philosophers have suggested that we adopt an alternative approach to accounting for semantic content. <span class="citation" data-cites="Egan14">Egan (2014)</span> argues that we should assume, as a working hypothesis, that the semantic content associated with cognition will never be naturalised. This is not because of any spookiness associated with semantic content, but because ascription of semantic content is an inherently messy matter that is influenced by endless, unsystematisable pragmatic concerns <span class="citation" data-cites="Egan03 Chomsky95a">(Chomsky 1995; Egan 2003)</span>. There is unlikely to be a natural science of semantic content determination for similar reasons as there is unlikely to be a natural science of rude jokes. Egan suggests that ascriptions of semantic content, although they fall outside the domain of natural science, nevertheless play an auxiliary role in scientific explanation by functioning as an ‘intentional gloss’ that relates formal computational models to our informal, non-scientific descriptions of behavioural success and failure.</p>
<p>A different approach suggests that assignments of semantic content should be treated as a special kind of scientific fiction or idealisation in computational cognitive science <span class="citation" data-cites="Mollo21 Sprevak13 Chirimuuta21">(Chirimuuta forthcoming; Mollo forthcoming; Sprevak 2013)</span>. This builds on recent philosophical work that emphasises the critical role of idealisations, fictions, and felicitous falsehoods in science, and that these features are often not undesirable and impossible to disentangle from literal truth telling <span class="citation" data-cites="Elgin17 Potochnik17 Morrison14">(Elgin 2017; Morrison 2014; Potochnik 2017)</span>.</p>
<p>Philosophers disagree on many points regarding how to model semantic content, but there is a broad consensus that a computational theory will need to be supplemented by something else – whether that be a naturalistic theory of content, an intentional gloss, or a reinterpretation of scientific practice – that accounts for how the states subject to computational rules gain their semantic content. This completes our description of one aspect of cognition for which a purely computational model is unlikely to be sufficient.</p>
<h2 data-number="2.4" id="content-and-physical-computation"><span class="header-section-number">2.4</span> Content and physical computation</h2>
<p>An important wrinkle has not yet been mentioned. The preceding discussion adopted the assumption that a computational model is defined exclusively in terms of formal rules (whether those be symbolic or numerical rules). This fits with how mathematicians and theoretical computer scientists talk about their computational models. In these contexts, a computational model is a purely abstract, mathematically definable entity (e.g. a set-theoretic entity). However, it does not reflect how many researchers in science and engineering talk about computational models. In these contexts, a computational model is often tied to its embodiment in a physical system. Part of one’s goal in proposing a computational model is to suggest that the formal transitions are implemented in some specific physical system. In the case of computational cognitive science, these formal transitions are normally assumed to be implemented, at some temporal scale, in the cognitive agent’s physical behaviour or neural responses.</p>
<p>When a formal computation is physically implemented, the physical states that are manipulated will stand in some non-formal relations to distal entities in the world. Physically implemented computations are inherently connected to the rest of the world: their computational states stand in law-like causal relations to objects in their environment, they have a history (and one that might involve past learning and evolution). Given this, it is by no means obvious that a physically implemented computation, unlike a purely formal abstract computation, is silent about, or does not determine, assignment of semantic content. Understanding whether and how physical computations and semantic content are related is a substantial question and one that is distinct from those considered above <span class="citation" data-cites="Dewhurst18 Lee18 Mollo18 Piccinini15 Rescorla13 Sprevak08a Shagrir18">(for various proposals about the relationship between physically implemented computation and semantic content, see Dewhurst 2018; Lee 2018; Mollo 2018; Piccinini 2015 pp. 26–50; Rescorla 2013; Shagrir 2018; Sprevak 2010)</span>. At the moment, there is no consensus about whether, and to what extent, physical implementation constrains the facts about a computation’s semantics. In light of this, Searle’s observation that ‘syntax is not sufficient for semantics’, even if valid for the formal cases he had in mind, may have questionable lasting significance for computational cognitive science <span class="citation" data-cites="Boden89 Chalmers96 Dennett87a">(Boden 1989; Chalmers 1996 pp. 326–7; Dennett 1987 pp. 323–6)</span></p>
<h1 data-number="3" id="consciousness-the-hard-problem"><span class="header-section-number">3</span> Consciousness – The hard problem</h1>
<p>‘Consciousness’ may refer to many different kinds of cognitive phenomena, including sleep and wakefulness, self-consciousness, reportability, information integration, and allocation of attention <span class="citation" data-cites="Gulick18">(see Gulick 2018 for a survey)</span>. This section focuses exclusively on ‘no go’ arguments concerning <em>phenomenal consciousness</em>. ‘Phenomenal consciousness’ refers to the subjective, qualitative feelings that accompany many aspects of cognition. When you touch a piece of silk, taste a raspberry, or hear the song of a blackbird, over and above various processes of classification, judgement, report, attentional shift, control of behaviour, and planning, you also undergo subjective sensations. There is something it subjectively <em>feels like</em> to do these things. Some philosophers reserve the term ‘qualia’ to refer to these qualitative feelings <span class="citation" data-cites="Tye18">(Tye 2018)</span>. The ‘hard problem’ of consciousness is to explain why these feelings accompany cognition and to account for their distribution across our cognitive life <span class="citation" data-cites="Chalmers96 Chalmers10b">(Chalmers 1996 pp. 3–1; 2010a)</span>.</p>
<h2 data-number="3.1" id="the-conceivability-argument-against-physicalism"><span class="header-section-number">3.1</span> The conceivability argument against physicalism</h2>
<p>The conceivability argument is a ‘no go’ argument against any physicalist theory of phenomenal consciousness. It is usually phrased in terms of the conceivability of zombies. A zombie is a hypothetical being who is a physical duplicate of a human and who lives in a universe that is a physical duplicate of our universe – a world with exactly the same physical laws and exactly the same distribution of physical properties. The only difference between our world and a zombie world is that the relevant entities in the zombie world either lack conscious experience or have a different distribution of phenomenal experiences across their cognitive life from our own. A zombie’s cognitive processing either occurs ‘in the dark’ or it is accompanied by different phenomenal experiences from our own (e.g. it might experience the qualitative feeling we associate with tasting raspberries when it sees tastes blueberries and vice versa).</p>
<p>It does not matter to the conceivability argument whether such a zombie could come into existence in our world, has ever existed, or is ever likely to exist. What matters for the argument is only whether one can rationally <em>conceive</em> of such a being. Can one imagine a physical duplicate who lacks phenomenal consciousness, or who has a different distribution of phenomenal experiences from our own? Many philosophers have argued that zombies of both kinds are easily conceivable <span class="citation" data-cites="Chalmers96 Kripke80 Nagel74">(Chalmers 1996 pp. 96–7; Kripke 1980 pp. 144–55; Nagel 1974)</span>. By this, they don’t meant that zombies could exist in our own world, or that we should entertain doubts about whether other humans are zombies. Indeed, for many dualists it is impossible for a zombie to exist in our world. What they mean is that the <em>idea</em> of a zombie is a coherent one – it does not contain a contradiction, like the idea of a married bachelor or the highest prime number.</p>
<p>The next step in the argument relies on the assumption that rational conceivability is a reliable guide to possibility. If a world with zombies is conceivable, then we should believe, barring evidence to the contrary, that it corresponds to a genuine possibility. But if a zombie world is possible, then the physical laws and physical states could be the same as they are in our world and the relevant beings in that world either lack phenomenal experience or have different phenomenal experiences. That means that in our <em>actual</em> world, there must be some further ingredient, over and above the physical facts, that is responsible for our phenomenal experience. Something beyond the physical facts must account for the difference between our world and a zombie world. The existence of phenomenal consciousness and its distribution across our cognitive life cannot rest on the physical facts alone, because those facts could have been the same and the phenomenal experiences vary. Advocates of the conceivability argument conclude that a purely physicalist theory is unable to answer the hard problem of consciousness <span class="citation" data-cites="Chalmers96 Chalmers10a">(Chalmers 1996 pp. 93–171; 2010b)</span>. (This conclusion is shared by <span class="citation" data-cites="Jackson82">Jackson (1982)</span>’s knowledge argument, based around a thought experiment with Mary, a neuroscientist who has never seen colour; the reasoning behind the knowledge argument is closely related to that of the conceivability argument; for the connection, see <span class="citation" data-cites="Chalmers10a">Chalmers (2010b)</span>, pp. 192–196).</p>
<p>According to the conceivability argument, no physicalist theory can answer two important questions about phenomenal consciousness: (C1) How does phenomenal conscious experience arise at all? (C2) Why are our phenomenal conscious experiences distributed in the way that they are across our cognitive life? No matter which physical facts one cites about the brain or environment in response to these questions, none (either singly or jointly) entail that conscious experience occurs – for it is possible that the same physical facts could have obtained and those conscious experiences been absent or different (as they are in a zombie world). This raises the question of what, over and above the physical facts, is responsible for the existence and distribution of our phenomenal experiences. Advocates of the conceivability argument propose various remedies at this point, all of which involve expanding or revising our current physicalist scientific ontology. Our focus will not be on those remedies, but only on the negative point that phenomenal consciousness is a ‘no go’ area for physicalist theories <span class="citation" data-cites="Chalmers10c">(see Chalmers 2010c pp. 126–37, for a survey of non-physicalist proposals)</span>.</p>
<h2 data-number="3.2" id="the-conceivability-argument-against-computational-functionalism"><span class="header-section-number">3.2</span> The conceivability argument against computational functionalism</h2>
<p>The conceivability argument against physicalism has been adapted to create a ‘no go’ argument against computational theories of phenomenal consciousness.</p>
<p>The relevant consideration here that a zombie who is our <em>computational</em> duplicate is conceivable – an entity who performs the same computations as we do but who either lacks conscious experience entirely, or who has a different distribution of conscious experiences. Similar motivations apply as in the case of the original conceivability argument against physicalism. One might imagine a system implementing any computational process, or computing any function, but for this to fail to be accompanied by a phenomenal experience, or for it to be accompanied by a phenomenal experience different to our own. No matter how complicated the computation, nothing about performing a computation seems to <em>entail</em> anything about the existence or distribution of subjective experiences. One might imagine an exact computational duplicate of a human – a system who undergoes the same computational transitions – but whose cognitive life remains ‘all dark’ inside, or who has different subjective experiences <span class="citation" data-cites="Block78 Maudlin89 Dennett78d">(for detailed examples of such thought experiments, see Block 1978; Dennett 1978; Maudlin 1989)</span>. In this respect, phenomenal consciousness appears different from other cognitive capacities. Cognitive capacities like classification, planning, and motor control, which do lend themselves to computational modelling, are defined by their <em>function</em> – by what they do, and how they contribute to behaviour. Phenomenal consciousness is not defined by its function, but by its <em>experiential feel</em> <span class="citation" data-cites="Chalmers10b">(Chalmers 2010a pp. 6–9)</span>. As with the original argument, it does not matter whether a computational zombie could exist in our world. What matters is only whether this type of zombie is rationally conceivable.</p>
<p>A separate consideration is that the original conceivability argument against physicalism entails a ‘no go’ claim concerning any attempt to solve the hard problem by appeal to physical computations <span class="citation" data-cites="Chalmers96">(Chalmers 1996 p. 95)</span>. Plausibly, any world that is a physical duplicate is a world that is a duplicate in terms of the physical computations that are performed. The physical facts about a world – e.g. about brains, their environment, and the laws that relate them – should be sufficient to determine which physical computations are implemented. If one accepts the conceivability argument against physicalism, then it is possible for a physical duplicate of our own world to exist in which phenomenal consciousness is absent or distributed differently. Therefore, it is possible for a world that is a duplicate in terms of the physical computations performed to exist in which phenomenal consciousness is absent or differently distributed. Hence, in our actual world there must be some additional ingredient, above and beyond the physically implemented computations, that accounts for phenomenal consciousness. No theory that appeals to physical computations alone can be sufficient to explain existence and distribution of our phenomenal experiences.</p>
<h2 data-number="3.3" id="naturalistic-dualism"><span class="header-section-number">3.3</span> Naturalistic dualism</h2>
<p>Advocates of the conceivability argument are careful about the scope of their ‘no go’ claim. What is claimed is that <em>solving the hard problem</em> by appeal to physical or computational facts is not possible. No physical or computational theory can answer C1 or C2. This does not mean, however, that a physical or computational theory cannot answer other important questions about phenomenal consciousness.</p>
<p>Chalmers <span class="citation" data-cites="Chalmers10b Chalmers10d">(2010a, 2010d)</span> suggests that a computational or physical theory can tell us a great deal about <em>correlations</em> between the physical/computational facts and phenomenal facts. The conceivability argument does not deny that such correlations exist or that they hold reliably. The important point, however, is that any information that a physical/computational model of phenomenal consciousness provides is at best correlational. Such a model cannot solve the hard problem of consciousness, because it is possible for a world to be physically identical and for those correlations to fail to obtain.</p>
<p>An analogy might help to clarify this point. Suppose that one were to engage in a study of the phenomena of lightning and thunder that is purely correlational. One might build a computational model of the phenomena that describes instances of each and correlations between them. In a similar fashion, one might embark on a study of physical states and phenomenally conscious states and attempt to describe the correlations between them. In both cases, what would be missing is a full understanding of how the relevant variables are linked. Lightning regularly co-occurs with thunder, but no pattern of lightning occurrences entails an occurrence of thunder, and vice versa. In the case of lightning and thunder, this deficiency in a purely correlational model can be remedied by introducing extra physical variables – e.g. distributions of electrical charges in the air, ground potential, measurements of air density. In a suitably enlarged model – one that contains more physical variables and charts the relationships between them – it would be possible to see why the observed correlations between lightning and thunder obtain and exactly how and why they fail – i.e. how changes to one physical variable <em>necessitate</em> changes to the other. In the case of phenomenal consciousness, the conceivability argument aims to show that this is not possible. One must go entirely <em>outside</em> the realm of physical variables to account for the correlation between physical states and phenomenal states. Filling in the gap between brain processes and phenomenal experience cannot be done by introducing extra physical variables into one’s model (or by introducing more complex physical relationships between variables). No matter how many physical variables one introduces, none necessitates phenomenal experiences – for all those physical variables could be the same and the consciousness experience absent or different. A physical/computational model of consciousness can only provide us with the common physical/computational correlates of phenomenal conscious, not an explanation of how or why that experience occurs.</p>
<h2 data-number="3.4" id="eliminativism-and-related-replies"><span class="header-section-number">3.4</span> Eliminativism and related replies</h2>
<p>Not all philosophers accept the reasoning behind the conceivability argument. Dennett argues that human conceptual and imaginative capacities have not evolved to draw reliable conclusions about hypothetical zombies. For all we know, zombie thought experiments work on our imaginations a little like viewing an M.C. Escher drawing: we appear to see something remarkable, but only because we have failed to spot some contradiction hidden in the picture. Dennett suggests that the correct inference to make is not that we have established a hard a priori limit to what a physical/computational model of phenomenal consciousness can achieve, but that the entire project of trying to set a hard a priori limit on what a physical/computational model can achieve is deeply misconceived <span class="citation" data-cites="Dennett13">(Dennett 2013)</span>. It could be that a truly thorough, mature conceptualisation of a physical and computational duplicate of ourselves, imagined down to the smallest detail, would rule out the possibility that it could be a zombie <span class="citation" data-cites="Dennett01c Dennett95m">(Dennett 1995, 2001)</span>.</p>
<p>Dennett’s doubts about the reliability of our intuitions in zombie thought experiments may temper enthusiasm for the proposition that zombies are conceivable, but this by itself does not rescue physicalism. In order to do this, Dennett also commits to the a speculative, positive claim that <em>if</em> we were to successfully wrap our heads around the right computational (and/or physical) relationships that correlate with consciousness, then we would see that they <em>must</em> bring all aspects of consciousness along with them. Advocates of the conceivability argument, while typically open to the idea that zombie intuitions are defeasible (we might be deluding ourselves about the conceivability of a zombie), tend to pour scorn on this latter contention. No matter how complex a computational model is they say, it simply isn’t clear how its operation could necessitate conscious experiences <span class="citation" data-cites="Strawson94">(Strawson 2010)</span>. The idea that somewhere out there, in the landscape of all possible computational models, a series of operations exists that magically necessitates conscious experience is pure moonshine or physicalist dogma <span class="citation" data-cites="Strawson18">(Strawson 2018)</span>.</p>
<p>A position one might be driven towards, and which Dennett defends in other work, is that certain aspects of consciousness – the irreducibly private, first-person aspects targeted by zombie thought experiments – are not real. This amounts to a form of eliminativism about phenomenal consciousness <span class="citation" data-cites="IrvineSprevak20">(Irvine &amp; Sprevak 2020)</span>. Such positions face a heavy intuitive burden. Our subjective feelings are among the things we are most certain exist. Denying their reality may strike one as unacceptable. Nevertheless, the predictive and explanatory benefits offered by past scientific theories have prompted us to abandon other seemingly secure assumptions about the world. If it can be shown that we are somehow labouring under an illusion concerning phenomenal experience, then many of the difficulties posed by the conceivability argument to computational modelling would evaporate. If there were no such thing as phenomenal consciousness, then there would be nothing for a computational model to explain.</p>
<p>However, in addition the intuitive burden just mentioned, a further difficulty faces eliminativist approaches. This is to explain how the claimed illusion associated with phenomenal consciousness works. This is called the ‘illusion problem’ <span class="citation" data-cites="Frankish16a">(Frankish 2016)</span>. Some eliminativists claim that the illusion problem can be solved by appeal to some physical/computational story about the mechanisms of our internal information processing and self report <span class="citation" data-cites="Frankish16a Clark00 Dennett91 Graziano16">(Clark 2000; Dennett 1991; Frankish 2016; Graziano 2016)</span>. However, while such an account might be able to explain why we <em>believe</em> or <em>act</em> as if we had phenomenal consciousness, it is not clear how it can generate the felt first-person illusion of consciousness <span class="citation" data-cites="Chalmers96">(Chalmers 1996 pp. 184–91)</span>. In general, it is not clear how any physical or computational process can generate the felt first-person illusion of conscious experience, any more than it can generate the felt first-person reality of conscious experience. The illusion problem may in the end be no easier for a physicalist/computational theory to solve than the original hard problem of consciousness <span class="citation" data-cites="Prinz16">(Prinz 2016)</span>.</p>
<h1 data-number="4" id="central-reasoning-the-frame-problem"><span class="header-section-number">4</span> Central reasoning – The frame problem</h1>
<p>A third major target for philosophical ‘no go’ arguments is <em>central reasoning</em>. This concerns our ability to engage in reliable, general-purpose reasoning on a large and open-ended set of representations, including our common-sense understanding of the world. Modelling central reasoning with computation is closely tied to the problem of building a form of general artificial intelligence (AGI). Our current AI systems function effectively only within carefully constrained problem domains (e.g. detecting credit-card fraud, recognising faces, winning at Go). They perform poorly, or not at all, if the nature of their problem changes, or if contextual or background assumptions change <span class="citation" data-cites="MarcusDavis19 LakeUllman17">(Lake et al. 2017; Marcus &amp; Davis 2019)</span>. In contrast, humans are relatively robust and flexible general-purpose problem solvers. They can rapidly switch between different kinds of task without significant interference or relearning, import relevant information across tasks, and they are aware of how their reasoning should be adapted as background assumptions and context change. What is more, they do this in a way that is informed by a vast database of common-sense knowledge about how the physical and social world works.</p>
<p>Small fragments of human central reasoning have been computationally modelled using various logics, heuristics, and other formalisms <span class="citation" data-cites="McCarthy90 NewellSimon72 DavisMorgenstern04 Anderson07 GigerenzerToddABC99">(J. R. Anderson 2007; Davis &amp; Morgenstern 2004; Gigerenzer et al. 1999; e.g. McCarthy 1990; Newell &amp; Simon 1972)</span>. However, modelling human general-purpose reasoning in full – and in particular, accounting for its flexibility, reliability, and common-sense knowledge base – remains an unsolved problem. This is evinced by our failure to build a computational model that exhibits anything like the levels of reliability, flexibility, and context-sensitivity demonstrated by humans. Philosophers have attempted to show that this lacuna in computational modelling is no accident, but arises because central reasoning is a ‘no go’ area for computational approaches to cognition.</p>
<h2 data-number="4.1" id="the-frame-problem"><span class="header-section-number">4.1</span> The frame problem</h2>
<p>‘No go’ arguments about central reasoning are often treated as examples of the frame problem in AI. This is misleading, as in AI the frame problem refers to a more narrowly defined problem specific to logic-based approaches. The frame problem is about how a logic-based reasoning system should represent the effects of an action without having to explicitly represent an action’s many non-effects <span class="citation" data-cites="McCarthyHayes69">(McCarthy &amp; Hayes 1969)</span>. Most actions leave most properties unchanged – eating a sandwich does not (normally) change the location of Antarctica. However, that the action <em>Eat(Sandwich)</em> does not change the property <em>Position(Antarctica)</em> is not a logical truth, but something that needs to be somehow encoded in the system’s knowledge base. Introducing this kind of information in the form of extra axioms that enumerate the non-effects of every action – ‘frame axioms’ – is unworkable as a general solution. As the number of actions and properties increases, the system would suffer an explosion in the number of frame axioms. The frame problem is how to encode this ‘no change’ information in a more efficient way. It is normally interpreted as the problem of formalising the rule that an action does not change a property unless there is evidence to the contrary. Formalising this principle poses numerous technical hurdles, and it has stimulated important developments in non-monotonic logics, but it is widely regarded as a solved issue for logic-based approaches <span class="citation" data-cites="Shanahan97 Lifschitz15 Shanahan16">(Lifschitz 2015; Shanahan 1997, 2016)</span>.</p>
<p>A number of philosophers, inspired by the frame problem, have argued that there are broader and more fundamental difficulties with modelling central reasoning with computation. They do not always agree about the precise nature of these problems, or about their exact scope or severity. A number of these problems – confusingly also labelled the ‘frame problem’ – can be found in the essays inside <span class="citation" data-cites="Pylyshyn87">Pylyshyn (1987)</span> and <span class="citation" data-cites="FordPylyshyn96">Ford &amp; Pylyshyn (1996)</span>, useful critical reflections on which are provided by <span class="citation" data-cites="Chow13">Chow (2013)</span>, <span class="citation" data-cites="Samuels10">Samuels (2010)</span>, <span class="citation" data-cites="Shanahan16">Shanahan (2016)</span>, and <span class="citation" data-cites="Wheeler08a">Wheeler (2008)</span>. The next two sections describe attempts by philosophers to identify these ‘no go’ barriers to computational accounts of human central reasoning.</p>
<h2 data-number="4.2" id="dreyfuss-argument"><span class="header-section-number">4.2</span> Dreyfus’s argument</h2>
<p>The first comes from Hubert Dreyfus <span class="citation" data-cites="Dreyfus72 Dreyfus92">(1972, 1992)</span>. Dreyfus initially focused on classical, symbolic computational approaches to central reasoning. The kind of model he had in mind is exemplified by Douglas Lenat’s Cyc project. This aimed to encode the entirety of human common-sense knowledge in a database of explicit symbolic representations over which a logic-based inference system could run queries <span class="citation" data-cites="LenatFeigenbaum91">(Lenat &amp; Feigenbaum 1991)</span>. Dreyfus argued that any such project for modelling human central reasoning faced two insuperable problems.</p>
<p>First, the goal of encoding all human common-sense knowledge in symbolic form was unattainable. Drawing on the work of Heidegger, Merleau-Ponty, and the later Wittgenstein, Dreyfus argued that any attempt to formalise common sense with symbolic representations will fail to capture a background of implicit assumptions, significances, and skills that are required in order for that formalisation to be used effectively. There is no way to make our common-sense knowledge explicit and symbolically encoded without presupposing a rich, implicit background of know-how. Fragments of common-sense reasoning can be formalised, but attempts to formalise it all will leave gaps, and attempts to fill those gaps will introduce gaps elsewhere. According to Dreyfus, the goal of formalising common-sense reasoning will run into the same problems that caused Husserl’s twentieth-century phenomenological attempt to describe all the principles and beliefs that underlie human intelligent behaviour to fail <span class="citation" data-cites="DreyfusDreyfus88 Dreyfus91">(Dreyfus 1991; Dreyfus &amp; Dreyfus 1988)</span>. (Searle makes a similar point about what he calls the ‘Background’ in <span class="citation" data-cites="Searle92">Searle (1992)</span>, pp. 175–196.)</p>
<p>Second, even if human common-sense knowledge could be formalised in symbolic rules, another problem arises concerning how a system would be able to use that information. Potentially, any piece of information could be relevant to any problem – there is no way to screen off, a priori, any piece of knowledge as irrelevant. However, given the size of the common-sense knowledge base, the system cannot consider every piece of information it has in turn and explore all their potential implications. How then does it select which representations are relevant to its current problem? In order to do this, the system needs to know a considerable amount about the nature of the current problem – about its current context and which background assumptions it is safe to make. How does it know this? Unless the external designers cheat and tell it the answer, the only way seems to be to deploy its vast database of common-sense knowledge to determine the type of situation it is in. But that leads back to the original problem of how it uses that information and how it selects which pieces of information are relevant. In order to deploy its vast database of knowledge, the system has to know which pieces of knowledge are relevant to the current context; in order to know this, it has to know what the current context is; but in order to know that, it needs to be able to deploy its knowledge effectively, which it can’t do because it doesn’t know which pieces of knowledge are relevant. Dreyfus concludes that any computational model that attempts to perform flexible, context-sensitive central reasoning will be trapped in an endless loop of attempting to determine context and relevance <span class="citation" data-cites="Dreyfus92">(Dreyfus 1992 pp. 206–24)</span>.</p>
<p>Dreyfus claimed that these two problems affect any attempt to account for central reasoning with a classical, symbolic computational model. In later work, Dreyfus attempted to extend his argument to other kinds of computational model. He considered connectionist networks trained under supervised learning and reinforcement learning. He cautiously concluded that although these approaches might get around the first problem (because they are not committed to formalising knowledge with symbolic representations), they are still plagued by something akin to the second problem. Current machine learning regimes tend to tune models to relatively specific, narrow problems domains and after training networks have not (yet) shown the flexibility to reproduce general-purpose central reasoning <span class="citation" data-cites="Dreyfus92 Dreyfus07">(Dreyfus 1992 pp. xxxiii–xliii; 2007)</span>. It is worth noting that the character of Dreyfus’s argument changes here from that of an unqualified ‘no go’ claim (it is <em>impossible</em> for a computational model to account for central reasoning), to a more nuanced prediction based on what has been achieved by computational models to date (we do not know how to train a connectionist network to be flexible enough to provide a model of central reasoning).</p>
<p>Dreyfus proposed that central reasoning should be modelled with a dynamical, embodied approach that falls under the heading of ‘Heideggerian AI’. The details of such a view are unclear, but broadly speaking the idea is that our inferential skills and embodied knowledge are coordinated and arranged such that they are solicited by the external situation and current context to bring certain knowledge to the fore. The resources needed to determine relevance and context therefore do not lie in a computation inside our heads, but are somehow encoded in the dynamical relationship between ourselves and the external world <span class="citation" data-cites="Haugeland98">(Haugeland 1998)</span>. Wheeler <span class="citation" data-cites="Wheeler05 Wheeler08a">(2005, 2008)</span> develops a version of Heideggerian AI that takes inspiration from the situated robotics movement <span class="citation" data-cites="Brooks91">(Brooks 1991)</span>. <span class="citation" data-cites="Dreyfus07">Dreyfus (2007)</span> argues instead for an approach based around the neurodynamics work of <span class="citation" data-cites="Freeman00">Freeman (2000)</span>. Neither has yet produced a working model that performs appreciably better than conventional computational alternatives.</p>
<h2 data-number="4.3" id="fodors-argument"><span class="header-section-number">4.3</span> Fodor’s argument</h2>
<p>According to Jerry Fodor, two problems conspire to prevent a computational model being able to account for central reasoning: the ‘globality’ problem and the ‘relevance’ problem <span class="citation" data-cites="Fodor83 Fodor00 Fodor08">(Fodor 1983, 2000, 2008)</span>. Like Dreyfus, Fodor focused primarily on how these problems affect classical, symbolic models. Fodor thought that non-symbolic (e.g. connectionist) models faced other problems related to their ability to reproduce the systematicity and compositionality of human thought that render them unsuitable as models of central reasoning <span class="citation" data-cites="FodorLepore92 FodorPylyshyn88 Fodor08">(Fodor 2008; Fodor &amp; Lepore 1992; Fodor &amp; Pylyshyn 1988)</span>. (For a review of other connectionist approaches to central reasoning, see <span class="citation" data-cites="Samuels10">Samuels (2010)</span>, pp. 289–290.)</p>
<p>The globality problem concerns how a system computes certain properties that are relevant to central reasoning: simplicity, centrality, and conservativeness of beliefs. Fodor suggested that these epistemic properties are ‘global’, by which he meant that they may depend on any number of the system’s beliefs; they are not features that supervene exclusively on local properties of the individual belief of which they are predicated. A belief might count as simple in one context (i.e. relative to one set of surrounding beliefs), but complex in another. The simplicity of a belief is not an intrinsic property of that belief. Therefore, simplicity cannot depend solely on a belief’s intrinsic computational properties. Fodor suggests that classical computational processes are sensitive <em>only</em> to intrinsic, local computational properties of the representations they manipulate. Therefore, any reasoning that requires sensitivity to global properties cannot be a classical computational process.</p>
<p>Fodor’s globality argument has been roundly criticised <span class="citation" data-cites="LudwigSchneider08 Schneider11 Samuels10">(Ludwig &amp; Schneider 2008; Samuels 2010; Schneider 2011)</span>. Critics point out that computations may be sensitive, not only to the local properties of individual representations, but also to <em>relations</em> between representations: how a belief’s local computational properties relate to the properties of other representations and how these relate to the system’s general rules of syntactic processing. The failure of a global property, such as simplicity, to supervene on a belief’s local computational properties has no bearing on whether simplicity can be tracked by a computational process. Simplicity may supervene on, and be reliably computationally tracked by following, syntactic relationships between multiple representations. Fodor anticipates this response, however – in <span class="citation" data-cites="Fodor00">Fodor (2000)</span> he labels it M(CTM). He argues that solving the globality problem in this way runs into his second problem.</p>
<p>The second problem – the relevance problem – arises when a reasoning system needs to make an inference based on a large number of beliefs, any number of which may be relevant to the problem at hand. Typically, only a few of these beliefs will be relevant – for example, to computation of a global property like simplicity. Only a fraction of the system’s representations need to be considered. The relevance problem is to determine the membership of this fraction. Humans tend to employ representations that are relevant to their current inference or planning task – they hone in on contextually appropriate beliefs and goals. How does their cognitive system know which representations are relevant without doing an exhaustive search through all its beliefs and goals to check each and their consequences for relevance? How does it determine which representations to consider when computing a global property like simplicity? Echoing the worries raised by Dreyfus, Fodor says we do not know of a computational method to solve this problem – one that is able to pare down the set of all the system’s representations to a subset relevant to the current task, when that task may change rapidly and about which no a priori assumptions can be made. This <em>relevance</em> problem lies at the heart of both Fodor’s and Dreyfus’s objections to computational accounts of central reasoning <span class="citation" data-cites="Chow13">(Chow 2013 pp. 312–3)</span>.</p>
<h2 data-number="4.4" id="responses-to-the-problem"><span class="header-section-number">4.4</span> Responses to the problem</h2>
<p>Many philosophers have suggested that humans solve the relevance problem using some set of heuristics. They point to heuristic methods used by Internet search engines, which approximately determine relevance, and the fact that humans often fail to spot relevant information or deploy irrelevant information when they engage in general-purpose reasoning <span class="citation" data-cites="Clark02 Lormand90 Carruthers06 Samuels05 Samuels10">(Carruthers 2006; Clark 2002; Lormand 1990; Samuels 2005, 2010)</span>. While considerations like these might increase our confidence that certain aspects of the relevance problem can be solved by computational means, they do not cut much ice unless one can say in detail how the observed success rate of humans is produced. Heuristics might at some level inform human central reasoning, but unless one can say which heuristics are used and how these produce the impressive flexibility and reliability seen in human reasoning, it is hard to say that one has solved the relevance problem <span class="citation" data-cites="Chow13">(see Chow 2013 pp. 315–21)</span>.</p>
<p><span class="citation" data-cites="ShanahanBaars05">Shanahan &amp; Baars (2005)</span> and <span class="citation" data-cites="Schneider11">Schneider (2011)</span> suggest that the relevance problem is solved by the Global Workspace Theory (GWT) of consciousness. GWT is a proposed large-scale architecture for human cognition in which multiple ‘specialist’ cognitive processes compete for access to a global workspace where central reasoning takes place. Access to the global workspace is controlled by ‘attention-like’ processes <span class="citation" data-cites="Baars88">(Baars 1988)</span>. <span class="citation" data-cites="MashourRoelfsemaChangeux20">Mashour et al. (2020)</span> and <span class="citation" data-cites="DehaeneChangeux04">Dehaene &amp; Changeux (2004)</span> describe a possible neural basis for GWT. <span class="citation" data-cites="GoyalDidolkarLamb21">Goyal et al. (2021)</span> suggest GWT as a way to enable special-purpose AI systems to share information and coordinate actions. GWT is a promising architecture, but it does not answer the worries raised by Dreyfus and Fodor. The model does not explain the mechanism by which information from specialists is regulated to guarantee relevance to the context and the contents of the central workspace. <span class="citation" data-cites="BaarsFranklin03">Baars &amp; Franklin (2003)</span> suggest there is an interplay between ‘executive functions’, ‘specialist networks’, and ‘attention codelets’ that control access to the global workspace, but exactly how these components function to track relevance is left unclear. As with the suggestions about heuristics, GWT is not (or not yet) a worked-out solution to the relevance problem <span class="citation" data-cites="Sprevak18a">(see Sprevak 2019 pp. 557–8)</span>.</p>
<p>A notable feature of current philosophical ‘no go’ arguments about central reasoning is that, unlike the arguments of Sections 2 and 3, they do not directly apply to all computational models. Both Dreyfus’s and Fodor’s arguments consist in pointing out problems with <em>past</em> and <em>current</em> computational approaches to central reasoning. The persuasive force of this against untried future computational approaches is questionable. Sceptics might see the consideration they raise as evidence that central reasoning is likely to never yield to a computational approach – Dreyfus and Fodor suggest that, pending evidence to the contrary, this is the rational conclusion to draw. Fans of computational modelling might respond that no one thought that modelling central reasoning would be anything other than a hard research problem; it is not surprising that it has not been solved yet, and the landscape of untried computational models is very large <span class="citation" data-cites="Samuels10">(Samuels 2010 pp. 288–92)</span>.</p>
<h1 data-number="5" id="conclusion"><span class="header-section-number">5</span> Conclusion</h1>
<p>This chapter describes a small sample of philosophical issues that have received attention in the computational cognitive sciences. Its focus has been on ‘no go’ arguments regarding three aspects of human cognition: semantic content, phenomenal consciousness, and central reasoning. A ‘no go’ argument aims to identify a possible or expected gap in the model of cognition we hope to eventually obtain from the computational cognitive sciences. One might worry that predicting such gaps now is rash or impractical given the relatively early state of development of the cognitive sciences. This is not the case. The project bears directly on questions about the estimated plausibility of proposed computational approaches, the motivations for pursuing them, and the rationale for investing in future research on computational or non-computational alternatives. Such judgements cannot be avoided and are made not uncommonly on the basis of hunches about the likely future success of research programmes. Philosophical work in this area can help to systematise evidence and enable decision makers give a reason-based appraisal of what is and is not likely to be achieved.</p>
<h1 class="unnumbered" id="bibliography">Bibliography</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-AdamsAizawa21" class="csl-entry" role="doc-biblioentry">
Adams, F., &amp; Aizawa, K. (2021). <span>‘Causal theories of mental content’</span>. Zalta E. N. (ed.) <em>The <span>Stanford</span> encyclopedia of philosophy</em>, Spring 2021.
</div>
<div id="ref-Anderson07" class="csl-entry" role="doc-biblioentry">
Anderson, J. R. (2007). <em>How can the human mind occur in a physical universe?</em> Oxford: Oxford University Press.
</div>
<div id="ref-Anderson14" class="csl-entry" role="doc-biblioentry">
Anderson, M. L. (2014). <em>After phrenology: Neural reuse and the interactive brain</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Baars88" class="csl-entry" role="doc-biblioentry">
Baars, B. (1988). <em>A cognitive theory of consciousness</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-BaarsFranklin03" class="csl-entry" role="doc-biblioentry">
Baars, B., &amp; Franklin, S. (2003). <span>‘How conscious experience and working memory interact’</span>, <em>Trends in Cognitive Sciences</em>, 7: 166–72.
</div>
<div id="ref-Barrett20" class="csl-entry" role="doc-biblioentry">
Barrett, H. C. (2020). <span>‘Towards a cognitive science of the human: Cross-cultural approaches and their urgency’</span>, <em>Trends in Cognitive Sciences</em>, 24: 620–38.
</div>
<div id="ref-BarrettKurzban06" class="csl-entry" role="doc-biblioentry">
Barrett, H. C., &amp; Kurzban, R. (2006). <span>‘Modularity in cognition: Framing the debate’</span>, <em>Psychological Review</em>, 113: 628–47.
</div>
<div id="ref-Block78" class="csl-entry" role="doc-biblioentry">
Block, N. (1978). <span>‘Troubles with functionalism’</span>. Savage C. W. (ed.) <em>Perception and cognition: Issues in the foundations of psychology, minnesota studies in the philosophy of science</em>, Vol. 9, pp. 261–325. University of Minnesota Press: Minneapolis.
</div>
<div id="ref-Block80" class="csl-entry" role="doc-biblioentry">
——. (1980). <span>‘What intuitions about homunculi don’t show’</span>, <em>Behavioral and Brain Sciences</em>, 3: 425–6.
</div>
<div id="ref-Block86" class="csl-entry" role="doc-biblioentry">
——. (1986). <span>‘Advertisement for a semantics for psychology’</span>, <em>Midwest Studies in Philosophy</em>, 10: 615–78.
</div>
<div id="ref-Boden89" class="csl-entry" role="doc-biblioentry">
Boden, M. A. (1989). <span>‘Escaping from the <span>C</span>hinese room’</span>. <em>Artificial intelligence in psychology</em>, pp. 82–100. MIT Press: Cambridge, MA.
</div>
<div id="ref-Brooks91" class="csl-entry" role="doc-biblioentry">
Brooks, R. A. (1991). <span>‘Intelligence without representation’</span>, <em>Artificial Intelligence</em>, 47: 139–59.
</div>
<div id="ref-Burge86" class="csl-entry" role="doc-biblioentry">
Burge, T. (1986). <span>‘Individualism and psychology’</span>, <em>Philosophical Review</em>, 95: 3–45.
</div>
<div id="ref-Carruthers06" class="csl-entry" role="doc-biblioentry">
Carruthers, P. (2006). <em>The architecture of the mind</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Chalmers96" class="csl-entry" role="doc-biblioentry">
Chalmers, D. J. (1996). <em>The conscious mind</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Chalmers10d" class="csl-entry" role="doc-biblioentry">
——. (2010d). <span>‘How can we construct a science of consciousness?’</span>. <em>The character of consciousness</em>, pp. 37–58. Oxford University Press.
</div>
<div id="ref-Chalmers10c" class="csl-entry" role="doc-biblioentry">
——. (2010c). <span>‘Consciousness and its place in nature’</span>. <em>The character of consciousness</em>, pp. 103–39. Oxford University Press.
</div>
<div id="ref-Chalmers10b" class="csl-entry" role="doc-biblioentry">
——. (2010a). <span>‘Facing up to the problem of consciousness’</span>. <em>The character of consciousness</em>, pp. 3–4. Oxford University Press.
</div>
<div id="ref-Chalmers10a" class="csl-entry" role="doc-biblioentry">
——. (2010b). <span>‘The two‐dimensional argument against materialism’</span>. <em>The character of consciousness</em>, pp. 141–205. Oxford University Press.
</div>
<div id="ref-Chalmers93a" class="csl-entry" role="doc-biblioentry">
——. (2012). <span>‘A computational foundation for the study of cognition’</span>, <em>Journal of Cognitive Science</em>, 12: 323–57.
</div>
<div id="ref-Chirimuuta21" class="csl-entry" role="doc-biblioentry">
Chirimuuta, M. (forthcoming). <em>How to simplify the brain</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Chomsky95a" class="csl-entry" role="doc-biblioentry">
Chomsky, N. (1995). <span>‘Language and nature’</span>, <em>Mind</em>, 104: 1–61.
</div>
<div id="ref-Chow13" class="csl-entry" role="doc-biblioentry">
Chow, S. J. (2013). <span>‘What’s the problem with the frame problem?’</span>, <em>Review of Philosophy and Psychology</em>, 4: 309–31.
</div>
<div id="ref-Clark00" class="csl-entry" role="doc-biblioentry">
Clark, A. (2000). <span>‘A case where access implies qualia?’</span>, <em>Analysis</em>, 60: 30–8.
</div>
<div id="ref-Clark02" class="csl-entry" role="doc-biblioentry">
——. (2002). <span>‘Global abductive inference and authoritative sources, or, how search engines can save cognitive science’</span>, <em>Cognitive Science Quarterly</em>, 2: 115–40.
</div>
<div id="ref-Cole20" class="csl-entry" role="doc-biblioentry">
Cole, D. (2020). <span>‘The <span>C</span>hinese room argument’</span>. Zalta E. N. (ed.) <em>The <span>Stanford</span> encyclopedia of philosophy</em>, Winter 2020.
</div>
<div id="ref-ColomboHartmann17" class="csl-entry" role="doc-biblioentry">
Colombo, M., &amp; Hartmann, S. (2017). <span>‘Bayesian cognitive science, unification, and explanation’</span>, <em>The British Journal for the Philosophy of Science</em>, 68: 451–84.
</div>
<div id="ref-Danks14" class="csl-entry" role="doc-biblioentry">
Danks, D. (2014). <em>Unifying the mind: Cognitive representations as graphical models</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-DavisMorgenstern04" class="csl-entry" role="doc-biblioentry">
Davis, E., &amp; Morgenstern, L. (2004). <span>‘Introduction: Progress in formal commonsense reasoning’</span>, <em>Artificial Intelligence</em>, 153: 1–2.
</div>
<div id="ref-DehaeneChangeux04" class="csl-entry" role="doc-biblioentry">
Dehaene, S., &amp; Changeux, J.-P. (2004). <span>‘Neural mechanisms for access to consciousness’</span>. Gazzaniga M. (ed.) <em>The cognitive neurosciences, <span>III</span></em>, pp. 1145–57. MIT Press: Cambridge, MA.
</div>
<div id="ref-Dennett78d" class="csl-entry" role="doc-biblioentry">
Dennett, D. C. (1978). <span>‘Why you can’t make a computer that feels pain’</span>, <em>Synthese</em>, 38: 415–56.
</div>
<div id="ref-Dennett87a" class="csl-entry" role="doc-biblioentry">
——. (1987). <em>The intentional stance</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Dennett91" class="csl-entry" role="doc-biblioentry">
——. (1991). <em>Consciousness explained</em>. Boston, MA: Little, Brown &amp; Company.
</div>
<div id="ref-Dennett95m" class="csl-entry" role="doc-biblioentry">
——. (1995). <span>‘The unimagined preposterousness of zombies’</span>, <em>Journal of Consciousness Studies</em>, 2: 322–6.
</div>
<div id="ref-Dennett01c" class="csl-entry" role="doc-biblioentry">
——. (2001). <span>‘The zombic hunch: Extinction of an intuition?’</span>, <em>Royal Institute of Philosophy Supplement</em>, 48: 27–43.
</div>
<div id="ref-Dennett13" class="csl-entry" role="doc-biblioentry">
——. (2013). <em>Intuition pumps and other tools for thinking</em>. New York, NY: W. W. Norton; Company.
</div>
<div id="ref-Dewhurst18" class="csl-entry" role="doc-biblioentry">
Dewhurst, J. (2018). <span>‘Individuation without representation’</span>, <em>The British Journal for the Philosophy of Science</em>, 69: 103–16.
</div>
<div id="ref-Dretske81" class="csl-entry" role="doc-biblioentry">
Dretske, F. (1981). <em>Knowledge and the flow of information</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Dretske95" class="csl-entry" role="doc-biblioentry">
——. (1995). <em>Naturalizing the mind</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Dreyfus72" class="csl-entry" role="doc-biblioentry">
Dreyfus, H. L. (1972). <em>What computers can’t do</em>. New York, NY: Harper &amp; Row.
</div>
<div id="ref-Dreyfus91" class="csl-entry" role="doc-biblioentry">
——. (1991). <em>Being-in-the-world: A commentary on <span>H</span>eidegger’s <em>being and time</em>, <span>Division I</span></em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Dreyfus92" class="csl-entry" role="doc-biblioentry">
——. (1992). <em>What computers still can’t do</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Dreyfus07" class="csl-entry" role="doc-biblioentry">
——. (2007). <span>‘Why <span>H</span>eideggerian <span>AI</span> failed and how fixing it would require making it more <span>H</span>eideggerian’</span>, <em>Artificial Intelligence</em>, 171: 1137–60.
</div>
<div id="ref-DreyfusDreyfus88" class="csl-entry" role="doc-biblioentry">
Dreyfus, H. L., &amp; Dreyfus, S. E. (1988). <span>‘Making a mind versus modeling the brain: Artificial intelligence back at a branchpoint’</span>, <em>Daedalus</em>, 117: 15–44.
</div>
<div id="ref-Egan03" class="csl-entry" role="doc-biblioentry">
Egan, F. (2003). <span>‘Naturalistic inquiry: Where does mental representation fit in?’</span>. Antony L. M. &amp; Hornstein N. (eds) <em>Chomsky and his critics</em>. Blackwell: Oxford.
</div>
<div id="ref-Egan14" class="csl-entry" role="doc-biblioentry">
——. (2014). <span>‘How to think about mental content’</span>, <em>Philosophical Studies</em>, 170: 115–35.
</div>
<div id="ref-Elgin17" class="csl-entry" role="doc-biblioentry">
Elgin, C. Z. (2017). <em>True enough</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Eliasmith05" class="csl-entry" role="doc-biblioentry">
Eliasmith, C. (2005). <span>‘Neurosemantics and categories’</span>. Cohen H. &amp; Lefebvre C. (eds) <em>Handbook of categorization in cognitive science</em>, pp. 1035–55. Elsevier: Amsterdam.
</div>
<div id="ref-Eliasmith13" class="csl-entry" role="doc-biblioentry">
——. (2013). <em>How to build a brain: A neural architecture for biological cognition</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Fodor78" class="csl-entry" role="doc-biblioentry">
Fodor, J. A. (1978). <span>‘Tom <span>S</span>wift and his procedural grandmother’</span>, <em>Cognition</em>, 6: 229–47.
</div>
<div id="ref-Fodor80b" class="csl-entry" role="doc-biblioentry">
——. (1980). <span>‘Searle on what only brains can do’</span>, <em>Behavioral and Brain Sciences</em>, 3: 431–2.
</div>
<div id="ref-Fodor83" class="csl-entry" role="doc-biblioentry">
——. (1983). <em>The modularity of mind</em>. MIT Press.
</div>
<div id="ref-Fodor90f" class="csl-entry" role="doc-biblioentry">
——. (1990). <em>A theory of content and other essays</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Fodor98" class="csl-entry" role="doc-biblioentry">
——. (1998). <em>Concepts</em>. Oxford: Blackwell.
</div>
<div id="ref-Fodor00" class="csl-entry" role="doc-biblioentry">
——. (2000). <em>The mind doesn’t work that way</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Fodor08" class="csl-entry" role="doc-biblioentry">
——. (2008). <em><span>LOT2</span>: The language of thought revisited</em>. Oxford: Oxford University Press.
</div>
<div id="ref-FodorLepore92" class="csl-entry" role="doc-biblioentry">
Fodor, J. A., &amp; Lepore, E. (1992). <em>Holism: A shopper’s guide</em>. Oxford: Blackwell.
</div>
<div id="ref-FodorPylyshyn88" class="csl-entry" role="doc-biblioentry">
Fodor, J. A., &amp; Pylyshyn, Z. W. (1988). <span>‘Connectionism and cognitive architecture’</span>, <em>Cognition</em>, 28: 3–71.
</div>
<div id="ref-FordPylyshyn96" class="csl-entry" role="doc-biblioentry">
Ford, K. M., &amp; Pylyshyn, Z. W. (Eds). (1996). <em>The robot’s dilemma revisited</em>. Norwood, NJ: Ablex.
</div>
<div id="ref-Frankish16a" class="csl-entry" role="doc-biblioentry">
Frankish, K. (2016). <span>‘Illusionism as a theory of consciousness’</span>, <em>Journal of Consciousness Studies</em>, 23: 11–39.
</div>
<div id="ref-Freeman00" class="csl-entry" role="doc-biblioentry">
Freeman, W. J. (2000). <em>How brains make up their minds</em>. New York, NY: Columbia University Press.
</div>
<div id="ref-GigerenzerToddABC99" class="csl-entry" role="doc-biblioentry">
Gigerenzer, G., Todd, P. M., &amp; ABC Research Group, the (Eds). (1999). <em>Simple heuristics that make us smart</em>. New York, NY: Oxford University Press.
</div>
<div id="ref-GoyalDidolkarLamb21" class="csl-entry" role="doc-biblioentry">
Goyal, A., Didolkar, A., Lamb, A., Badola, K., Ke, N. R., Rahaman, N., Binas, J., et al. (2021). <em>Coordination among neural modules through a shared global workspace</em>.
</div>
<div id="ref-Graziano16" class="csl-entry" role="doc-biblioentry">
Graziano, M. S. A. (2016). <span>‘Consciousness engineered’</span>, <em>Journal of Consciousness Studies</em>, 23: 98–115.
</div>
<div id="ref-Gulick18" class="csl-entry" role="doc-biblioentry">
Gulick, R. van. (2018). <span>‘Consciousness’</span>. Zalta E. N. (ed.) <em>The <span>Stanford</span> encyclopedia of philosophy</em>, Spring 2018.
</div>
<div id="ref-Harman87" class="csl-entry" role="doc-biblioentry">
Harman, G. (1987). <span>‘(Nonsolipsistic) conceptual role semantics’</span>. Lepore E. (ed.) <em>New directions in semantics</em>, pp. 55–81. Academic Press: London.
</div>
<div id="ref-Harnad90" class="csl-entry" role="doc-biblioentry">
Harnad, S. (1990). <span>‘The symbol grounding problem’</span>, <em>Physica D</em>, 42: 335–46.
</div>
<div id="ref-Haugeland98" class="csl-entry" role="doc-biblioentry">
Haugeland, J. (1998). <span>‘Mind embodied and embedded’</span>. Haugeland J. (ed.) <em>Having thought: Essays in the metaphysics of mind</em>, pp. 207–40. Harvard University Press: Cambridge, MA.
</div>
<div id="ref-HenrichHeineNorenzayan10" class="csl-entry" role="doc-biblioentry">
Henrich, J., Heine, S. J., &amp; Norenzayan, A. (2010). <span>‘The weirdest people in the world?’</span>, <em>Behavioral and Brain Sciences</em>, 33: 61–135.
</div>
<div id="ref-IrvineSprevak20" class="csl-entry" role="doc-biblioentry">
Irvine, E., &amp; Sprevak, M. (2020). <span>‘Eliminativism about consciousness’</span>. Kriegel U. (ed.) <em>The oxford handbook of the philosophy of consciousness</em>, pp. 348–70. Oxford University Press: Oxford.
</div>
<div id="ref-Isaac17" class="csl-entry" role="doc-biblioentry">
Isaac, A. M. C. (2019). <span>‘The semantics latent in <span>S</span>hannon information’</span>, <em>The British Journal for the Philosophy of Science</em>, 70: 103–25.
</div>
<div id="ref-Jackson82" class="csl-entry" role="doc-biblioentry">
Jackson, F. (1982). <span>‘Epiphenomenal qualia’</span>, <em>Philosophical Quarterly</em>, 32: 127–36.
</div>
<div id="ref-JohnsonLaird78" class="csl-entry" role="doc-biblioentry">
Johnson-Laird, P. N. (1978). <span>‘What’s wrong with <span>G</span>randma’s guide to procedural semantics: A reply to <span>J</span>erry <span>F</span>odor’</span>, <em>Cognition</em>, 6: 249–61.
</div>
<div id="ref-Kripke80" class="csl-entry" role="doc-biblioentry">
Kripke, S. A. (1980). <em>Naming and necessity</em>. Cambridge, MA: Harvard University Press.
</div>
<div id="ref-LakeUllman17" class="csl-entry" role="doc-biblioentry">
Lake, B. M., Ullman, T. D., Tenenbaum, J. B., &amp; Gershman, S. J. (2017). <span>‘Building machines that learn and think like people’</span>, <em>Behavioral and Brain Sciences</em>, 40: e253.
</div>
<div id="ref-Lee18" class="csl-entry" role="doc-biblioentry">
Lee, J. (2018). <span>‘Mechanisms, wide functions and content: Towards a computational pluralism’</span>, <em>The British Journal for the Philosophy of Science</em>. DOI: <a href="https://doi.org/10.1093/bjps/axy061">10.1093/bjps/axy061</a>
</div>
<div id="ref-LenatFeigenbaum91" class="csl-entry" role="doc-biblioentry">
Lenat, D. B., &amp; Feigenbaum, E. A. (1991). <span>‘On the thresholds of knowledge’</span>, <em>Artificial Intelligence</em>, 47: 185–250.
</div>
<div id="ref-Lifschitz15" class="csl-entry" role="doc-biblioentry">
Lifschitz, V. (2015). <span>‘The dramatic true story of the frame default’</span>, <em>Journal of Philosophical Logic</em>, 44: 163–96.
</div>
<div id="ref-Loewer17" class="csl-entry" role="doc-biblioentry">
Loewer, B. (2017). <span>‘A guide to naturalizing semantics’</span>. Hale B., Wright C., &amp; Miller A. (eds) <em>Companion to the philosophy of language</em>, 2nd ed., pp. 174–96. John Wiley &amp; Sons: New York, NY.
</div>
<div id="ref-Lormand90" class="csl-entry" role="doc-biblioentry">
Lormand, E. (1990). <span>‘Framing the frame problem’</span>, <em>Synthese</em>, 82: 353–74.
</div>
<div id="ref-LudwigSchneider08" class="csl-entry" role="doc-biblioentry">
Ludwig, K., &amp; Schneider, S. (2008). <span>‘Fodor’s challenge to the classical computational theory of mind’</span>, <em>Mind and Language</em>, 23/3: 123–43.
</div>
<div id="ref-Machery18" class="csl-entry" role="doc-biblioentry">
Machery, E. (forthcoming). <span>‘Discovery and confirmation in evolutionary psychology’</span>. Prinz J. (ed.) <em>The oxford handbook of philosophy of psychology</em>. Oxford University Press.
</div>
<div id="ref-MarcusDavis19" class="csl-entry" role="doc-biblioentry">
Marcus, G., &amp; Davis, E. (2019). <em>Rebooting <span>AI</span>: Building artificial intelligence we can trust</em>. New York, NY: Penguin Books.
</div>
<div id="ref-MashourRoelfsemaChangeux20" class="csl-entry" role="doc-biblioentry">
Mashour, G. A., Roelfsema, P. R., Changeux, J.-P., &amp; Dehaene, S. (2020). <span>‘Conscious processing and the <span>G</span>lobal <span>N</span>euronal <span>W</span>orkspace hypothesis’</span>, <em>Neuron</em>, 105: 776–98.
</div>
<div id="ref-Maudlin89" class="csl-entry" role="doc-biblioentry">
Maudlin, T. (1989). <span>‘Computation and consciousness’</span>, <em>The Journal of Philosophy</em>, 86: 407–32.
</div>
<div id="ref-McCarthy90" class="csl-entry" role="doc-biblioentry">
McCarthy, J. (1990). <em>Formalizing common sense: Papers by <span>J</span>ohn <span>McCarthy</span></em>. (V. L. Lifschitz, Ed.). Norwood, NJ: Ablex.
</div>
<div id="ref-McCarthyHayes69" class="csl-entry" role="doc-biblioentry">
McCarthy, J., &amp; Hayes, P. J. (1969). <span>‘Some philosophical problems from the standpoint of artificial intelligence’</span>. Meltzer B. &amp; Michie D. (eds) <em>Machine intelligence 4</em>, pp. 463–502. Edinburgh University Press: Edinburgh.
</div>
<div id="ref-Millikan04a" class="csl-entry" role="doc-biblioentry">
Millikan, R. G. (2004). <em>The varieties of meaning</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Mollo18" class="csl-entry" role="doc-biblioentry">
Mollo, D. C. (2018). <span>‘Functional individuation, mechanistic implementation: The proper way of seeing the mechanistic view of concrete computation’</span>, <em>Synthese</em>, 195: 3477–97.
</div>
<div id="ref-Mollo21" class="csl-entry" role="doc-biblioentry">
——. (forthcoming). <span>‘Deflationary realism: Representation and idealisation in cognitive science’</span>, <em>Mind and Language</em>.
</div>
<div id="ref-Morrison14" class="csl-entry" role="doc-biblioentry">
Morrison, M. (2014). <em>Reconstructing reality: Models, mathematics, and simulations</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Nagel74" class="csl-entry" role="doc-biblioentry">
Nagel, T. (1974). <span>‘What is it like to be a bat?’</span>, <em>Philosophical Review</em>, 83: 435–50.
</div>
<div id="ref-NeanderSchulte21" class="csl-entry" role="doc-biblioentry">
Neander, K., &amp; Schulte, P. (2021). <span>‘Teleological theories of mental content’</span>. Zalta E. N. (ed.) <em>The <span>Stanford</span> encyclopedia of philosophy</em>, Spring 2021.
</div>
<div id="ref-NewellSimon72" class="csl-entry" role="doc-biblioentry">
Newell, A., &amp; Simon, H. A. (1972). <em>Human problem solving</em>. Englewood Cliffs, NJ: Prentice-Hall.
</div>
<div id="ref-Nisbett03" class="csl-entry" role="doc-biblioentry">
Nisbett, R. E. (2003). <em>The geography of thought</em>. New York, NY: The Free Press.
</div>
<div id="ref-Papineau87" class="csl-entry" role="doc-biblioentry">
Papineau, D. (1987). <em>Reality and representation</em>. Oxford: Blackwell.
</div>
<div id="ref-Piccinini15" class="csl-entry" role="doc-biblioentry">
Piccinini, G. (2015). <em>The nature of computation</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Potochnik17" class="csl-entry" role="doc-biblioentry">
Potochnik, A. (2017). <em>Idealization and the aims of science</em>. Chicago, IL: University of Chicago Press.
</div>
<div id="ref-Prinz16" class="csl-entry" role="doc-biblioentry">
Prinz, J. (2016). <span>‘Against illusionism’</span>, <em>Journal of Consciousness Studies</em>, 23: 186–96.
</div>
<div id="ref-Putnam81" class="csl-entry" role="doc-biblioentry">
Putnam, H. (1981). <em>Reason, truth and history</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-Pylyshyn87" class="csl-entry" role="doc-biblioentry">
Pylyshyn, Z. W. (Ed.). (1987). <em>The robot’s dilemma</em>. Norwood, NJ: Ablex.
</div>
<div id="ref-Ramsey07" class="csl-entry" role="doc-biblioentry">
Ramsey, W. M. (2007). <em>Representation reconsidered</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-Rescorla13" class="csl-entry" role="doc-biblioentry">
Rescorla, M. (2013). <span>‘Against structuralist theories of computational implementation’</span>, <em>The British Journal for the Philosophy of Science</em>, 64: 681–707.
</div>
<div id="ref-RollsTreves11" class="csl-entry" role="doc-biblioentry">
Rolls, E. T., &amp; Treves, A. (2011). <span>‘The neural encoding of information in the brain’</span>, <em>Progress in Neurobiology</em>, 95: 448–90.
</div>
<div id="ref-Ryder04" class="csl-entry" role="doc-biblioentry">
Ryder, D. (2004). <span>‘<span>SINBAD</span> neurosemantics: A theory of mental representation’</span>, <em>Mind and Language</em>, 19: 211–40.
</div>
<div id="ref-Samuels05" class="csl-entry" role="doc-biblioentry">
Samuels, R. (2005). <span>‘The complexity of cognition: Tractability arguments for massive modularity’</span>. Carruthers P., Laurence S., &amp; Stich S. P. (eds) <em>The innate mind: Vol. I, structure and contents</em>, pp. 107–21. Oxford University Press: Oxford.
</div>
<div id="ref-Samuels10" class="csl-entry" role="doc-biblioentry">
——. (2010). <span>‘Classical computationalism and the many problems of cognitive relevance’</span>, <em>Studies in History and Philosophy of Science</em>, 41: 280–93.
</div>
<div id="ref-SchankAbelson77" class="csl-entry" role="doc-biblioentry">
Schank, R. C., &amp; Abelson, R. P. (1977). <em>Scripts, plans, goals, and understanding</em>. Hillsdale, NJ: Lawrence Erlbaum.
</div>
<div id="ref-Schneider11" class="csl-entry" role="doc-biblioentry">
Schneider, S. (2011). <em>The language of thought: A new philosophical direction</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Searle80" class="csl-entry" role="doc-biblioentry">
Searle, J. R. (1980). <span>‘Minds, brains, and programs’</span>, <em>Behavioral and Brain Sciences</em>, 3: 417–24.
</div>
<div id="ref-Searle84b" class="csl-entry" role="doc-biblioentry">
——. (1984). <em>Minds, brains and science</em>. Cambridge, MA: Harvard University Press.
</div>
<div id="ref-Searle92" class="csl-entry" role="doc-biblioentry">
——. (1992). <em>The rediscovery of the mind</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Sellars62" class="csl-entry" role="doc-biblioentry">
Sellars, W. (1962). <span>‘Philosophy and the scientific image of man’</span>. Colodny R. (ed.) <em>Frontiers of science and philosophy</em>, pp. 35–78. University of Pittsburgh Press: Pittsburgh, PA.
</div>
<div id="ref-Shagrir12" class="csl-entry" role="doc-biblioentry">
Shagrir, O. (2012). <span>‘Structural representations and the brain’</span>, <em>The British Journal for the Philosophy of Science</em>, 63: 519–45.
</div>
<div id="ref-Shagrir18" class="csl-entry" role="doc-biblioentry">
——. (2018). <span>‘In defense of the semantic view of computation’</span>, <em>Synthese</em>.
</div>
<div id="ref-Shanahan97" class="csl-entry" role="doc-biblioentry">
Shanahan, M. (1997). <em>Solving the frame problem</em>. Cambridge, MA: Bradford Books, MIT Press.
</div>
<div id="ref-Shanahan16" class="csl-entry" role="doc-biblioentry">
——. (2016). <span>‘The frame problem’</span>. Zalta E. N. (ed.) <em>The <span>Stanford</span> encyclopedia of philosophy</em>, Spring 2016.
</div>
<div id="ref-ShanahanBaars05" class="csl-entry" role="doc-biblioentry">
Shanahan, M., &amp; Baars, B. (2005). <span>‘Applying global workspace theory to the frame problem’</span>, <em>Cognition</em>, 98: 157–76.
</div>
<div id="ref-Shea13" class="csl-entry" role="doc-biblioentry">
Shea, N. (2013). <span>‘Naturalising representational content’</span>, <em>Philosophy Compass</em>, 8: 496–509.
</div>
<div id="ref-Shea18" class="csl-entry" role="doc-biblioentry">
——. (2018). <em>Representation in cognitive science</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Skyrms10" class="csl-entry" role="doc-biblioentry">
Skyrms, B. (2010). <em>Signals</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Sprevak08a" class="csl-entry" role="doc-biblioentry">
Sprevak, M. (2010). <span>‘Computation, individuation, and the received view on representation’</span>, <em>Studies in History and Philosophy of Science</em>, 41: 260–70.
</div>
<div id="ref-Sprevak13" class="csl-entry" role="doc-biblioentry">
——. (2013). <span>‘Fictionalism about neural representations’</span>, <em>The Monist</em>, 96: 539–60.
</div>
<div id="ref-Sprevak16" class="csl-entry" role="doc-biblioentry">
——. (2016). <span>‘Philosophy of the psychological and cognitive sciences’</span>. Humphreys P. (ed.) <em>Oxford handbook for the philosophy of science</em>, pp. 92–114. Oxford University Press: Oxford.
</div>
<div id="ref-Sprevak18a" class="csl-entry" role="doc-biblioentry">
——. (2019). <span>‘Review of <span>Susan Schneider</span>, <em><span>T</span>he <span>L</span>anguage of <span>T</span>hought: <span>A</span> <span>N</span>ew <span>P</span>hilosophical <span>D</span>irection</em>’</span>, <em>Mind</em>, 128: 555–64.
</div>
<div id="ref-Strawson94" class="csl-entry" role="doc-biblioentry">
Strawson, G. (2010). <em>Mental reality</em>., 2nd ed. Cambridge, MA: MIT Press.
</div>
<div id="ref-Strawson18" class="csl-entry" role="doc-biblioentry">
——. (2018). <span>‘The consciousness deniers’</span>, <em>The New York Review of Books</em>.
</div>
<div id="ref-Swoyer91" class="csl-entry" role="doc-biblioentry">
Swoyer, C. (1991). <span>‘Structural representation and surrogative reasoning’</span>, <em>Synthese</em>, 87: 449–508.
</div>
<div id="ref-Tye18" class="csl-entry" role="doc-biblioentry">
Tye, M. (2018). <span>‘Qualia’</span>. Zalta E. N. (ed.) <em>The <span>Stanford</span> encyclopedia of philosophy</em>, Summer 2018.
</div>
<div id="ref-Usher01" class="csl-entry" role="doc-biblioentry">
Usher, M. (2001). <span>‘A statistical referential theory of content: Using information theory to account for misrepresentation’</span>, <em>Mind and Language</em>, 16: 311–34.
</div>
<div id="ref-Wakefield03" class="csl-entry" role="doc-biblioentry">
Wakefield, J. C. (2003). <span>‘The <span>C</span>hinese room argument reconsidered: Essentialism, indeterminacy, and <span>Strong AI</span>’</span>, <em>Minds and Machines</em>, 13: 285–319.
</div>
<div id="ref-Wheeler05" class="csl-entry" role="doc-biblioentry">
Wheeler, M. (2005). <em>Reconstructing the cognitive world</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Wheeler08a" class="csl-entry" role="doc-biblioentry">
——. (2008). <span>‘Cognition in context: Phenomenology, situated robotics and the frame problem’</span>, <em>International Journal of Philosophical Studies</em>, 16: 323–49.
</div>
<div id="ref-Winograd72" class="csl-entry" role="doc-biblioentry">
Winograd, T. (1972). <span>‘Understanding natural language’</span>, <em>Cognitive Psychology</em>, 3: 1–91.
</div>
</div>
</div>

                            </div>
                            
                        </div>

                    </div>

                    <div class="is-col is-33">     
                        <div class="is-hidden-print is-hidden-mobile">
                            
                                <h1 style="margin-top: 0px;">Contents</h1>
                                <ul class="is-unstyled">
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#introduction"><span style="visibility: visible;">1</span> &nbsp;  Introduction</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#overview-of-chapter"><span style="visibility: visible;">1.1</span> &nbsp;  Overview of chapter</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#semantic-content-searles-chinese-room-argument"><span style="visibility: visible;">2</span> &nbsp;  Semantic content &#x2013; Searle&#x2019;s Chinese room argument</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-chinese-room-argument"><span style="visibility: visible;">2.1</span> &nbsp;  The Chinese room argument</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-problem-of-semantic-content"><span style="visibility: visible;">2.2</span> &nbsp;  The problem of semantic content</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#theories-of-content"><span style="visibility: visible;">2.3</span> &nbsp;  Theories of content</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#content-and-physical-computation"><span style="visibility: visible;">2.4</span> &nbsp;  Content and physical computation</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#consciousness-the-hard-problem"><span style="visibility: visible;">3</span> &nbsp;  Consciousness &#x2013; The hard problem</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-conceivability-argument-against-physicalism"><span style="visibility: visible;">3.1</span> &nbsp;  The conceivability argument against physicalism</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-conceivability-argument-against-computational-functionalism"><span style="visibility: visible;">3.2</span> &nbsp;  The conceivability argument against computational functionalism</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#naturalistic-dualism"><span style="visibility: visible;">3.3</span> &nbsp;  Naturalistic dualism</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#eliminativism-and-related-replies"><span style="visibility: visible;">3.4</span> &nbsp;  Eliminativism and related replies</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#central-reasoning-the-frame-problem"><span style="visibility: visible;">4</span> &nbsp;  Central reasoning &#x2013; The frame problem</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-frame-problem"><span style="visibility: visible;">4.1</span> &nbsp;  The frame problem</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#dreyfuss-argument"><span style="visibility: visible;">4.2</span> &nbsp;  Dreyfus&#x2019;s argument</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#fodors-argument"><span style="visibility: visible;">4.3</span> &nbsp;  Fodor&#x2019;s argument</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#responses-to-the-problem"><span style="visibility: visible;">4.4</span> &nbsp;  Responses to the problem</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#conclusion"><span style="visibility: visible;">5</span> &nbsp;  Conclusion</a>
            
        </span>
        
    </li>
    
</ul>

                            
                            
                        </div>
                    </div>
                </div>
            </main>

        <footer class="footer"></footer>

        </div>

        <script src="https://marksprevak.com/kube/js/kube.min.js"></script>
<script>
    $K.init();
</script>


    </body>
</html>
