<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Philosophical issues in computational cognitive sciences | Mark Sprevak</title>
        <meta name="description" content="">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="stylesheet" href="https://marksprevak.com/kube/css/kube.min.css" />
<link rel="stylesheet" href="https://marksprevak.com/css-customisations/sprevak.css" />
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<title>Mark Sprevak</title>
<base href="https://marksprevak.com/">
<link rel="canonical" href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/">

<link href="https://fonts.googleapis.com/css?family=Roboto:400,700%7CLato:400,700" rel="stylesheet">

    </head>
    <body>
        <div class="page wrapper">

            <header class="header">
                <div class="is-navbar-container" style="padding-bottom: 6px; padding-top: 0px; margin-bottom: 12px; border-bottom: 1px solid; border-color: rgba(0, 0, 0, 0.3);">
    <div class="is-brand">
        <div class="titlebar"><a href="https://marksprevak.com/">Mark&nbsp;Sprevak</a></div>
        
        <a href="#"
                style="color: rgba(0, 0, 0, 0.8); text-decoration: none; border-bottom: none; font-size:18px;"
                class="is-hidden-print nav-toggle is-push-right-mobile is-shown-mobile icon-kube-menu"
                data-kube="toggle"
                data-target="#top-navbar"></a>
    </div>
    <div id="top-navbar" class="is-navbar is-hidden-print is-hidden-mobile">
        <nav class="is-push-right">
            <ul style="text-align: right;">
                
                
                
                
                
                <li  >
                    <a href="https://marksprevak.com/publications/" style="text-decoration: none; border-bottom: none;">Publications</a>
                </li>
                
                
                
                <li  >
                    <a href="https://marksprevak.com/talks/" style="text-decoration: none; border-bottom: none;">Talks</a>
                </li>
                
                
                
                <li  >
                    <a href="https://marksprevak.com/outreach/" style="text-decoration: none; border-bottom: none;">Outreach</a>
                </li>
                
                
                
                <li  >
                    <a href="https://marksprevak.com/cv/" style="text-decoration: none; border-bottom: none;">CV</a>
                </li>
                
                
                
                <li  >
                    <a href="https://marksprevak.com/phds/" style="text-decoration: none; border-bottom: none;">PhD study</a>
                </li>
                
                
                
                <li  >
                    <a href="https://marksprevak.com/mscs/" style="text-decoration: none; border-bottom: none;">MSc study</a>
                </li>
                
                
                
                <li  >
                    <a href="https://marksprevak.com/teaching/" style="text-decoration: none; border-bottom: none;">Teaching</a>
                </li>
                
                
            </ul>
        </nav>
    </div>
</div>

            </header>

            <main class="main">
                <div class="is-row">

                    <div class="is-col is-67">     

                        <div style="padding-bottom: 30px;">
                            <div style="margin-bottom: 10px;">
                                <h1 class="is-color-black" style="margin-top: 0px; margin-bottom: 0px;">Philosophical issues in computational cognitive sciences</h1>
                                
                                <p class="is-muted" style="margin-top: 10px;">
                                    
                                        2023  &nbsp;
                                    
                                    <em>The Cambridge Handbook of Computational Cognitive Sciences</em>
(edited by Ron Sun), Cambridge University Press: Cambridge,
pp.&#xA0;1201&#x2013;1227

                                </p>
                                <p class="is-small" style="margin-top: 10px;">
                                    <span>Last updated 6 June 2023</span>
                                    
                                </p>
                            </div>
                            <div class="is-hidden-print">
                                
<a href="https://marksprevak.com/pdf/paper/Sprevak--Philosophical-Issues-in-Computational-Cognitive-Sciences.pdf" target="_blank" class="label is-primary is-focus" style="margin-left: 0px; margin-right:5px;">
    <i class="far fa-file-pdf" style="font-size: 12px;"></i>
    &nbsp;PDF
</a>




<a href="https://dx.doi.org/10.1017/9781108755610.043" target="_blank" class="label is-tertiary is-focus" style="margin-left: 0; padding-left: 0; margin-right:2px;">
    doi&nbsp;
    <i class="fas fa-external-link-alt"></i>
</a>


                            </div>
                        </div>

                        <div class="is-hidden-mobile">
                            
                            <div class="article-style" style="margin-left: 30px; margin-right: 30px; margin-bottom: 30px;">
                                <p>What counts as a philosophical issue in computational cognitive
science? This chapter briefly reviews possible answers before focusing
on a specific subset of philosophical issues. These surround challenges
that have been raised by philosophers regarding the scope of
computational models of cognition. The arguments suggest that there are
aspects of human cognition that may, for various reasons, resist
explanation or description in terms of computation. The primary targets
of these &#x2018;no go&#x2019; arguments have been <em>semantic content</em>,
<em>phenomenal consciousness</em>, and <em>central reasoning</em>. This
chapter reviews the arguments and considers possible replies. It
concludes by highlighting the differences between the arguments, their
limitations, and how they might contribute to the wider project of
estimating the value of ongoing research programmes in computational
cognitive science.</p>

                            </div>
                            
                        </div>
                        <div class="is-shown-mobile">
                            
                            
                            <div class="is-muted is-smaller is-hidden-print">
                                Abstract:
                            </div>
                            <div class="article-style" style="margin-bottom: 30px;">
                                <p>What counts as a philosophical issue in computational cognitive
science? This chapter briefly reviews possible answers before focusing
on a specific subset of philosophical issues. These surround challenges
that have been raised by philosophers regarding the scope of
computational models of cognition. The arguments suggest that there are
aspects of human cognition that may, for various reasons, resist
explanation or description in terms of computation. The primary targets
of these &#x2018;no go&#x2019; arguments have been <em>semantic content</em>,
<em>phenomenal consciousness</em>, and <em>central reasoning</em>. This
chapter reviews the arguments and considers possible replies. It
concludes by highlighting the differences between the arguments, their
limitations, and how they might contribute to the wider project of
estimating the value of ongoing research programmes in computational
cognitive science.</p>

                            </div>
                            
                        </div>

                        <div>
                            
                            <div class="is-shown-mobile">
                                
                                    <h1 style="margin-top: 0px;" id="internal-mds-toc">Contents</h1>
                                    <ul class="is-unstyled">
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#introduction"><span style="visibility: visible;">1</span> &nbsp;  Introduction</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#overview-of-chapter"><span style="visibility: visible;">1.1</span> &nbsp;  Overview of chapter</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#semantic-content-searles-chinese-room-argument"><span style="visibility: visible;">2</span> &nbsp;  Semantic content &#x2013; Searle&#x2019;s Chinese room argument</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-chinese-room-argument"><span style="visibility: visible;">2.1</span> &nbsp;  The Chinese room argument</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-problem-of-semantic-content"><span style="visibility: visible;">2.2</span> &nbsp;  The problem of semantic content</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#theories-of-content"><span style="visibility: visible;">2.3</span> &nbsp;  Theories of content</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#content-and-physical-computation"><span style="visibility: visible;">2.4</span> &nbsp;  Content and physical computation</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#phenomenal-consciousness-the-hard-problem"><span style="visibility: visible;">3</span> &nbsp;  Phenomenal consciousness &#x2013; The hard problem</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-conceivability-argument-against-physicalism"><span style="visibility: visible;">3.1</span> &nbsp;  The conceivability argument against physicalism</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-conceivability-argument-against-computational-functionalism"><span style="visibility: visible;">3.2</span> &nbsp;  The conceivability argument against computational functionalism</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#naturalistic-dualism"><span style="visibility: visible;">3.3</span> &nbsp;  Naturalistic dualism</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#eliminativism-and-related-replies"><span style="visibility: visible;">3.4</span> &nbsp;  Eliminativism and related replies</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#central-reasoning-the-frame-problem"><span style="visibility: visible;">4</span> &nbsp;  Central reasoning &#x2013; The frame problem</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-frame-problem"><span style="visibility: visible;">4.1</span> &nbsp;  The frame problem</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#dreyfuss-argument"><span style="visibility: visible;">4.2</span> &nbsp;  Dreyfus&#x2019;s argument</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#fodors-argument"><span style="visibility: visible;">4.3</span> &nbsp;  Fodor&#x2019;s argument</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#responses-to-the-problems"><span style="visibility: visible;">4.4</span> &nbsp;  Responses to the problems</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#conclusion"><span style="visibility: visible;">5</span> &nbsp;  Conclusion</a>
            
        </span>
        
    </li>
    
</ul>

                                
                                
                            </div>
                            <div class="article-style">
                                <div>
<h1 data-number="1" id="introduction"><span
class="header-section-number">1</span> Introduction</h1>
<p>In 1962, Wilfred Sellars wrote: ‘The aim of philosophy, abstractly
formulated, is to understand how things in the broadest possible sense
of the term hang together in the broadest possible sense of the term’
<span class="citation" data-cites="Sellars62">(Sellars 1962 p.
35)</span>. On this view, philosophical issues are marked out not by
having some uniquely philosophical subject matter, but in terms of the
overall scope of the enquiry. When one turns to philosophical issues,
what one is doing is taking a step back from some of the details of the
science and considering how matters hang together relative to the broad
ambitions and goals that motivated the scientific enquiry in the first
place. In the case of the computational cognitive sciences, this may
involve asking such questions as: Are there aspects of cognition or
behaviour that are not amenable to computational modelling? How do
distinct computational models of cognition and behaviour fit together to
tell a coherent story about cognition and behaviour? What exactly does a
specific computational model tell (or fail to tell) us about cognition
and behaviour? What distinguishes computational models from alternative
approaches to modelling cognition and behaviour? How does a
computational model connect to, and help to answer, our pre-theoretical
questions about what minds are and how they work?</p>
<p>Progress in answering these questions may come from any or all sides.
It would be a mistake to think that philosophical issues are somehow
only within the purview of academic philosophers. Anyone who takes
computational modelling seriously as an attempt to study cognition is
likely to want to know the answers to these questions and is also liable
to be able to contribute to the project of answering them. What
philosophers bring to this joint project is a set of conceptual tools
and approaches that have been developed in other domains to address
structurally similar issues. They also have the luxury of being allowed
to think and write about the big questions.</p>
<p>Sellars had a relatively narrow conception of what it meant to
understand how things hang together. He interpreted this as an attempt
to reconcile two separate images that we have of how the world works:
the <em>scientific image</em> (which describes the posits of the natural
sciences – cells, molecules, atoms, forces, etc.) and the <em>manifest
image</em> (which describes the posits of human common-sense
understanding of the world – persons, thoughts, feelings, ideas, etc.)
<span class="citation" data-cites="Sellars62">(Sellars 1962)</span>.
This chapter adopts a somewhat looser interpretation of the project.
Models in the computational cognitive sciences are often partial,
provisional, and selected from many possible alternatives that are also
consistent with the data. It would be misleading to think that current
computational cognitive science contains a single, coherent account that
is ‘the’ scientific image of cognition. Similar concerns could also be
raised about our manifest image of the world in light of observations of
cross-cultural differences in human folk understanding and
conceptualisations of the world <span class="citation"
data-cites="Nisbett03 HenrichHeineNorenzayan10 Barrett20">(Barrett 2020;
Henrich et al. 2010; Nisbett 2003)</span>. The view adopted in this
chapter is that the philosopher’s goal is to understand how the many
(and varied) current approaches to computational modelling of cognition
hang together, both with each other, with work in the other sciences
(including neuroscience, cellular biology, evolutionary biology, and the
social sciences), and with our various pre-theoretical folk questions
and insights regarding the mind. There is no prior commitment here to a
single, well-defined scientific image or manifest image, but rather the
ambition to understand how the various perspectives we have on cognition
and behaviour cohere and allow us to understand what minds are and how
they work <span class="citation" data-cites="Sprevak16">(Sprevak
2016)</span>.</p>
<p>Under this broad heading, there is a huge range of work. This
includes consideration of how to interpret the terms of specific
computational models – about which parameters one should be a ‘realist’
or an ‘instrumentalist’ <span class="citation"
data-cites="Rescorla15 ColomboSeries12">(Colombo &amp; Seriès 2012;
Rescorla 2016)</span>; how to make sense of theoretical concepts that
appear across multiple models, like the notion of a cognitive ‘module’
<span class="citation" data-cites="Samuels98 Carruthers06">(Carruthers
2006; Samuels 1998)</span>; analysis and formalisation of general
features of experimental methodology in computational neuroscience <span
class="citation" data-cites="Glymour01 Machery12">(Glymour 2001; Machery
2013)</span>; identification of differences between computational
approaches and rival approaches to modelling cognition <span
class="citation" data-cites="Eliasmith03 Gelder95">(Eliasmith 2003;
Gelder 1995)</span>; consideration of how techniques in machine learning
and AI might inform work in computational neuroscience <span
class="citation" data-cites="Sullivan19 Buckner21">(Buckner 2021;
Sullivan 2019)</span>; interpretation of experimental results that
function as evidence for specific computational models <span
class="citation"
data-cites="Block07 SheaBayne10 ApperlyButterfill09">(Apperly &amp;
Butterfill 2009; Block 2007; Shea &amp; Bayne 2010)</span>; and
consideration of how computational models of cognition connect to wider
questions about the nature of the human mind, its subjective
experiences, its evolutionary history, and the kinds of social and
technological structures that it builds <span class="citation"
data-cites="Sterelny03 Dennett18 Clark15 GodfreySmith16">(Clark 2016;
Dennett 2017; Godfrey-Smith 2016; Sterelny 2003)</span>.</p>
<p>The primary focus here will, by necessity, be narrower than the full
extent of issues within this diverse intellectual landscape. This
chapter focuses on challenges raised to computational modelling that
arise from philosophical work on the nature of cognition and
consciousness.</p>
<h2 data-number="1.1" id="overview-of-chapter"><span
class="header-section-number">1.1</span> Overview of chapter</h2>
<p>When building a computational model in the cognitive sciences,
researchers generally aim to build a model of some prescribed subdomain
within cognition or behaviour (e.g. of face recognition, cheater
detection, word segmentation, or depth perception). Splitting up human
cognition into various smaller domains raises questions about
<em>how</em> one should do this. This is the problem of how one should
<em>individuate</em> our cognitive capacities and overt behaviour <span
class="citation" data-cites="Machery18 BarrettKurzban06 Anderson14">(M.
L. Anderson 2014; Barrett &amp; Kurzban 2006; Machery
forthcoming)</span>. It also raises questions about how the separate
models of individual cognitive subdomains that one hopes to obtain will
subsequently be woven together to create a coherent, integrated
understanding of cognition. This concerns the issue of how one should
<em>unify</em> models of distinct aspects of cognition <span
class="citation"
data-cites="Danks14 Eliasmith13 ColomboHartmann17">(Colombo &amp;
Hartmann 2017; Danks 2014; Eliasmith 2013)</span>.</p>
<p>This chapter focuses on a set of issues that are related, but
posterior, to the two just mentioned. These concern possible
<em>gaps</em> left by this strategy for modelling cognition. If this
strategy were in an ideal world to run to completion, would there be any
aspects of cognition or behaviour that would be missing from the final
picture? Are there any aspects of cognition for which we should
<em>not</em> expect to obtain a computational model? Are there cognitive
domains that are, for some reason, ‘no go’ areas for computational
modelling? The chapter examines three possible candidates: <em>semantic
content</em> (Section 2), <em>phenomenal consciousness</em> (Section 3),
and <em>central reasoning</em> (Section 4). In each case, philosophers
have argued that there are good reasons to believe that we cannot obtain
an adequate computational model of the domain in question.</p>
<p>These ‘no go’ arguments may be subdivided further into <em>in
principle</em> and <em>in practice</em> arguments. In principle
arguments aim to show that it is <em>impossible</em> for any
computational model to account for the cognitive capacity in question.
In practice arguments are weaker. They aim only to show that, given our
current state of knowledge, we should not expect to discover such a
model – an adequate model <em>might</em> exist, but we should not expect
to find it, at least in the foreseeable future.</p>
<h1 data-number="2"
id="semantic-content-searles-chinese-room-argument"><span
class="header-section-number">2</span> Semantic content – Searle’s
Chinese room argument</h1>
<p>John Searle’s Chinese room argument is one of the oldest and most
notorious ‘no go’ arguments concerning computational modelling of
cognition. The precise nature of its intended target has been liable to
shift between different presentations of the argument. Searle has
claimed in various contexts that the argument shows that
<em>understanding</em>, <em>semantic content</em>,
<em>intentionality</em>, and <em>consciousness</em> cannot adequately be
captured by a computational model <span class="citation"
data-cites="Searle92">(according to him, all these properties are
linked, see Searle 1992 pp. 127–97)</span>. In his original formulation,
Searle’s target was <em>understanding</em>, and specifically our ability
to understand simple stories. He considered whether a computational
model would adequately be able to account for this cognitive capacity.
More precisely, he considered whether such a model would be able to
explain the difference between understanding and not understanding a
simple story <span class="citation"
data-cites="Searle80 SchankAbelson77 Winograd72">(Searle 1980;
cf. models of understanding in Schank &amp; Abelson 1977; Winograd
1972)</span>.</p>
<h2 data-number="2.1" id="the-chinese-room-argument"><span
class="header-section-number">2.1</span> The Chinese room argument</h2>
<p>Searle’s argument consisted in a thought experiment concerning
implementation of the computation. Imagine a monolingual English speaker
inside a room with a rule-book and sheets of paper. The rule-book
contains instructions in English on what to do if presented with Chinese
symbols. The instructions might take the form: ‘If you see Chinese
symbol X on one sheet of paper and Chinese symbol Y on another, then
write down Chinese symbol Z on a third sheet of paper’. Pieces of paper
with Chinese writing are passed into the room and the person inside
follows the rules and passes pieces of paper out. Chinese speakers
outside the room label the sheets that are passed in ‘story’ and
‘questions’, respectively, and the sheets that come out ‘answers to
questions’. Imagine that the rule-book is as sophisticated as you like,
and certainly sophisticated enough that the responses that the person
inside the room gives are indistinguishable from those of a native
Chinese speaker. Does the person inside the room thereby understand
Chinese? Searle claims that they do not <span class="citation"
data-cites="Block80 Maudlin89 Wakefield03">(for discussion of the
reliability of his intuition here, see Block 1980; Maudlin 1989;
Wakefield 2003)</span>.</p>
<p>Searle observes that the Chinese room is a computer, and he
identifies the rule-book with the (symbolic) computation that it
performs. He then reminds us that the thought experiment does not depend
on the particular rule-book used: it does not matter how sophisticated
the rule-book, the person inside the room would still be shuffling
Chinese symbols without understanding what they mean. Since any symbolic
computational process can be described by some rule-book, the thought
experiment shows that the person inside the Chinese room will not
understand the meaning of the Chinese expressions they manipulate no
matter which symbolic computation they perform. Therefore, we can
conclude that the performance of a symbolic computation is insufficient,
by itself, to account for the difference between the system performing
the computation understanding and not understanding what the Chinese
expressions mean. Searle infers from this that any attempt to model
understanding purely in terms of a formal, symbolic computation is
doomed to failure. According to him, the reason why is that a formal
computational model cannot induce <em>semantic</em> properties, which
are essential to accounting for a semantically laden cognitive process
like understanding <span class="citation" data-cites="Searle80">(Searle
1980 p. 422)</span>.</p>
<h2 data-number="2.2" id="the-problem-of-semantic-content"><span
class="header-section-number">2.2</span> The problem of semantic
content</h2>
<p>Many objections have been raised to Searle’s Chinese room argument
<span class="citation" data-cites="Cole20">(for a summary, see Cole
2020)</span>. However, it is notable that despite the argument’s many
defects, the main conclusion that Searle drew has been left largely
unchallenged by subsequent attacks. This is that <em>manipulation of
formal symbols</em> is insufficient to generate the semantic properties
associated with cognitive processes like understanding. In Searle’s
terms, the Chinese room thought experiment, whatever its specific
shortcomings, is an illustration of a valid general principle that
‘syntax is not sufficient for semantics’ <span class="citation"
data-cites="Searle84b">(Searle 1984)</span>. Note that ‘syntax’ here
does not refer to the static grammatical properties of symbols or
well-formedness of linguistic expressions, but refers to the algorithmic
rules by which symbolic expressions are manipulated or transformed
during a computation. ‘Semantics’ refers specifically to the
denotational aspects of the meaning associated with symbolic expressions
– their intentional properties (i.e. what they refer to in the
world).</p>
<p>Searle is not alone in making this claim. <span class="citation"
data-cites="Putnam81">Putnam (1981)</span> argued that manipulating
symbols (mere ‘syntactic play’) cannot determine what a computation’s
symbols refer to, or whether they carry any referential semantic content
at all (pp. 10–11). <span class="citation" data-cites="Burge86">Burge
(1986)</span>, building on earlier work by Putnam and himself on
referring terms in natural language, noted that a physical duplicate of
a computer placed in a different physical environment might undergo
exactly the same formal transitions, but have different meaning attached
to its symbolic expressions based on its relationship to different
environmental properties. <span class="citation"
data-cites="Fodor78">Fodor (1978)</span> described two physically
identical devices that undergo the same symbol-shuffling processes, one
of which runs a simulation of the Six-Day War (with its symbols
referring to tank divisions, jet planes, and infantry units) and the
other runs a simulation of a chess game (with its symbols referring to
knights, bishops, and pawns). <span class="citation"
data-cites="Harnad90">Harnad (1990)</span> argued that all computational
models based on symbol processing face a ‘symbol grounding’ problem:
although some of their symbols might have their semantic content
determined by their formal relationship to other symbols, that sort of
process has to bottom out somewhere with symbolic expressions that have
their meaning determined in some other way (e.g. by causal, non-formal
relationships to external objects in the environment in perception or
motor control).</p>
<p>These considerations are also not confined to symbolic computational
models of cognition. Similar observations could be made about
computational models that are defined over numerical values or over
probabilities. Consider artificial neural networks. These computational
models consist in collections of abstract nodes and connections that
chain together long sequences of mathematical operations on numerical
activation values or connection weights (adding, multiplying,
thresholding values). What do these numerical activation values or
connection weights mean? How do they relate to distal properties or
objects in the environment? As outside observers, we might
<em>interpret</em> numerical values inside an artificial neural network
as referring to certain things (just as, in a similar fashion, we might
interpret certain symbolic expressions in a classical, symbolic
computation as referring to certain things). Independent of our
interpreting attitudes, however, the mathematical rules that define an
artificial neural network do not fix this semantic content. The rules
associated with an artificial neural network describe how numerical
values are transformed during a computation (during inference or
learning), but they do not say what those numbers (either individually
or taken in combination) represent in the world. Numerical rules no more
imbue an artificial neural network with semantic content than do the
symbolic rules that operate over expressions for a classical, symbolic
computation <span class="citation" data-cites="Searle90b">(cf. Searle
1990)</span>. Computational models that operate over probabilities or
probability distributions face a similar kind of problem. These models
are normally defined in terms of operations on probability distributions
(understood as ensembles of numerical values that satisfy the
requirements for a measure of probability). These distributions might be
interpreted by us as external observers as probabilities of certain
events occurring, but the mathematical rules governing the
transformation of these distributions do not usually, by themselves,
determine what those distal events are.</p>
<p>It is worth emphasising that there is no suggestion here that
computational and semantic aspects of cognition are wholly independent.
It is likely that some symbolic expressions get their meaning fixed via
their formal computational role (plausibly, this is the case for
expressions that represent logical connectives like AND and OR). What is
being claimed is that not <em>all</em> semantic content can be
determined in this way, by formal computational role. An adequate
account of semantic aspects of cognition will need to include not only
formal relationships among computational states, but also non-formal
relationships between those computational states and distal states in
the external environment <span class="citation"
data-cites="Block86 JohnsonLaird78 Harman87">(for discussion of this
point in relation to procedural semantics or conceptual-role semantics,
see Block 1986; Harman 1987; Johnson-Laird 1978)</span>.</p>
<h2 data-number="2.3" id="theories-of-content"><span
class="header-section-number">2.3</span> Theories of content</h2>
<p>A lesson that philosophers have absorbed from this is that a
computational model will need to be supplemented by another kind of
model in order to adequately account for cognition’s semantic
properties. The project of modelling cognition should correspondingly be
seen as possessing at least two distinct branches. One branch consists
in describing the formal computational transitions or functions
associated with a cognitive process. The other branch connects the
abstract symbols or numerical values described in the first branch to
distal objects in the environment via semantic relations <span
class="citation" data-cites="Chalmers93a">(see Chalmers 2012 pp.
334–5)</span>. This two-pronged approach is most clearly laid out in the
writings of Jerry Fodor. Fodor argued that one should sharply
distinguish between one’s <em>computational theory</em> (which describes
the dynamics of abstract computational vehicles) and one’s <em>theory of
content</em> (which describes how those vehicles get associated with
specific distal representational content). It would be a mistake to
think that one’s computational theory can determine semantic properties
or vice versa <span class="citation" data-cites="Fodor98">(see Fodor
1998 pp. 9–12)</span>. (Fodor <span class="citation"
data-cites="Fodor80b">(1980)</span> makes this observation in his
response to the Chinese room argument, essentially conceding that
Searle’s conclusion about pure syntax is correct but obvious.)</p>
<p>What does a theory of content look like? Fodor argued that a good
theory of content should try to answer two questions about human
cognition: (S1) How do its computational states get their semantic
properties? (S2) Which specific semantic contents do they have? Fodor
also suggested that a theory of content suitable for fulfilling the
explanatory ambitions of computational cognitive science should be
<em>naturalistic</em>. What this last condition means is that the
answers a theory of content gives to questions S1 or S2 should not
employ semantic or intentional concepts. A theory of content should
explain how semantic content in cognition arises, and how specific
semantic contents get determined, in terms of the kinds of non-semantic
properties and processes that typically feature in the natural sciences
(e.g. physical, causal processes that occur inside the brain or the
environment). A theory of content should not attempt to answer S1 or S2
by, for example, appealing to the semantic or mental properties of
external observers or the intentional mental states of the subject
themselves <span class="citation" data-cites="Fodor90f Loewer17">(Fodor
1990 p. 32; Loewer 2017)</span>.</p>
<p>Fodor developed his own naturalistic theory of content, which he
called the ‘asymmetric dependency theory’. This theory claimed that
semantic content in cognition is determined by a complex series of
law-like relationships obtaining between current environmental stimuli
and formal symbols inside the cognitive agent <span class="citation"
data-cites="Fodor90f">(Fodor 1990)</span>. In contrast, teleological
theories of content attempt to naturalise content by appeal to
conditions that were rewarded during past learning, or that were
selected for in the cognitive agent’s evolutionary history <span
class="citation"
data-cites="Dretske95 Millikan04a Papineau87 Ryder04">(Dretske 1995;
Millikan 2004; Papineau 1987; Ryder 2004)</span>. Use-based theories of
content attempt to naturalise content by appeal to isomorphisms between
multiple computational states in the cognitive agent and states of the
world, claiming that their structural correspondence accounts for how
the computational states represent <span class="citation"
data-cites="Ramsey07 Shagrir12 Swoyer91">(Ramsey 2007; Shagrir 2012;
Swoyer 1991)</span>. Information-theoretic theories of content attempt
to naturalise content by appeal to Shannon information <span
class="citation" data-cites="Dretske81">(Dretske 1981)</span>; recent
variants of this approach propose that semantic content is determined by
whichever distal states maximise mutual information with an internal
computational state <span class="citation"
data-cites="Skyrms10 Isaac17 Usher01">(Isaac 2019; Skyrms 2010; Usher
2001)</span> – this echoes methods used by external observers in
cognitive neuroscience to assign representational content to neural
responses in the sensory or motor systems <span class="citation"
data-cites="Usher01 Eliasmith05 RollsTreves11">(Eliasmith 2005; Rolls
&amp; Treves 2011; Usher 2001)</span>. <span class="citation"
data-cites="Shea18">Shea (2018)</span> provides a powerful naturalistic
theory of content that weaves together elements of all the approaches
above and suggests that naturalistic semantic content is determined by
different types of condition in different contexts.</p>
<p>No naturalistic theory of content has yet proved entirely adequate,
and naturalising content remains more of an aspiration than an attained
solution. Among the challenges faced by current approaches are allowing
for the possibility of misrepresentation; avoiding introducing
unacceptably large amounts of indeterminacy in cognitive semantic
content; and providing a sufficiently general theory of cognitive
semantic content that will cover not only the representations involved
in perception and motor control but also more abstract representations
like DEMOCRACY, TIMETABLE, and QUARK <span class="citation"
data-cites="AdamsAizawa21 NeanderSchulte21 Shea13">(see Adams &amp;
Aizawa 2021; Neander &amp; Schulte 2021; Shea 2013)</span>.</p>
<p>Some philosophers have suggested the need for a different approach to
explaining semantic content in the computational cognitive sciences.
<span class="citation" data-cites="Egan14">Egan (2014)</span> argues
that we should assume, at least as a working hypothesis, that cognitive
semantic content <em>cannot</em> be naturalised. This is not because the
semantic content in question is determined by some magical,
non-naturalistic means, but because the way in which we ascribe semantic
content to formal computational models is an inherently messy matter
that is influenced by endless, unsystematisable pragmatic concerns <span
class="citation" data-cites="Egan03 Chomsky95a">(Chomsky 1995; Egan
2003)</span>. Semantic content determination is just not the sort of
subject matter that lends itself to description by any concise
non-intentional theory – one is unlikely to find a naturalistic theory
of semantic content for similar reasons that one is unlikely to find a
concise non-intentional theory of jokes, excuses, or anecdotes. Egan
suggests that pragmatic ascription of semantic content to computational
models nevertheless plays a residual role in scientific explanation by
functioning as an ‘intentional gloss’ that relates formal computational
models to our informal, non-scientific descriptions of behavioural
success and failure <span class="citation" data-cites="Egan10">(Egan
2010)</span>.</p>
<p>A different approach to Egan’s suggests that ascriptions of semantic
content to computational models should be treated as a kind of
idealisation or fiction within computational cognitive science <span
class="citation" data-cites="Mollo21 Sprevak13 Chirimuuta21">(Chirimuuta
forthcoming; Coelho Mollo 2021; Sprevak 2013)</span>. This builds on a
broader trend of work in philosophy of science that emphasises the value
of idealisations and fictions in all domains of scientific modelling,
from particle physics to climate science. Idealisations and fictions
should be understood not necessarily as defects in a model, but as
potentially valuable compromises that provide benefits with respect to
understanding, prediction, and control that would be unavailable from a
scientific model that is restricted to literal truth telling <span
class="citation" data-cites="Elgin17 Potochnik17 Morrison14">(Elgin
2017; Morrison 2014; Potochnik 2017)</span>.</p>
<p>While philosophers do not agree about how to answer S1 and S2, there
is near consensus that a <em>purely</em> computational theory would not
be adequate. A computational model of cognition must be supplemented by
something else – a naturalistic theory of content, an intentional gloss,
or a reinterpretation of scientific practice – that explains how the
(symbolic or numerical) states subject to computational rules gain their
semantic content. Moreover, this is widely assumed to be an <em>in
principle</em> limitation to what a computational model of cognition can
provide. It is not a shortcoming that can be remedied by moving to a new
computational model or one with more sophisticated formal rules.</p>
<h2 data-number="2.4" id="content-and-physical-computation"><span
class="header-section-number">2.4</span> Content and physical
computation</h2>
<p>The preceding discussion operated under the assumption that a
computational model is defined <em>exclusively</em> in terms of formal
rules (whether those be symbolic or numerical). This fits with one way
in which computational models are discussed in the sciences.
Mathematicians, formal linguists, and theoretical computer scientists
often define a computational model as a purely abstract, notional entity
<span class="citation" data-cites="BoolosBurgess02">(e.g. a
set-theoretic construction such as a Turing machine, Boolos et al.
2002)</span>. However, researchers in the applied sciences and in
engineering often talk about their computational models in a different
way. In these contexts, a computational model is often also tied to its
implementation in a particular physical system. Part of a researcher’s
intention in proposing such a model is to suggest that the formal
transitions in question are implemented in that specific physical
system. In the case of the computational cognitive sciences, formal
transitions are normally assumed to be implemented (at some
spatiotemporal scale) in the cognitive agent’s physical behaviour or
neural responses.</p>
<p>If a formal computation is physically implemented, the physical
states that are manipulated will necessarily stand in some non-formal
relations to distal entities in the world. Physically implemented
computations cannot help but stand in law-like causal relations to
objects in their environment, or have a history (and one that might
involve past learning and evolution). Given this, it is by no means
obvious that a <em>physically implemented</em> computation, unlike a
purely formal abstract computation, is silent about, or does not
determine, assignment of semantic content. Understanding whether and how
physical implementation relates to semantic content is a substantial
question and one that is distinct from those considered above <span
class="citation"
data-cites="Dewhurst18 Lee18 Mollo18 Piccinini15 Rescorla13 Sprevak08a Shagrir18">(Coelho
Mollo 2018; for various proposals about the relationship between
physically implemented computation and semantic content, see Dewhurst
2018; Lee 2018; Piccinini 2015 pp. 26–50; Rescorla 2013; Shagrir 2020;
Sprevak 2010)</span>. At the moment, there is no consensus among
philosophers about whether, and to what extent, physical implementation
constrains the semantics of a computation’s states. Consequently, it is
worth bearing in mind that Searle’s observation that ‘syntax is not
sufficient for semantics’, even if true for the purely formal
computations that he had in mind, may not apply to the physically
implemented computations proposed in many areas of the computational
cognitive sciences <span class="citation"
data-cites="Boden89 Chalmers96 Dennett87a">(see Boden 1989; Chalmers
1996 pp. 326–7; Dennett 1987 pp. 323–6)</span></p>
<h1 data-number="3" id="phenomenal-consciousness-the-hard-problem"><span
class="header-section-number">3</span> Phenomenal consciousness – The
hard problem</h1>
<p>‘Consciousness’ may refer to many different kinds of mental
phenomena, including sleep and wakefulness, self-consciousness,
reportability, information integration, and allocation of attention
<span class="citation" data-cites="Gulick18">(see Gulick 2018 for a
survey)</span>. This section focuses exclusively on a ‘no go’ argument
concerning <em>phenomenal consciousness</em>. ‘Phenomenal consciousness’
refers to the subjective, qualitative feelings that accompany some
aspects of cognition. When you touch a piece of silk, taste a raspberry,
or hear the song of a blackbird, over and above any processes of
classification, judgement, report, attentional shift, control of
behaviour, and planning, you also undergo subjective sensations. There
is something it <em>feels like</em> to do these things. Some
philosophers reserve the term ‘qualia’ to refer to these feelings <span
class="citation" data-cites="Tye18">(Tye 2018)</span>. The <em>hard
problem</em> of consciousness is to explain why phenomenal feelings
accompany certain aspects of cognition and to account for their
distribution across our cognitive life <span class="citation"
data-cites="Chalmers96 Chalmers10b">(Chalmers 1996 pp. 3–31;
2010a)</span>.</p>
<h2 data-number="3.1"
id="the-conceivability-argument-against-physicalism"><span
class="header-section-number">3.1</span> The conceivability argument
against physicalism</h2>
<p>The conceivability argument against physicalism is a ‘no go’ argument
phrased in terms of the conceivability of a philosophical zombie. A
philosophical zombie is a hypothetical being who is a physical duplicate
of a human and who lives in a world that is a physical duplicate of our
universe – a world with the same physical laws and the same instances of
physical properties. The difference between our world and the zombie
world is that the agents in the zombie world either lack conscious
experience or have a different distribution of phenomenal experiences
across their mental life from our own. A zombie’s cognitive processes
occur ‘in the dark’ or they are accompanied by different phenomenal
experiences from our own (e.g. it might experience the qualitative
feeling we associate with tasting raspberries when it tastes blueberries
and vice versa).</p>
<p>It is irrelevant to the conceivability argument whether a
philosophical zombie could come into existence in our world, has ever
existed, or is ever likely to exist. What matters is only whether one
can coherently <em>conceive</em> of such a being. Can one imagine a
physical duplicate of our world where a counterpart of a human either
lacks phenomenal consciousness or has a different distribution of
phenomenal experiences from one’s own? Many philosophers have argued
that this is indeed conceivable <span class="citation"
data-cites="Chalmers96 Kripke80 Nagel74">(Chalmers 1996 pp. 96–7; Kripke
1980 pp. 144–55; Nagel 1974)</span>. By this, they don’t mean that
zombies could exist in our world, or that we should entertain doubts
about whether other humans are zombies. What they mean is that the
<em>idea</em> of a zombie is a coherent one – it does not contain a
contradiction; it is unlike the idea of a married bachelor or the
highest prime number.</p>
<p>The next step in the conceivability argument is to say that our
ability to conceive of a scenario is a reliable guide to whether it is
possible. If a world in which zombies exist is conceivable, then we
should believe, pending evidence to the contrary, that it corresponds to
a genuine possibility. However, if a zombie world is possible, then the
distribution of physical properties and physical laws could be exactly
as it is in our world and the beings of that world either lack
phenomenal experience or have different phenomenal experiences from our
own. That means that in <em>our</em> world there must be some additional
ingredient, over and above the physical facts, that is responsible for
the existence and distribution of our phenomenal experiences. Something
other than the physical laws and physical properties must explain the
difference between our world and a zombie world. Our phenomenal
consciousness cannot be determined only by the physical facts because
those facts also hold in the zombie world. Advocates of the
conceivability argument conclude that a theory of consciousness that
appeals exclusively to physical facts is unable to explain the existence
and distribution of our phenomenal experiences <span class="citation"
data-cites="Chalmers96 Chalmers10a">(Chalmers 1996 pp. 93–171;
2010b)</span>.</p>
<p>According to the conceivability argument, a physicalist theory cannot
answer the following questions: (C1) How does our phenomenal conscious
experience arise at all? (C2) Why are our phenomenal conscious
experiences distributed in the way they are across our mental life? No
matter which physical facts one cites, none adequately answer C1 or C2
because the same physical facts could have obtained and those conscious
experiences be absent or different, as they are in a zombie world. This
raises the question of what – if not the totality of physical facts – is
responsible for the existence and distribution of our phenomenal
experiences. Advocates of the conceivability argument have various
suggestions at this point, all of which involve expanding or revising
our current scientific ontology. The focus of this chapter will not be
on those options, but only on the negative point that phenomenal
consciousness is somehow out of bounds for current approaches to
modelling cognition <span class="citation" data-cites="Chalmers10c">(see
Chalmers 2010c pp. 126–37, for a survey of non-physicalist
options)</span>.</p>
<h2 data-number="3.2"
id="the-conceivability-argument-against-computational-functionalism"><span
class="header-section-number">3.2</span> The conceivability argument
against computational functionalism</h2>
<p>The conceivability argument against physicalism may be modified to
generate a ‘no go’ argument against computational accounts of phenomenal
consciousness.</p>
<p>The primary consideration here is that a hypothetical zombie who is
our <em>computational</em> duplicate seems to be conceivable. This is a
being who performs exactly the same computation as we do but who either
lacks conscious experience or has a different distribution of conscious
experiences from our own. Similar reasoning to justify both the
conceivability and possibility of such a being applies as in the case of
the original conceivability argument against physicalism. It seems
possible to imagine a being implementing any computation one chooses, or
computing any function, and for this to fail to be accompanied by a
phenomenal experience, or for it to be accompanied by a phenomenal
experience different from our own. No matter how complex the rules of a
computation, nothing about it seems to <em>necessitate</em> the
existence or distribution of specific subjective experiences. One might
imagine a silicon or clockwork device functioning as a computational
duplicate of a human – undergoing the same computational transitions –
but its cognitive life remaining ‘all dark’ inside, or being accompanied
by different subjective experiences from our own <span class="citation"
data-cites="Block78 Maudlin89 Dennett78d">(for analysis of such thought
experiments, see Block 1978; Dennett 1978; Maudlin 1989)</span>. As with
the original conceivability argument, it does not matter whether a
computational zombie could exist in our world; what matters is only
whether a world with such a being is conceivable.</p>
<p>A separate consideration is that the original conceivability argument
appears to entail a ‘no go’ conclusion concerning any computational
model of consciousness that has a physical implementation <span
class="citation" data-cites="Chalmers96">(Chalmers 1996 p. 95)</span>.
Plausibly, any world that is a physical duplicate of our world is a
world that is also a duplicate in terms of the physical computations
that are performed. It seems reasonable to assume that the physical
facts about a world fix which physical computations occur in that world.
According to the original conceivability argument, a world that is a
physical duplicate of ours could be one in which there is no
consciousness or consciousness is distributed differently. Putting these
two claims together, a world that is a duplicate of ours in terms of the
physical computations performed could be one in which phenomenal
consciousness is absent or differently distributed. Hence, in our world
there must be some extra factor, over and above any physical
computations, that explains the existence and distribution of our
phenomenal experiences. A scientific model that appeals only to physical
computations – which are shared with our zombie counterparts – would be
unable to explain the existence and distribution of our phenomenal
experiences.</p>
<p>It is worth stressing that the conceivability argument places no
barrier against a computational or physical model explaining access
consciousness. ‘Access consciousness’ refers to the aspects of
consciousness associated with reportability and information sharing:
storage of information in working memory, information sharing across
various processes of planning, reporting, control of action, decision
making, and so on <span class="citation"
data-cites="Block90b Block07">(Block 1990, 2007)</span>. <span
class="citation" data-cites="Baars88">Baars (1988)</span> proposed
Global Workspace Theory (GWT) as a way in which information from
different cognitive processes comes together. Dehaene and colleagues
developed GWT and provided a possible neural implementation <span
class="citation"
data-cites="DehaeneChangeux11 DehaeneChangeux04 DehaeneChangeux06">(Dehaene
et al. 2006; Dehaene &amp; Changeux 2004, 2011)</span>. A theory of this
kind might be able to account for how and why certain pieces of
information get shared and play a greater role in driving thought,
action, and report. However, advocates of the conceivability argument
claim that a model of access consciousness cannot explain phenomenal
consciousness. Following similar reasoning to that described in the
previous section, they argue that one can conceive of a system having
access consciousness, but it still lacking phenomenal consciousness or
having a different distribution of phenomenal experience to our own.
Access consciousness does not necessitate the occurrence of phenomenal
feelings <span class="citation" data-cites="CohenDennett11">(for a
contrary view, see Cohen &amp; Dennett 2011)</span>. For these thinkers,
explaining access consciousness is classified under the heading of an
‘easy problem’ of consciousness <span class="citation"
data-cites="Chalmers10b">(Chalmers 2010a)</span>.</p>
<h2 data-number="3.3" id="naturalistic-dualism"><span
class="header-section-number">3.3</span> Naturalistic dualism</h2>
<p>It is important to understand the extent of the intended ‘no go’
claim about phenomenal consciousness. What is claimed is that
<em>solving the hard problem</em> is beyond the ability of a physical or
computational model of consciousness. This does not mean, however, that
a physical or computational account can tell us nothing about phenomenal
consciousness. Chalmers <span class="citation"
data-cites="Chalmers10b Chalmers10d">(2010a, 2010d)</span> argues that a
computational or physical model can, for example, tell us a great deal
about <em>correlations</em> between physical/computational states and
our phenomenal experiences. The conceivability argument does not deny
that such correlations exist, and measurement of brain activity shows
ample evidence of correlations between brain states and phenomenal
experience. Describing and systematising these correlations may have
considerable value to science in terms of allowing us to categorise,
predict, and control our phenomenal states. Such a model cannot,
however, explain why phenomenal experience occurs, for it cannot rule
out the possibility that the same physical or computational states could
occur without any conscious accompaniment.</p>
<p>An analogy might help to clarify this point. Suppose that one were to
begin a correlational study of the phenomena of lightning and thunder.
One might build a statistical model that captures the relationship
between observations of the two phenomena. In a similar fashion, one
might engage in a correlational study of brain states and phenomenally
conscious states and attempt to capture their relationship. In both
cases, something would be missing from the model that is produced. What
would be missing is an understanding of how and why the two variables
are linked. Lightning typically co-occurs with thunder, but not always,
and no pattern of lightning <em>necessitates</em> an observation of
thunder (atmospheric conditions might cause sound waves to be refracted
or deadened before they reach the observer). This gap in the model can
be rectified by introducing further physical variables
(e.g. distributions of electrical charges in the air, measurements of
air density and temperature). In an enlarged, more detailed, physical
model, it should be possible to explain why observations of lightning
are correlated with observations of thunder, and how and why such
correlations might fail to obtain. In the case of phenomenal
consciousness, the conceivability argument claims to show that this kind
of remedy is not available. The ‘explanatory gap’ between the two
variables cannot be filled by introducing extra physical variables into
one’s model. No matter how many physical variables one adds, the model
will still not entail the occurrence of phenomenal experiences – for,
according to the conceivability argument, all these physical variables
could be the same and the consciousness experience be absent or
different. A physical/computational model of consciousness can provide
us with a description of the correlates of consciousness, but it cannot
provide an explanation of why those correlates are accompanied by
phenomenal experience.</p>
<h2 data-number="3.4" id="eliminativism-and-related-replies"><span
class="header-section-number">3.4</span> Eliminativism and related
replies</h2>
<p>Not all philosophers accept the reasoning behind the conceivability
argument. Dennett argues that one can easily be misled by ‘intuition
pumps’ like zombie thought experiments. These can work on our
imagination like viewing a picture by M.C. Escher: we appear to see
something new and remarkable, but only because certain considerations
have been omitted or played up and we have failed to spot some hidden
inconsistency in the imagined scenario. Dennett suggests that a more
reasonable conclusion to draw is not that phenomenal consciousness is a
‘no go’ domain for computational modelling of cognition but that the
project of trying, from the armchair, to set a limit on what a
physical/computational model can and cannot explain is deeply
misconceived <span class="citation" data-cites="Dennett13">(Dennett
2013)</span>. For all we know, a truly thorough, mature
conceptualisation of a physical or computational duplicate of our world,
imagined down to the smallest detail, would rule out the possibility
that there could be zombies <span class="citation"
data-cites="Dennett01c Dennett95m">(Dennett 1995, 2001)</span>.</p>
<p>Dennett’s remarks about the reliability of our intuitions about
zombies may dampen one’s confidence in the ‘no go’ argument. However,
this by itself does not block the argument. In order to do this, Dennett
also commits to the more speculative, positive claim that <em>if</em> we
were to successfully wrap our heads around some future correct
computational model of consciousness, then we would see that it
<em>must</em> bring all aspects of consciousness along with it.
Advocates of the conceivability argument, while typically open to the
idea that zombie intuitions are not apodictically certain (we might be
deluding ourselves about the conceivability of a zombie world), tend to
pour scorn on this latter contention. No matter how complex a
computational model is, they say, it simply is not clear how it could
entail that specific conscious experiences occur <span class="citation"
data-cites="Strawson94">(Strawson 2010)</span>. The idea that, somewhere
in the space of all possible computational models, some model exists
that entails conscious experience is, according to these critics, pure
moonshine or physicalist dogma <span class="citation"
data-cites="Strawson18">(Strawson 2018)</span>.</p>
<p>A position one might be driven towards, and which Dennett defends in
<em>Consciousness Explained</em> <span class="citation"
data-cites="Dennett91">(1991)</span>, is that certain aspects of
consciousness – namely, the first-person felt aspects targeted by zombie
thought experiments – are not real. This amounts to a form of
eliminativism about phenomenal consciousness <span class="citation"
data-cites="IrvineSprevak20">(Irvine &amp; Sprevak 2020)</span>. Such
positions face a heavy intuitive burden. The existence and character of
our feelings of phenomenal consciousness seem to be among the things
about which we are most certain. Denying these subjective ‘data’, which
are accessible to anyone via introspection, may strike one as
unacceptable. Nevertheless, past scientific theories have prompted us to
abandon other seemingly secure assumptions about the world. If it can be
shown that when we introspect on our experience we are mistaken, then
perhaps eliminativism can be defended. The potential benefits of
eliminativism about phenomenal consciousness are considerable: the hard
problem of consciousness and the challenge posed by the conceivability
argument would dissolve. If there is no phenomenal consciousness, then
there is nothing for a computational model to explain.</p>
<p>Unfortunately, in addition to the difficulty just mentioned, a
further problem faces eliminativist accounts. This is to explain how the
(false) data we have about the existence and character of our phenomenal
consciousness arise in the first place. This is the so-called
<em>illusion problem</em> <span class="citation"
data-cites="Frankish16a">(Frankish 2016)</span>. Some researchers claim
that our impression that we have phenomenal consciousness is caused by
misfiring of mechanisms of our internal information processing and self
report <span class="citation"
data-cites="Frankish16a Clark00 Dennett91 Graziano16">(Clark 2000;
Dennett 1991; Frankish 2016; Graziano 2016)</span>. However, such
accounts tend to explain only why we <em>believe</em> or <em>act as
if</em> we have phenomenal consciousness. It is not clear how the
hypothesised mechanisms generate the <em>felt</em> first-person illusion
of consciousness <span class="citation"
data-cites="Chalmers96">(Chalmers 1996 pp. 184–91)</span>. In other
words, it is not clear how unreliable introspective mechanisms could
generate the false impression of phenomenal consciousness, any more than
reliable introspective mechanisms could generate the true impression of
phenomenal consciousness. The challenge that an eliminativist faces is
to show that the illusion problem is easier to solve by computational or
physical means than the hard problem of consciousness <span
class="citation" data-cites="Prinz16">(see Prinz 2016)</span>.</p>
<h1 data-number="4" id="central-reasoning-the-frame-problem"><span
class="header-section-number">4</span> Central reasoning – The frame
problem</h1>
<p>A third major target for philosophical ‘no go’ arguments is
<em>central reasoning</em>. This concerns our ability to engage in
reliable, general-purpose reasoning over a large and open-ended set of
representations, including our common-sense understanding of the world.
Modelling human-level central reasoning is closely tied to the problem
of creating a machine with artificial general intelligence. Current AI
systems tend to function only within relatively constrained problem
domains (e.g. detecting credit card fraud, recognising faces, winning at
Go). They generally perform poorly, or not at all, if the nature of
their problem changes, or if relevant contextual or background
assumptions change <span class="citation"
data-cites="MarcusDavis19 LakeUllman17">(Lake et al. 2017; Marcus &amp;
Davis 2019)</span>. In contrast, humans are relatively robust and
flexible general-purpose reasoners. They can rapidly switch between
different tasks without significant interference or relearning, they can
deploy relevant information across tasks, and they tend to be aware of
how their reasoning should be adjusted when background assumptions and
context change.</p>
<p>Small fragments of human-level central reasoning have been
computationally modelled using various logics, heuristics, and other
formalisms <span class="citation"
data-cites="McCarthy90 NewellSimon72 DavisMorgenstern04 Anderson07 GigerenzerToddABC99">(J.
R. Anderson 2007; Davis &amp; Morgenstern 2004; Gigerenzer et al. 1999;
e.g. McCarthy 1990; Newell &amp; Simon 1972)</span>. However, modelling
human-level central reasoning in full – in particular, accounting for
its flexibility, reliability, and deep common-sense knowledge base –
remains an unsolved problem. Philosophers have attempted to argue that
this lacuna is no accident, but arises because central reasoning is in a
certain respect a ‘no go’ area for computational accounts of
cognition.</p>
<h2 data-number="4.1" id="the-frame-problem"><span
class="header-section-number">4.1</span> The frame problem</h2>
<p>Philosophers often describe their ‘no go’ arguments about central
reasoning as instances of the frame problem in AI. This can be
misleading as ‘the frame problem’ refers to a more narrowly defined
problem specific to logic-based approaches to reasoning in AI. The frame
problem in AI concerns how a logic-based reasoner should represent the
effects of actions without having to represent all of an action’s
non-effects <span class="citation"
data-cites="McCarthyHayes69">(McCarthy &amp; Hayes 1969)</span>. Few
actions change every property in the world – eating a sandwich does not
(normally) change the location of Australia. However, the information
that <em>Eat(Sandwich)</em> does not change <em>Position(Australia)</em>
is not a logical truth but something that needs to be encoded somehow,
either explicitly or implicitly, in the system’s knowledge base.
Introducing this kind of ‘no change’ information in the form of extra
axioms that state every non-effect of every action – ‘frame axioms’ – is
unworkable. As the number of actions (<span
class="math inline">\(N\)</span>) and properties (<span
class="math inline">\(M\)</span>) increases, the system would need to
store approximately <span class="math inline">\(NM\)</span> axioms. The
frame problem in AI concerns how to encode this ‘no change’ information
more efficiently. The challenge is normally interpreted as the problem
of formalising a general inference rule that an action does not change a
property unless the reasoning system has evidence to the contrary.
Formalising this rule poses numerous technical hurdles, and it has
stimulated important developments in non-monotonic logics, but it is
widely regarded as a solved issue within logic-based AI <span
class="citation"
data-cites="Shanahan97 Lifschitz15 Shanahan16">(Lifschitz 2015; Shanahan
1997, 2016)</span>.</p>
<p>A number of philosophers, inspired by the original frame problem,
have suggested that there are broader and more fundamental difficulties
with explaining human-level central reasoning with computation. They do
not, however, agree about the precise nature of these difficulties,
their scope, or their severity. A number of proposals – confusingly also
called the ‘frame problem’ – can be found in <span class="citation"
data-cites="Pylyshyn87">Pylyshyn (1987)</span> and <span
class="citation" data-cites="FordPylyshyn96">Ford &amp; Pylyshyn
(1996)</span>. Useful critical reflections on this work are found in
<span class="citation" data-cites="Chow13">Chow (2013)</span>, <span
class="citation" data-cites="Samuels10">Samuels (2010)</span>, <span
class="citation" data-cites="Shanahan16">Shanahan (2016)</span>, and
<span class="citation" data-cites="Wheeler08a">Wheeler (2008)</span>.
The remainder of this section summarises two attempts by philosophers to
pinpoint the problem with modelling human-level central reasoning.</p>
<h2 data-number="4.2" id="dreyfuss-argument"><span
class="header-section-number">4.2</span> Dreyfus’s argument</h2>
<p>The first argument was developed by Hubert Dreyfus <span
class="citation" data-cites="Dreyfus72 Dreyfus92">(1972, 1992)</span>.
Dreyfus initially targeted classical, symbolic computational approaches
to central reasoning. The sort of computational model he had in mind was
exemplified by Douglas Lenat’s Cyc project. This project aimed to encode
all of human common-sense knowledge in a giant symbolic database of
representations over which a logic-based system could run queries to
produce general-purpose reasoning <span class="citation"
data-cites="LenatFeigenbaum91">(Lenat &amp; Feigenbaum 1991)</span>.
Dreyfus argued that no model of this kind could capture human-level
general-purpose reasoning. This was for two main reasons.</p>
<p>First, it would be impossible to encode all of human common-sense
knowledge with a single symbolic database. Drawing on ideas from
Heidegger, Merleau-Ponty, and the later Wittgenstein, Dreyfus suggested
that any attempt to formalise human common-sense knowledge will fail to
capture a background of implicit assumptions, significances, and skills
that are required in order for that formalisation to be used
effectively. These philosophers defended the idea that our common-sense
knowledge presupposes a rich background of implicit know-how. Fragments
of this know-how can be explicitly articulated in a set of symbolic
rules, but not all of it at once. Attempts to formalise all of human
common-sense knowledge in one symbolic system will, for various reasons,
leave gaps, and attempts to fill those gaps will introduce further gaps
elsewhere. The goal of formalising the entirety of human common-sense
knowledge in symbolic terms will run into the same kinds of problems
that caused Husserl’s twentieth-century phenomenological attempt to
describe explicitly all the principles and beliefs that underlie human
intelligent behaviour to fail <span class="citation"
data-cites="DreyfusDreyfus88 Dreyfus91">(Dreyfus 1991; Dreyfus &amp;
Dreyfus 1988)</span>. <span class="citation"
data-cites="Searle92">(Searle makes a similar point regarding what he
calls the ‘Background’ in Searle 1992 pp. 175–96.)</span></p>
<p>Second, even if human common-sense knowledge could be encoded in a
single symbolic database, the computational system would find itself
unable to use that information efficiently. Potentially, any piece of
information from the database could be relevant to any task. Without
knowledge about the specific problems the system was facing, there would
be no way to screen off any piece of knowledge as irrelevant. Because
the database would be so large, the system would not be able to consider
every piece of information it had in turn and explore all its potential
implications. How, then, would it select which symbolic representations
were relevant to a specific problem at hand? In order to answer this, it
would need to know which specific problem it was facing – about its
context and which background assumptions it was safe to make. But how
would it know this? Unless the programmer had told it the answer, the
only way would seem to be to deploy its database of common-sense
knowledge to infer the type of situation it was in and the nature of the
problem it now faced. But that leads one back to the original question
of how it was to use information in that database efficiently. In order
to deploy its vast database efficiently, the system would have to know
which pieces of knowledge were relevant to the problem at hand. In order
to know that, it would have to know what that problem was. But in order
to know this, it would need to be able to use its database of knowledge
efficiently, which it cannot do because it would not know which pieces
of knowledge were relevant. Dreyfus concludes that any computational
model that attempts to perform central reasoning would be trapped in an
endless loop of attempting to determine context and relevance <span
class="citation" data-cites="Dreyfus92">(Dreyfus 1992 pp.
206–24)</span>.</p>
<p>Dreyfus claimed that these two problems affect any classical,
symbolic computational attempt to model human-level general-purpose
reasoning. In later work, Dreyfus attempted to extend his ‘no go’
argument to other kinds of computational model – connectionist networks
trained under supervised learning and reinforcement learning. He
cautiously concluded that although these models might avoid the first
problem (connectionist networks are not committed to formalising
knowledge with symbolic representations), they are still affected by
something similar to the second problem. Our current methods for
training connectionist networks and reinforcement-learning systems tend
to tune these models to relatively narrow problem domains. Such systems
have not shown the flexibility to reproduce human-level general-purpose
central reasoning; they tend to be relatively brittle <span
class="citation" data-cites="Dreyfus92 Dreyfus07">(Dreyfus 1992 pp.
xxxiii–xliii; 2007)</span>. It is worth noting that the character of
Dreyfus’s argument changes here from that of an in principle ‘no go’ (it
is <em>impossible</em> for any classical, symbolic computational model
to account for central reasoning) to more of a hedged prediction based
on what has been achieved by machine-learning methods to date (we do not
– yet – know of a method to train a connectionist network to exhibit
human-level flexibility in general-purpose reasoning).</p>
<p>Dreyfus proposed that central reasoning should be modelled using a
dynamical, embodied approach to cognition that has come to be known as
‘Heideggerian AI’. The details of such a view are unclear, but broadly
speaking the idea is that the relevant inferential skills and embodied
knowledge for general-purpose reasoning are coordinated and arranged
such that they are solicited by the external situation and current
context to bring certain subsets of knowledge to the fore. The resources
needed to determine relevance therefore do not lie in a computation
inside our heads, but are somehow encoded in the dynamical relationship
between ourselves and the external world <span class="citation"
data-cites="Haugeland98">(Haugeland 1998)</span>. Wheeler <span
class="citation" data-cites="Wheeler05 Wheeler08a">(2005, 2008)</span>
develops a version of Heideggerian AI that takes inspiration from the
situated robotics movement <span class="citation"
data-cites="Brooks91">(Brooks 1991)</span>. <span class="citation"
data-cites="Dreyfus07">Dreyfus (2007)</span> argues for an alternative
approach based around the neurodynamics work of <span class="citation"
data-cites="Freeman00">Freeman (2000)</span>. Neither has yet produced a
working model that performs appreciably better at modelling human-like
context sensitivity than more conventional computational
alternatives.</p>
<h2 data-number="4.3" id="fodors-argument"><span
class="header-section-number">4.3</span> Fodor’s argument</h2>
<p>Jerry Fodor argued that two related problems prevent a computational
model from being able to account for human-level central reasoning. He
called these the ‘globality’ problem and the ‘relevance’ problem <span
class="citation" data-cites="Fodor83 Fodor00 Fodor08">(Fodor 1983, 2000,
2008)</span>. Like Dreyfus, Fodor focused primarily on how these
problems affect classical, symbolic models of central reasoning. Fodor
believed that a non-symbolic model (e.g. a connectionist system) would
be unsuited to modelling human-level central reasoning because it cannot
account for the systematicity and compositionality that he considered
necessary features of human thought <span class="citation"
data-cites="FodorLepore92 FodorPylyshyn88 Fodor08">(Fodor 2008; for that
argument, see Fodor &amp; Lepore 1992; Fodor &amp; Pylyshyn
1988)</span>. <span class="citation" data-cites="Samuels10">(For
discussion of connectionist approaches to central reasoning, see Samuels
2010 pp. 289–90.)</span></p>
<p>The globality problem concerns how a reasoning system computes
certain epistemic properties that are relevant to general-purpose
reasoning: simplicity, centrality, and conservativeness of
representations. Fodor suggested that these properties are ‘global’, by
which he meant that they may depend on any number of the system’s other
representations. They are not features that supervene exclusively on
intrinsic properties of the individual representation of which they are
predicated. A representation might count as simple in one context – for
example, relative to one set of surrounding beliefs – but complex in
another. The simplicity of a representation is not an intrinsic property
of a representation. Hence, its simplicity cannot depend solely on a
representation’s intrinsic, local syntactic properties. Fodor claimed
that a classical computational process is sensitive <em>only</em> to the
intrinsic, local syntactic properties of the representations it
manipulates. Therefore, any central reasoning that requires sensitivity
to global properties cannot be a classical computational process.</p>
<p>Fodor’s globality argument has been roundly criticised <span
class="citation"
data-cites="LudwigSchneider08 Schneider11 Samuels10">(e.g. by Ludwig
&amp; Schneider 2008; Samuels 2010; Schneider 2011)</span>. Critics
point out that computations may be sensitive not only to the intrinsic
properties of individual representations, but also to syntactic
relationships between representations: for example, how a
representation’s local syntactic properties relate to the local
syntactic properties of other representations and how they relate to the
system’s rules of syntactic processing. The failure of an epistemic
property like simplicity to supervene on a representation’s intrinsic,
local syntactic properties does not mean that simplicity cannot be
tracked or evaluated by a computational process. Simplicity may
supervene on, and be reliably tracked by following, the syntactic
relationships between representations. <span class="citation"
data-cites="Fodor00">Fodor (2000)</span> anticipates this response,
however – he labels it M(CTM). He argues that solving the globality
problem in this way runs into his second problem.</p>
<p>The second problem arises when a reasoning system needs to make an
inference based on a large number of representations, any combination of
which may be relevant to the problem at hand. Typically, only a tiny
fraction of these representations will be relevant to the inference. The
relevance problem is to determine the membership of this fraction.
Humans tend to be good at focusing in on only those representations from
their entire belief set that are relevant to their current context or
task. But we do not know how they do this. Echoing the worries raised by
Dreyfus, Fodor says we do not know of a computational method that is
able to pare down the set of all the system’s representations to only
those relevant to the current task.</p>
<h2 data-number="4.4" id="responses-to-the-problems"><span
class="header-section-number">4.4</span> Responses to the problems</h2>
<p>Some philosophers have responded to these problems by emphasising the
role of heuristics in relevance determination. They point to the
computational methods used by Internet search engines, which, although
far from perfect, often do a decent job of returning relevant results
from very large datasets. They also stress that humans sometimes fail to
deploy relevant information or that they use irrelevant information when
reasoning <span class="citation"
data-cites="Clark02 Lormand90 Carruthers06 Samuels05 Samuels10">(Carruthers
2006; Clark 2002; Lormand 1990; Samuels 2005, 2010)</span>. These two
considerations might increase our confidence that human-level central
reasoning – and in particular the relevance problem – might be tackled
by computational means. However, it does not cut much ice unless one can
say which heuristics are used and how the observed success rate of
humans is produced. Heuristics might, at some level, inform human
central reasoning, but unless one can say precisely how they do this –
and ideally produce a working computational model that exhibits levels
of flexibility and reliability similar to those seen in human reasoning
– it is hard to say that one has solved the problem <span
class="citation" data-cites="Chow13">(see Chow 2013 pp.
315–21)</span>.</p>
<p><span class="citation" data-cites="ShanahanBaars05">Shanahan &amp;
Baars (2005)</span> and <span class="citation"
data-cites="Schneider11">Schneider (2011)</span> suggest that the issues
that Dreyfus and Fodor raise can be resolved within GWT. GWT is a
proposed large-scale computational architecture in which multiple
‘specialist’ cognitive processes compete for access to a global
workspace where central reasoning takes place. Access to the global
workspace is controlled by ‘attention-like’ processes <span
class="citation" data-cites="Baars88">(Baars 1988)</span>. <span
class="citation" data-cites="MashourRoelfsemaChangeux20">Mashour et al.
(2020)</span> and <span class="citation"
data-cites="DehaeneChangeux04">Dehaene &amp; Changeux (2004)</span>
describe a possible neural basis for GWT. <span class="citation"
data-cites="GoyalDidolkarLamb21">Goyal et al. (2021)</span> suggest GWT
as a way to enable several special-purpose AI systems to share
information and coordinate decision making. GWT is a promising
architecture, but it is unclear whether it can function as a response to
the arguments of Dreyfus and Fodor. The model does not explain the
mechanism by which information from specialist processes is regulated so
as to be relevant to the current context and the contents of the central
workspace. <span class="citation" data-cites="BaarsFranklin03">Baars
&amp; Franklin (2003)</span> suggest there is an interplay between
‘executive functions’, ‘specialist networks’, and ‘attention codelets’
that control access to the global workspace, but exactly how these
components work to track relevance is left unclear. As with the
suggestion about heuristics, GWT is not (or not yet) a worked-out
solution to the relevance-determination problem <span class="citation"
data-cites="Sprevak18a">(see Sprevak 2019 pp. 557–8)</span>.</p>
<p>A notable feature of the ‘no go’ arguments that target human-level
central reasoning is that, unlike the ‘no go’ arguments of Sections 2
and 3, they do not straightforwardly generalise across the space of all
computational models. Both Dreyfus’s and Fodor’s arguments consist in
pointing out problems with specific computational approaches to central
reasoning – primarily, with classical, symbolic models and current
connectionist and reinforcement-learning approaches. The persuasive
force of what they say against untried or as-yet unexplored
computational approaches is unclear. Sceptics might see in their
arguments evidence that central reasoning is unlikely to ever yield to a
computational approach – Dreyfus and Fodor both suggest that the track
record of failure of computational models should lead one to infer that
no future computational model will succeed. Fans of computational
modelling might respond that explaining central reasoning is an
extremely hard research problem and it should not be surprising if it
has not yet been solved by computational methods. The landscape of
as-yet untried computational methods is very large and, pending evidence
to the contrary, we should not presume that central reasoning cannot
yield to a computational model <span class="citation"
data-cites="Samuels10">(Samuels 2010 pp. 288–92)</span>.</p>
<h1 data-number="5" id="conclusion"><span
class="header-section-number">5</span> Conclusion</h1>
<p>This chapter describes a small sample of philosophical issues in the
computational cognitive sciences. Its focus has been ‘no go’ arguments
regarding three distinct aspects of human cognition: semantic content,
phenomenal consciousness, and central reasoning. One might worry that
the project of placing limits on what the computational cognitive
sciences can achieve is rash given their relatively early state of
development. But this would be to misinterpret how the ‘no go’ arguments
function. These arguments attempt to formalise objections – of different
types and different strengths – to the assumption that every aspect of
cognition can be adequately explained with computation. This need not
shut down debate on the topic, but can serve as an opening move and a
potentially helpful spur. The project bears directly on questions about
the estimated plausibility of future research programmes within the
cognitive sciences, the motivations for pursuing them, and the rationale
for devoting resources to computational versus non-computational
approaches. Such judgements cannot be avoided; they are made regularly
within the cognitive sciences. They are also best made on a considered
basis, with reasons marshalled and assessed. Philosophical work in this
area can help to systematise evidence and provide decision makers with
reason-based considerations about what challenges the computational
cognitive sciences are likely to face.</p>
<h1 class="unnumbered" id="bibliography">Bibliography</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<div id="ref-AdamsAizawa21" class="csl-entry" role="doc-biblioentry">
Adams, F., &amp; Aizawa, K. (2021). <span>‘<a
href="https://plato.stanford.edu/archives/spr2021/entries/content-causal/">Causal
theories of mental content</a>’</span>. Zalta E. N. (ed.) <em>The
<span>Stanford</span> encyclopedia of philosophy</em>, Spring 2021.
</div>
<div id="ref-Anderson07" class="csl-entry" role="doc-biblioentry">
Anderson, J. R. (2007). <em>How can the human mind occur in a physical
universe?</em> Oxford: Oxford University Press.
</div>
<div id="ref-Anderson14" class="csl-entry" role="doc-biblioentry">
Anderson, M. L. (2014). <em>After phrenology: Neural reuse and the
interactive brain</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-ApperlyButterfill09" class="csl-entry"
role="doc-biblioentry">
Apperly, I. A., &amp; Butterfill, S. A. (2009). <span>‘Do humans have
two systems to track belief and belief-like states?’</span>,
<em>Psychological Review</em>, 116: 953–70.
</div>
<div id="ref-Baars88" class="csl-entry" role="doc-biblioentry">
Baars, B. (1988). <em>A cognitive theory of consciousness</em>.
Cambridge: Cambridge University Press.
</div>
<div id="ref-BaarsFranklin03" class="csl-entry" role="doc-biblioentry">
Baars, B., &amp; Franklin, S. (2003). <span>‘How conscious experience
and working memory interact’</span>, <em>Trends in Cognitive
Sciences</em>, 7: 166–72.
</div>
<div id="ref-Barrett20" class="csl-entry" role="doc-biblioentry">
Barrett, H. C. (2020). <span>‘Towards a cognitive science of the human:
Cross-cultural approaches and their urgency’</span>, <em>Trends in
Cognitive Sciences</em>, 24: 620–38.
</div>
<div id="ref-BarrettKurzban06" class="csl-entry" role="doc-biblioentry">
Barrett, H. C., &amp; Kurzban, R. (2006). <span>‘Modularity in
cognition: Framing the debate’</span>, <em>Psychological Review</em>,
113: 628–47.
</div>
<div id="ref-Block78" class="csl-entry" role="doc-biblioentry">
Block, N. (1978). <span>‘Troubles with functionalism’</span>. Savage C.
W. (ed.) <em>Perception and cognition: Issues in the foundations of
psychology, minnesota studies in the philosophy of science</em>, Vol. 9,
pp. 261–325. University of Minnesota Press: Minneapolis.
</div>
<div id="ref-Block80" class="csl-entry" role="doc-biblioentry">
——. (1980). <span>‘What intuitions about homunculi don’t show’</span>,
<em>Behavioral and Brain Sciences</em>, 3: 425–6.
</div>
<div id="ref-Block86" class="csl-entry" role="doc-biblioentry">
——. (1986). <span>‘Advertisement for a semantics for psychology’</span>,
<em>Midwest Studies in Philosophy</em>, 10: 615–78.
</div>
<div id="ref-Block90b" class="csl-entry" role="doc-biblioentry">
——. (1990). <span>‘Consciousness and accessibility’</span>,
<em>Behavioral and Brain Sciences</em>, 13: 596–8.
</div>
<div id="ref-Block07" class="csl-entry" role="doc-biblioentry">
——. (2007). <span>‘Consciousness, accessibility, and the mesh between
psychology and neuroscience’</span>, <em>Behavioral and Brain
Sciences</em>, 30: 481–548.
</div>
<div id="ref-Boden89" class="csl-entry" role="doc-biblioentry">
Boden, M. A. (1989). <span>‘Escaping from the <span>C</span>hinese
room’</span>. <em>Artificial intelligence in psychology</em>, pp.
82–100. MIT Press: Cambridge, MA.
</div>
<div id="ref-BoolosBurgess02" class="csl-entry" role="doc-biblioentry">
Boolos, G., Burgess, J. P., &amp; Jeffrey, R. C. (2002).
<em>Computability and logic</em>., 4th ed. Cambridge: Cambridge
University Press.
</div>
<div id="ref-Brooks91" class="csl-entry" role="doc-biblioentry">
Brooks, R. A. (1991). <span>‘Intelligence without
representation’</span>, <em>Artificial Intelligence</em>, 47: 139–59.
</div>
<div id="ref-Buckner21" class="csl-entry" role="doc-biblioentry">
Buckner, C. (2021). <span>‘Black boxes or unflattering mirrors?
Comparative bias in the science of machine behaviour’</span>, <em>The
British Journal for the Philosophy of Science</em>. DOI: <a
href="https://doi.org/10.1086/714960">10.1086/714960</a>
</div>
<div id="ref-Burge86" class="csl-entry" role="doc-biblioentry">
Burge, T. (1986). <span>‘Individualism and psychology’</span>,
<em>Philosophical Review</em>, 95: 3–45.
</div>
<div id="ref-Carruthers06" class="csl-entry" role="doc-biblioentry">
Carruthers, P. (2006). <em>The architecture of the mind</em>. Oxford:
Oxford University Press.
</div>
<div id="ref-Chalmers96" class="csl-entry" role="doc-biblioentry">
Chalmers, D. J. (1996). <em>The conscious mind</em>. Oxford: Oxford
University Press.
</div>
<div id="ref-Chalmers10d" class="csl-entry" role="doc-biblioentry">
——. (2010d). <span>‘How can we construct a science of
consciousness?’</span> <em>The character of consciousness</em>, pp.
37–58. Oxford University Press.
</div>
<div id="ref-Chalmers10c" class="csl-entry" role="doc-biblioentry">
——. (2010c). <span>‘Consciousness and its place in nature’</span>.
<em>The character of consciousness</em>, pp. 103–39. Oxford University
Press.
</div>
<div id="ref-Chalmers10b" class="csl-entry" role="doc-biblioentry">
——. (2010a). <span>‘Facing up to the problem of consciousness’</span>.
<em>The character of consciousness</em>, pp. 3–34. Oxford University
Press.
</div>
<div id="ref-Chalmers10a" class="csl-entry" role="doc-biblioentry">
——. (2010b). <span>‘The two-dimensional argument against
materialism’</span>. <em>The character of consciousness</em>, pp.
141–205. Oxford University Press.
</div>
<div id="ref-Chalmers93a" class="csl-entry" role="doc-biblioentry">
——. (2012). <span>‘A computational foundation for the study of
cognition’</span>, <em>Journal of Cognitive Science</em>, 12: 323–57.
</div>
<div id="ref-Chirimuuta21" class="csl-entry" role="doc-biblioentry">
Chirimuuta, M. (forthcoming). <em>How to simplify the brain</em>.
Cambridge, MA: MIT Press.
</div>
<div id="ref-Chomsky95a" class="csl-entry" role="doc-biblioentry">
Chomsky, N. (1995). <span>‘Language and nature’</span>, <em>Mind</em>,
104: 1–61.
</div>
<div id="ref-Chow13" class="csl-entry" role="doc-biblioentry">
Chow, S. J. (2013). <span>‘What’s the problem with the frame
problem?’</span>, <em>Review of Philosophy and Psychology</em>, 4:
309–31.
</div>
<div id="ref-Clark00" class="csl-entry" role="doc-biblioentry">
Clark, A. (2000). <span>‘A case where access implies qualia?’</span>,
<em>Analysis</em>, 60: 30–8.
</div>
<div id="ref-Clark02" class="csl-entry" role="doc-biblioentry">
——. (2002). <span>‘Global abductive inference and authoritative sources,
or, how search engines can save cognitive science’</span>, <em>Cognitive
Science Quarterly</em>, 2: 115–40.
</div>
<div id="ref-Clark15" class="csl-entry" role="doc-biblioentry">
——. (2016). <em>Surfing uncertainty: Prediction, action, and the
embodied mind</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Mollo18" class="csl-entry" role="doc-biblioentry">
Coelho Mollo, D. (2018). <span>‘Functional individuation, mechanistic
implementation: The proper way of seeing the mechanistic view of
concrete computation’</span>, <em>Synthese</em>, 195: 3477–97.
</div>
<div id="ref-Mollo21" class="csl-entry" role="doc-biblioentry">
——. (2021). <span>‘Deflationary realism: Representation and idealisation
in cognitive science’</span>, <em>Mind and Language</em>, 1–19. DOI: <a
href="https://doi.org/10.1111/mila.12364">10.1111/mila.12364</a>
</div>
<div id="ref-CohenDennett11" class="csl-entry" role="doc-biblioentry">
Cohen, M. A., &amp; Dennett, D. C. (2011). <span>‘Consciousness cannot
be separated from function’</span>, <em>Trends in Cognitive
Sciences</em>, 15: 358–64.
</div>
<div id="ref-Cole20" class="csl-entry" role="doc-biblioentry">
Cole, D. (2020). <span>‘<a
href="https://plato.stanford.edu/archives/win2020/entries/chinese-room/">The
<span>C</span>hinese room argument</a>’</span>. Zalta E. N. (ed.)
<em>The <span>Stanford</span> encyclopedia of philosophy</em>, Winter
2020.
</div>
<div id="ref-ColomboHartmann17" class="csl-entry"
role="doc-biblioentry">
Colombo, M., &amp; Hartmann, S. (2017). <span>‘Bayesian cognitive
science, unification, and explanation’</span>, <em>The British Journal
for the Philosophy of Science</em>, 68: 451–84.
</div>
<div id="ref-ColomboSeries12" class="csl-entry" role="doc-biblioentry">
Colombo, M., &amp; Seriès, P. (2012). <span>‘Bayes on the brain—on
<span>B</span>ayesian modelling in neuroscience’</span>, <em>The British
Journal for the Philosophy of Science</em>, 63: 697–723.
</div>
<div id="ref-Danks14" class="csl-entry" role="doc-biblioentry">
Danks, D. (2014). <em>Unifying the mind: Cognitive representations as
graphical models</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-DavisMorgenstern04" class="csl-entry"
role="doc-biblioentry">
Davis, E., &amp; Morgenstern, L. (2004). <span>‘Introduction: Progress
in formal commonsense reasoning’</span>, <em>Artificial
Intelligence</em>, 153: 1–12.
</div>
<div id="ref-DehaeneChangeux04" class="csl-entry"
role="doc-biblioentry">
Dehaene, S., &amp; Changeux, J.-P. (2004). <span>‘Neural mechanisms for
access to consciousness’</span>. Gazzaniga M. (ed.) <em>The cognitive
neurosciences, <span>III</span></em>, pp. 1145–57. MIT Press: Cambridge,
MA.
</div>
<div id="ref-DehaeneChangeux11" class="csl-entry"
role="doc-biblioentry">
——. (2011). <span>‘Experimental and theoretical approaches to conscious
processing’</span>, <em>Neuron</em>, 70: 200–27.
</div>
<div id="ref-DehaeneChangeux06" class="csl-entry"
role="doc-biblioentry">
Dehaene, S., Changeux, J.-P., Naccache, L., Sackur, J., &amp; Sergent,
C. (2006). <span>‘Conscious, preconscious, and subliminal processing: A
testable taxonomy’</span>, <em>Trends in Cognitive Sciences</em>, 10:
204–11.
</div>
<div id="ref-Dennett78d" class="csl-entry" role="doc-biblioentry">
Dennett, D. C. (1978). <span>‘Why you can’t make a computer that feels
pain’</span>, <em>Synthese</em>, 38: 415–56.
</div>
<div id="ref-Dennett87a" class="csl-entry" role="doc-biblioentry">
——. (1987). <em>The intentional stance</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Dennett91" class="csl-entry" role="doc-biblioentry">
——. (1991). <em>Consciousness explained</em>. Boston, MA: Little, Brown
&amp; Company.
</div>
<div id="ref-Dennett95m" class="csl-entry" role="doc-biblioentry">
——. (1995). <span>‘The unimagined preposterousness of zombies’</span>,
<em>Journal of Consciousness Studies</em>, 2: 322–6.
</div>
<div id="ref-Dennett01c" class="csl-entry" role="doc-biblioentry">
——. (2001). <span>‘The zombic hunch: Extinction of an
intuition?’</span>, <em>Royal Institute of Philosophy Supplement</em>,
48: 27–43.
</div>
<div id="ref-Dennett13" class="csl-entry" role="doc-biblioentry">
——. (2013). <em>Intuition pumps and other tools for thinking</em>. New
York, NY: W. W. Norton; Company.
</div>
<div id="ref-Dennett18" class="csl-entry" role="doc-biblioentry">
——. (2017). <em>From bacteria to <span>B</span>ach and back: The
evolution of minds</em>. New York, NY: W. W. Norton; Company.
</div>
<div id="ref-Dewhurst18" class="csl-entry" role="doc-biblioentry">
Dewhurst, J. (2018). <span>‘Individuation without
representation’</span>, <em>The British Journal for the Philosophy of
Science</em>, 69: 103–16.
</div>
<div id="ref-Dretske81" class="csl-entry" role="doc-biblioentry">
Dretske, F. (1981). <em>Knowledge and the flow of information</em>.
Cambridge, MA: MIT Press.
</div>
<div id="ref-Dretske95" class="csl-entry" role="doc-biblioentry">
——. (1995). <em>Naturalizing the mind</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Dreyfus72" class="csl-entry" role="doc-biblioentry">
Dreyfus, H. L. (1972). <em>What computers can’t do</em>. New York, NY:
Harper &amp; Row.
</div>
<div id="ref-Dreyfus91" class="csl-entry" role="doc-biblioentry">
——. (1991). <em>Being-in-the-world: A commentary on
<span>H</span>eidegger’s <em>being and time</em>, <span>Division
I</span></em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Dreyfus92" class="csl-entry" role="doc-biblioentry">
——. (1992). <em>What computers still can’t do</em>. Cambridge, MA: MIT
Press.
</div>
<div id="ref-Dreyfus07" class="csl-entry" role="doc-biblioentry">
——. (2007). <span>‘Why <span>H</span>eideggerian <span>AI</span> failed
and how fixing it would require making it more
<span>H</span>eideggerian’</span>, <em>Artificial Intelligence</em>,
171: 1137–60.
</div>
<div id="ref-DreyfusDreyfus88" class="csl-entry" role="doc-biblioentry">
Dreyfus, H. L., &amp; Dreyfus, S. E. (1988). <span>‘Making a mind versus
modeling the brain: Artificial intelligence back at a
branchpoint’</span>, <em>Daedalus</em>, 117: 15–44.
</div>
<div id="ref-Egan03" class="csl-entry" role="doc-biblioentry">
Egan, F. (2003). <span>‘Naturalistic inquiry: Where does mental
representation fit in?’</span> Antony L. M. &amp; Hornstein N. (eds)
<em>Chomsky and his critics</em>. Blackwell: Oxford.
</div>
<div id="ref-Egan10" class="csl-entry" role="doc-biblioentry">
——. (2010). <span>‘Computational models: A modest role for
content’</span>, <em>Studies in History and Philosophy of Science</em>,
41: 253–9.
</div>
<div id="ref-Egan14" class="csl-entry" role="doc-biblioentry">
——. (2014). <span>‘How to think about mental content’</span>,
<em>Philosophical Studies</em>, 170: 115–35.
</div>
<div id="ref-Elgin17" class="csl-entry" role="doc-biblioentry">
Elgin, C. Z. (2017). <em>True enough</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Eliasmith03" class="csl-entry" role="doc-biblioentry">
Eliasmith, C. (2003). <span>‘Moving beyond metaphors: Understanding the
mind for what it is’</span>, <em>The Journal of Philosophy</em>, 10:
493–520.
</div>
<div id="ref-Eliasmith05" class="csl-entry" role="doc-biblioentry">
——. (2005). <span>‘Neurosemantics and categories’</span>. Cohen H. &amp;
Lefebvre C. (eds) <em>Handbook of categorization in cognitive
science</em>, pp. 1035–55. Elsevier: Amsterdam.
</div>
<div id="ref-Eliasmith13" class="csl-entry" role="doc-biblioentry">
——. (2013). <em>How to build a brain: A neural architecture for
biological cognition</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Fodor78" class="csl-entry" role="doc-biblioentry">
Fodor, J. A. (1978). <span>‘Tom <span>S</span>wift and his procedural
grandmother’</span>, <em>Cognition</em>, 6: 229–47.
</div>
<div id="ref-Fodor80b" class="csl-entry" role="doc-biblioentry">
——. (1980). <span>‘Searle on what only brains can do’</span>,
<em>Behavioral and Brain Sciences</em>, 3: 431–2.
</div>
<div id="ref-Fodor83" class="csl-entry" role="doc-biblioentry">
——. (1983). <em>The modularity of mind</em>. MIT Press.
</div>
<div id="ref-Fodor90f" class="csl-entry" role="doc-biblioentry">
——. (1990). <em>A theory of content and other essays</em>. Cambridge,
MA: MIT Press.
</div>
<div id="ref-Fodor98" class="csl-entry" role="doc-biblioentry">
——. (1998). <em>Concepts</em>. Oxford: Blackwell.
</div>
<div id="ref-Fodor00" class="csl-entry" role="doc-biblioentry">
——. (2000). <em>The mind doesn’t work that way</em>. Cambridge, MA: MIT
Press.
</div>
<div id="ref-Fodor08" class="csl-entry" role="doc-biblioentry">
——. (2008). <em><span>LOT2</span>: The language of thought
revisited</em>. Oxford: Oxford University Press.
</div>
<div id="ref-FodorLepore92" class="csl-entry" role="doc-biblioentry">
Fodor, J. A., &amp; Lepore, E. (1992). <em>Holism: A shopper’s
guide</em>. Oxford: Blackwell.
</div>
<div id="ref-FodorPylyshyn88" class="csl-entry" role="doc-biblioentry">
Fodor, J. A., &amp; Pylyshyn, Z. W. (1988). <span>‘Connectionism and
cognitive architecture’</span>, <em>Cognition</em>, 28: 3–71.
</div>
<div id="ref-FordPylyshyn96" class="csl-entry" role="doc-biblioentry">
Ford, K. M., &amp; Pylyshyn, Z. W. (Eds). (1996). <em>The robot’s
dilemma revisited</em>. Norwood, NJ: Ablex.
</div>
<div id="ref-Frankish16a" class="csl-entry" role="doc-biblioentry">
Frankish, K. (2016). <span>‘Illusionism as a theory of
consciousness’</span>, <em>Journal of Consciousness Studies</em>, 23:
11–39.
</div>
<div id="ref-Freeman00" class="csl-entry" role="doc-biblioentry">
Freeman, W. J. (2000). <em>How brains make up their minds</em>. New
York, NY: Columbia University Press.
</div>
<div id="ref-Gelder95" class="csl-entry" role="doc-biblioentry">
Gelder, T. van. (1995). <span>‘What might cognition be, if not
computation?’</span>, <em>The Journal of Philosophy</em>, 91: 345–81.
</div>
<div id="ref-GigerenzerToddABC99" class="csl-entry"
role="doc-biblioentry">
Gigerenzer, G., Todd, P. M., &amp; ABC Research Group, the (Eds).
(1999). <em>Simple heuristics that make us smart</em>. New York, NY:
Oxford University Press.
</div>
<div id="ref-Glymour01" class="csl-entry" role="doc-biblioentry">
Glymour, C. (2001). <em>The mind’s arrows: Bayes nets and graphical
causal models in psychology</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-GodfreySmith16" class="csl-entry" role="doc-biblioentry">
Godfrey-Smith, P. (2016). <span>‘Mind, matter, and metabolism’</span>,
<em>The Journal of Philosophy</em>, 113: 481–506.
</div>
<div id="ref-GoyalDidolkarLamb21" class="csl-entry"
role="doc-biblioentry">
Goyal, A., Didolkar, A., Lamb, A., Badola, K., Ke, N. R., Rahaman, N.,
Binas, J., et al. (2021). <em>Coordination among neural modules through
a shared global workspace</em>.
</div>
<div id="ref-Graziano16" class="csl-entry" role="doc-biblioentry">
Graziano, M. S. A. (2016). <span>‘Consciousness engineered’</span>,
<em>Journal of Consciousness Studies</em>, 23: 98–115.
</div>
<div id="ref-Gulick18" class="csl-entry" role="doc-biblioentry">
Gulick, R. van. (2018). <span>‘<a
href="https://plato.stanford.edu/archives/spr2018/entries/consciousness/">Consciousness</a>’</span>.
Zalta E. N. (ed.) <em>The <span>Stanford</span> encyclopedia of
philosophy</em>, Spring 2018.
</div>
<div id="ref-Harman87" class="csl-entry" role="doc-biblioentry">
Harman, G. (1987). <span>‘(Nonsolipsistic) conceptual role
semantics’</span>. Lepore E. (ed.) <em>New directions in semantics</em>,
pp. 55–81. Academic Press: London.
</div>
<div id="ref-Harnad90" class="csl-entry" role="doc-biblioentry">
Harnad, S. (1990). <span>‘The symbol grounding problem’</span>,
<em>Physica D</em>, 42: 335–46.
</div>
<div id="ref-Haugeland98" class="csl-entry" role="doc-biblioentry">
Haugeland, J. (1998). <span>‘Mind embodied and embedded’</span>.
Haugeland J. (ed.) <em>Having thought: Essays in the metaphysics of
mind</em>, pp. 207–40. Harvard University Press: Cambridge, MA.
</div>
<div id="ref-HenrichHeineNorenzayan10" class="csl-entry"
role="doc-biblioentry">
Henrich, J., Heine, S. J., &amp; Norenzayan, A. (2010). <span>‘The
weirdest people in the world?’</span>, <em>Behavioral and Brain
Sciences</em>, 33: 61–135.
</div>
<div id="ref-IrvineSprevak20" class="csl-entry" role="doc-biblioentry">
Irvine, E., &amp; Sprevak, M. (2020). <span>‘Eliminativism about
consciousness’</span>. Kriegel U. (ed.) <em>The oxford handbook of the
philosophy of consciousness</em>, pp. 348–70. Oxford University Press:
Oxford.
</div>
<div id="ref-Isaac17" class="csl-entry" role="doc-biblioentry">
Isaac, A. M. C. (2019). <span>‘The semantics latent in
<span>S</span>hannon information’</span>, <em>The British Journal for
the Philosophy of Science</em>, 70: 103–25.
</div>
<div id="ref-JohnsonLaird78" class="csl-entry" role="doc-biblioentry">
Johnson-Laird, P. N. (1978). <span>‘What’s wrong with
<span>G</span>randma’s guide to procedural semantics: A reply to
<span>J</span>erry <span>F</span>odor’</span>, <em>Cognition</em>, 6:
249–61.
</div>
<div id="ref-Kripke80" class="csl-entry" role="doc-biblioentry">
Kripke, S. A. (1980). <em>Naming and necessity</em>. Cambridge, MA:
Harvard University Press.
</div>
<div id="ref-LakeUllman17" class="csl-entry" role="doc-biblioentry">
Lake, B. M., Ullman, T. D., Tenenbaum, J. B., &amp; Gershman, S. J.
(2017). <span>‘Building machines that learn and think like
people’</span>, <em>Behavioral and Brain Sciences</em>, 40: e253.
</div>
<div id="ref-Lee18" class="csl-entry" role="doc-biblioentry">
Lee, J. (2018). <span>‘Mechanisms, wide functions and content: Towards a
computational pluralism’</span>, <em>The British Journal for the
Philosophy of Science</em>. DOI: <a
href="https://doi.org/10.1093/bjps/axy061">10.1093/bjps/axy061</a>
</div>
<div id="ref-LenatFeigenbaum91" class="csl-entry"
role="doc-biblioentry">
Lenat, D. B., &amp; Feigenbaum, E. A. (1991). <span>‘On the thresholds
of knowledge’</span>, <em>Artificial Intelligence</em>, 47: 185–250.
</div>
<div id="ref-Lifschitz15" class="csl-entry" role="doc-biblioentry">
Lifschitz, V. (2015). <span>‘The dramatic true story of the frame
default’</span>, <em>Journal of Philosophical Logic</em>, 44: 163–96.
</div>
<div id="ref-Loewer17" class="csl-entry" role="doc-biblioentry">
Loewer, B. (2017). <span>‘A guide to naturalizing semantics’</span>.
Hale B., Wright C., &amp; Miller A. (eds) <em>Companion to the
philosophy of language</em>, 2nd ed., pp. 174–96. John Wiley &amp; Sons:
New York, NY.
</div>
<div id="ref-Lormand90" class="csl-entry" role="doc-biblioentry">
Lormand, E. (1990). <span>‘Framing the frame problem’</span>,
<em>Synthese</em>, 82: 353–74.
</div>
<div id="ref-LudwigSchneider08" class="csl-entry"
role="doc-biblioentry">
Ludwig, K., &amp; Schneider, S. (2008). <span>‘Fodor’s challenge to the
classical computational theory of mind’</span>, <em>Mind and
Language</em>, 23/3: 123–43.
</div>
<div id="ref-Machery12" class="csl-entry" role="doc-biblioentry">
Machery, E. (2013). <span>‘In defense of reverse inference’</span>,
<em>The British Journal for the Philosophy of Science</em>, 65: 251–67.
</div>
<div id="ref-Machery18" class="csl-entry" role="doc-biblioentry">
——. (forthcoming). <span>‘Discovery and confirmation in evolutionary
psychology’</span>. Prinz J. (ed.) <em>The oxford handbook of philosophy
of psychology</em>. Oxford University Press.
</div>
<div id="ref-MarcusDavis19" class="csl-entry" role="doc-biblioentry">
Marcus, G., &amp; Davis, E. (2019). <em>Rebooting <span>AI</span>:
Building artificial intelligence we can trust</em>. New York, NY:
Penguin Books.
</div>
<div id="ref-MashourRoelfsemaChangeux20" class="csl-entry"
role="doc-biblioentry">
Mashour, G. A., Roelfsema, P. R., Changeux, J.-P., &amp; Dehaene, S.
(2020). <span>‘Conscious processing and the <span>G</span>lobal
<span>N</span>euronal <span>W</span>orkspace hypothesis’</span>,
<em>Neuron</em>, 105: 776–98.
</div>
<div id="ref-Maudlin89" class="csl-entry" role="doc-biblioentry">
Maudlin, T. (1989). <span>‘Computation and consciousness’</span>,
<em>The Journal of Philosophy</em>, 86: 407–32.
</div>
<div id="ref-McCarthy90" class="csl-entry" role="doc-biblioentry">
McCarthy, J. (1990). <em>Formalizing common sense: Papers by
<span>J</span>ohn <span>McCarthy</span></em>. (V. L. Lifschitz, Ed.).
Norwood, NJ: Ablex.
</div>
<div id="ref-McCarthyHayes69" class="csl-entry" role="doc-biblioentry">
McCarthy, J., &amp; Hayes, P. J. (1969). <span>‘Some philosophical
problems from the standpoint of artificial intelligence’</span>. Meltzer
B. &amp; Michie D. (eds) <em>Machine intelligence 4</em>, pp. 463–502.
Edinburgh University Press: Edinburgh.
</div>
<div id="ref-Millikan04a" class="csl-entry" role="doc-biblioentry">
Millikan, R. G. (2004). <em>The varieties of meaning</em>. Cambridge,
MA: MIT Press.
</div>
<div id="ref-Morrison14" class="csl-entry" role="doc-biblioentry">
Morrison, M. (2014). <em>Reconstructing reality: Models, mathematics,
and simulations</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Nagel74" class="csl-entry" role="doc-biblioentry">
Nagel, T. (1974). <span>‘What is it like to be a bat?’</span>,
<em>Philosophical Review</em>, 83: 435–50.
</div>
<div id="ref-NeanderSchulte21" class="csl-entry" role="doc-biblioentry">
Neander, K., &amp; Schulte, P. (2021). <span>‘<a
href="https://plato.stanford.edu/archives/spr2021/entries/content-teleological/">Teleological
theories of mental content</a>’</span>. Zalta E. N. (ed.) <em>The
<span>Stanford</span> encyclopedia of philosophy</em>, Spring 2021.
</div>
<div id="ref-NewellSimon72" class="csl-entry" role="doc-biblioentry">
Newell, A., &amp; Simon, H. A. (1972). <em>Human problem solving</em>.
Englewood Cliffs, NJ: Prentice-Hall.
</div>
<div id="ref-Nisbett03" class="csl-entry" role="doc-biblioentry">
Nisbett, R. E. (2003). <em>The geography of thought</em>. New York, NY:
The Free Press.
</div>
<div id="ref-Papineau87" class="csl-entry" role="doc-biblioentry">
Papineau, D. (1987). <em>Reality and representation</em>. Oxford:
Blackwell.
</div>
<div id="ref-Piccinini15" class="csl-entry" role="doc-biblioentry">
Piccinini, G. (2015). <em>The nature of computation</em>. Oxford: Oxford
University Press.
</div>
<div id="ref-Potochnik17" class="csl-entry" role="doc-biblioentry">
Potochnik, A. (2017). <em>Idealization and the aims of science</em>.
Chicago, IL: University of Chicago Press.
</div>
<div id="ref-Prinz16" class="csl-entry" role="doc-biblioentry">
Prinz, J. (2016). <span>‘Against illusionism’</span>, <em>Journal of
Consciousness Studies</em>, 23: 186–96.
</div>
<div id="ref-Putnam81" class="csl-entry" role="doc-biblioentry">
Putnam, H. (1981). <em>Reason, truth and history</em>. Cambridge:
Cambridge University Press.
</div>
<div id="ref-Pylyshyn87" class="csl-entry" role="doc-biblioentry">
Pylyshyn, Z. W. (Ed.). (1987). <em>The robot’s dilemma</em>. Norwood,
NJ: Ablex.
</div>
<div id="ref-Ramsey07" class="csl-entry" role="doc-biblioentry">
Ramsey, W. M. (2007). <em>Representation reconsidered</em>. Cambridge:
Cambridge University Press.
</div>
<div id="ref-Rescorla13" class="csl-entry" role="doc-biblioentry">
Rescorla, M. (2013). <span>‘Against structuralist theories of
computational implementation’</span>, <em>The British Journal for the
Philosophy of Science</em>, 64: 681–707.
</div>
<div id="ref-Rescorla15" class="csl-entry" role="doc-biblioentry">
——. (2016). <span>‘Bayesian sensorimotor psychology’</span>, <em>Mind
and Language</em>, 31: 3–36.
</div>
<div id="ref-RollsTreves11" class="csl-entry" role="doc-biblioentry">
Rolls, E. T., &amp; Treves, A. (2011). <span>‘The neural encoding of
information in the brain’</span>, <em>Progress in Neurobiology</em>, 95:
448–90.
</div>
<div id="ref-Ryder04" class="csl-entry" role="doc-biblioentry">
Ryder, D. (2004). <span>‘<span>SINBAD</span> neurosemantics: A theory of
mental representation’</span>, <em>Mind and Language</em>, 19: 211–40.
</div>
<div id="ref-Samuels98" class="csl-entry" role="doc-biblioentry">
Samuels, R. (1998). <span>‘Evolutionary psychology and the massive
modularity hypothesis’</span>, <em>The British Journal for the
Philosophy of Science</em>, 49: 575–602.
</div>
<div id="ref-Samuels05" class="csl-entry" role="doc-biblioentry">
——. (2005). <span>‘The complexity of cognition: Tractability arguments
for massive modularity’</span>. Carruthers P., Laurence S., &amp; Stich
S. P. (eds) <em>The innate mind: Vol. I, structure and contents</em>,
pp. 107–21. Oxford University Press: Oxford.
</div>
<div id="ref-Samuels10" class="csl-entry" role="doc-biblioentry">
——. (2010). <span>‘Classical computationalism and the many problems of
cognitive relevance’</span>, <em>Studies in History and Philosophy of
Science</em>, 41: 280–93.
</div>
<div id="ref-SchankAbelson77" class="csl-entry" role="doc-biblioentry">
Schank, R. C., &amp; Abelson, R. P. (1977). <em>Scripts, plans, goals,
and understanding</em>. Hillsdale, NJ: Lawrence Erlbaum.
</div>
<div id="ref-Schneider11" class="csl-entry" role="doc-biblioentry">
Schneider, S. (2011). <em>The language of thought: A new philosophical
direction</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Searle80" class="csl-entry" role="doc-biblioentry">
Searle, J. R. (1980). <span>‘Minds, brains, and programs’</span>,
<em>Behavioral and Brain Sciences</em>, 3: 417–24.
</div>
<div id="ref-Searle84b" class="csl-entry" role="doc-biblioentry">
——. (1984). <em>Minds, brains and science</em>. Cambridge, MA: Harvard
University Press.
</div>
<div id="ref-Searle90b" class="csl-entry" role="doc-biblioentry">
——. (1990). <span>‘Is the brain’s mind a computer program?’</span>,
<em>Scientific American</em>, 262: 20–5.
</div>
<div id="ref-Searle92" class="csl-entry" role="doc-biblioentry">
——. (1992). <em>The rediscovery of the mind</em>. Cambridge, MA: MIT
Press.
</div>
<div id="ref-Sellars62" class="csl-entry" role="doc-biblioentry">
Sellars, W. (1962). <span>‘Philosophy and the scientific image of
man’</span>. Colodny R. (ed.) <em>Frontiers of science and
philosophy</em>, pp. 35–78. University of Pittsburgh Press: Pittsburgh,
PA.
</div>
<div id="ref-Shagrir12" class="csl-entry" role="doc-biblioentry">
Shagrir, O. (2012). <span>‘Structural representations and the
brain’</span>, <em>The British Journal for the Philosophy of
Science</em>, 63: 519–45.
</div>
<div id="ref-Shagrir18" class="csl-entry" role="doc-biblioentry">
——. (2020). <span>‘In defense of the semantic view of
computation’</span>, <em>Synthese</em>, 197: 4083–108.
</div>
<div id="ref-Shanahan97" class="csl-entry" role="doc-biblioentry">
Shanahan, M. (1997). <em>Solving the frame problem</em>. Cambridge, MA:
Bradford Books, MIT Press.
</div>
<div id="ref-Shanahan16" class="csl-entry" role="doc-biblioentry">
——. (2016). <span>‘<a
href="https://plato.stanford.edu/archives/spr2016/entries/frame-problem/">The
frame problem</a>’</span>. Zalta E. N. (ed.) <em>The
<span>Stanford</span> encyclopedia of philosophy</em>, Spring 2016.
</div>
<div id="ref-ShanahanBaars05" class="csl-entry" role="doc-biblioentry">
Shanahan, M., &amp; Baars, B. (2005). <span>‘Applying global workspace
theory to the frame problem’</span>, <em>Cognition</em>, 98: 157–76.
</div>
<div id="ref-Shea13" class="csl-entry" role="doc-biblioentry">
Shea, N. (2013). <span>‘Naturalising representational content’</span>,
<em>Philosophy Compass</em>, 8: 496–509.
</div>
<div id="ref-Shea18" class="csl-entry" role="doc-biblioentry">
——. (2018). <em>Representation in cognitive science</em>. Oxford: Oxford
University Press.
</div>
<div id="ref-SheaBayne10" class="csl-entry" role="doc-biblioentry">
Shea, N., &amp; Bayne, T. (2010). <span>‘The vegetative state and the
science of consciousness’</span>, <em>The British Journal for the
Philosophy of Science</em>, 61: 459–84.
</div>
<div id="ref-Skyrms10" class="csl-entry" role="doc-biblioentry">
Skyrms, B. (2010). <em>Signals</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Sprevak08a" class="csl-entry" role="doc-biblioentry">
Sprevak, M. (2010). <span>‘Computation, individuation, and the received
view on representation’</span>, <em>Studies in History and Philosophy of
Science</em>, 41: 260–70.
</div>
<div id="ref-Sprevak13" class="csl-entry" role="doc-biblioentry">
——. (2013). <span>‘Fictionalism about neural representations’</span>,
<em>The Monist</em>, 96: 539–60.
</div>
<div id="ref-Sprevak16" class="csl-entry" role="doc-biblioentry">
——. (2016). <span>‘Philosophy of the psychological and cognitive
sciences’</span>. Humphreys P. (ed.) <em>Oxford handbook for the
philosophy of science</em>, pp. 92–114. Oxford University Press: Oxford.
</div>
<div id="ref-Sprevak18a" class="csl-entry" role="doc-biblioentry">
——. (2019). <span>‘Review of <span>Susan Schneider</span>,
<em><span>T</span>he <span>L</span>anguage of <span>T</span>hought:
<span>A</span> <span>N</span>ew <span>P</span>hilosophical
<span>D</span>irection</em>’</span>, <em>Mind</em>, 128: 555–64.
</div>
<div id="ref-Sterelny03" class="csl-entry" role="doc-biblioentry">
Sterelny, K. (2003). <em>Thought in a hostile world</em>. Oxford:
Blackwell.
</div>
<div id="ref-Strawson94" class="csl-entry" role="doc-biblioentry">
Strawson, G. (2010). <em>Mental reality</em>., 2nd ed. Cambridge, MA:
MIT Press.
</div>
<div id="ref-Strawson18" class="csl-entry" role="doc-biblioentry">
——. (2018). <span>‘The consciousness deniers’</span>, <em>The New York
Review of Books</em>.
</div>
<div id="ref-Sullivan19" class="csl-entry" role="doc-biblioentry">
Sullivan, E. (2019). <span>‘Understanding from machine learning
models’</span>, <em>The British Journal for the Philosophy of
Science</em>. DOI: <a
href="https://doi.org/10.1093/bjps/axz035">10.1093/bjps/axz035</a>
</div>
<div id="ref-Swoyer91" class="csl-entry" role="doc-biblioentry">
Swoyer, C. (1991). <span>‘Structural representation and surrogative
reasoning’</span>, <em>Synthese</em>, 87: 449–508.
</div>
<div id="ref-Tye18" class="csl-entry" role="doc-biblioentry">
Tye, M. (2018). <span>‘<a
href="https://plato.stanford.edu/archives/sum2018/entries/qualia/">Qualia</a>’</span>.
Zalta E. N. (ed.) <em>The <span>Stanford</span> encyclopedia of
philosophy</em>, Summer 2018.
</div>
<div id="ref-Usher01" class="csl-entry" role="doc-biblioentry">
Usher, M. (2001). <span>‘A statistical referential theory of content:
Using information theory to account for misrepresentation’</span>,
<em>Mind and Language</em>, 16: 311–34.
</div>
<div id="ref-Wakefield03" class="csl-entry" role="doc-biblioentry">
Wakefield, J. C. (2003). <span>‘The <span>C</span>hinese room argument
reconsidered: Essentialism, indeterminacy, and <span>Strong
AI</span>’</span>, <em>Minds and Machines</em>, 13: 285–319.
</div>
<div id="ref-Wheeler05" class="csl-entry" role="doc-biblioentry">
Wheeler, M. (2005). <em>Reconstructing the cognitive world</em>.
Cambridge, MA: MIT Press.
</div>
<div id="ref-Wheeler08a" class="csl-entry" role="doc-biblioentry">
——. (2008). <span>‘Cognition in context: Phenomenology, situated
robotics and the frame problem’</span>, <em>International Journal of
Philosophical Studies</em>, 16: 323–49.
</div>
<div id="ref-Winograd72" class="csl-entry" role="doc-biblioentry">
Winograd, T. (1972). <span>‘Understanding natural language’</span>,
<em>Cognitive Psychology</em>, 3: 1–191.
</div>
</div>
</div>

                            </div>
                            
                        </div>

                    </div>

                    <div class="is-col is-33">     
                        <div class="is-hidden-print is-hidden-mobile is-sticky">
                            
                                <h1 style="margin-top: 0px;">Contents</h1>
                                <ul class="is-unstyled">
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#introduction"><span style="visibility: visible;">1</span> &nbsp;  Introduction</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#overview-of-chapter"><span style="visibility: visible;">1.1</span> &nbsp;  Overview of chapter</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#semantic-content-searles-chinese-room-argument"><span style="visibility: visible;">2</span> &nbsp;  Semantic content &#x2013; Searle&#x2019;s Chinese room argument</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-chinese-room-argument"><span style="visibility: visible;">2.1</span> &nbsp;  The Chinese room argument</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-problem-of-semantic-content"><span style="visibility: visible;">2.2</span> &nbsp;  The problem of semantic content</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#theories-of-content"><span style="visibility: visible;">2.3</span> &nbsp;  Theories of content</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#content-and-physical-computation"><span style="visibility: visible;">2.4</span> &nbsp;  Content and physical computation</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#phenomenal-consciousness-the-hard-problem"><span style="visibility: visible;">3</span> &nbsp;  Phenomenal consciousness &#x2013; The hard problem</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-conceivability-argument-against-physicalism"><span style="visibility: visible;">3.1</span> &nbsp;  The conceivability argument against physicalism</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-conceivability-argument-against-computational-functionalism"><span style="visibility: visible;">3.2</span> &nbsp;  The conceivability argument against computational functionalism</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#naturalistic-dualism"><span style="visibility: visible;">3.3</span> &nbsp;  Naturalistic dualism</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#eliminativism-and-related-replies"><span style="visibility: visible;">3.4</span> &nbsp;  Eliminativism and related replies</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#central-reasoning-the-frame-problem"><span style="visibility: visible;">4</span> &nbsp;  Central reasoning &#x2013; The frame problem</a>
            
        </span>
        
    </li>
    
    <li>
        
        <ul class="is-unstyled" style="margin-left: 15px;">
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#the-frame-problem"><span style="visibility: visible;">4.1</span> &nbsp;  The frame problem</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#dreyfuss-argument"><span style="visibility: visible;">4.2</span> &nbsp;  Dreyfus&#x2019;s argument</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#fodors-argument"><span style="visibility: visible;">4.3</span> &nbsp;  Fodor&#x2019;s argument</a>
                    
                </span>
            </li>
            
            <li>
                <span style="font-size: 12px;">
                    
                    <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#responses-to-the-problems"><span style="visibility: visible;">4.4</span> &nbsp;  Responses to the problems</a>
                    
                </span>
            </li>
            
        </ul>
        
    </li>
    
    <li>
        
        <span style="font-size: 14px;">
            
            <a href="https://marksprevak.com/publications/philosophical-issues-in-computational-cognitive-sciences-06dd/#conclusion"><span style="visibility: visible;">5</span> &nbsp;  Conclusion</a>
            
        </span>
        
    </li>
    
</ul>

                            
                            
                        </div>
                    </div>
                </div>
            </main>

        <footer class="footer"></footer>

        </div>

        <script src="https://marksprevak.com/kube/js/kube.min.js"></script>
<script>
    $K.init();
</script>


    </body>
</html>
