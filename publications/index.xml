<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Publications on marksprevak.com </title>
    <link>http://marksprevak.com/publications/</link>
    <language>en-us</language>
    <author>Mark Sprevak</author>
    <rights>Copyright (c) Mark Sprevak.</rights>
    <updated>Tue, 16 Jan 2018 00:00:00 UTC</updated>
    
    <item>
      <title>Distributed cognition and the humanities</title>
      <link>http://marksprevak.com/publications/distributed-cognition-and-the-humanities/</link>
      <pubDate>Tue, 16 Jan 2018 00:00:00 UTC</pubDate>
      <author>Mark Sprevak</author>
      <guid>http://marksprevak.com/publications/distributed-cognition-and-the-humanities/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Realism about cognitive science</title>
      <link>http://marksprevak.com/publications/realism-about-cognitive-science-2018/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 UTC</pubDate>
      <author>Mark Sprevak</author>
      <guid>http://marksprevak.com/publications/realism-about-cognitive-science-2018/</guid>
      <description>&lt;div&gt;
&lt;h1 id=&#34;introduction&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;This chapter is about a puzzle. Realism about &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is often glossed as the claim that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s are mind independent: &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s exist and have their nature independent of human beliefs, interests, attitudes, and other mental states. &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s are out there, getting on with it, independently of human minds. How then should one understand realism about the mind? Having an answer to this is important if one wants to be a realist about cognitive science. The subject matter of cognitive science includes mental states, mental processes, and mental capacities. None of these are mind independent. But how then can one be a realist about them? This is our puzzle. My solution will be to distinguish between two types of mind dependence in cognitive science. One type is trivial and follows from the nature of the subject matter. The other type is non-trivial, and it is the true point of contention between a realist and an anti-realist about cognitive science. My aim in this chapter is to identify that point of contention.&lt;/p&gt;
&lt;p&gt;In Section 2, I describe different varieties of realism that one might adopt about cognitive science. In Section 3, I argue that realism that asserts mind independence has a special role to play in cognitive science. In Section 4, I present the puzzle about this variety of realism. In Section 5, I examine three solutions to the puzzle. Each draws a distinction between a trivial and a non-trivial form of mind dependence in a different way. My favoured proposal derives from the observation that theories in cognitive science aim to explain mental phenomena in terms of structured complexes – for example, in terms of computations, mechanisms, networks, or causal chains. I claim that realism in cognitive science should be understood as a claim about the &lt;em&gt;individuals and relations&lt;/em&gt; that compose those structures not about the &lt;em&gt;entire complexes taken as whole&lt;/em&gt;. Mind dependence about the wholes (hypothesised to realise, constitute, or otherwise compose mental processes) is trivial. Mind dependence about the parts and relations that make up those wholes is not. This is the true point of disagreement between realists and anti-realists about cognitive science.&lt;/p&gt;
&lt;h1 id=&#34;kinds-of-realism&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Kinds of realism&lt;/h1&gt;
&lt;p&gt;Realism is not a single claim but a range of possible claims that could be made about a range of subject matters. One might be a realist about one type of entity or subject matter and an anti-realist about another. One might be a realist about electrons but an anti-realist about beauty marks. ‘Local’ versions of realism should also be distinguished from ‘global’ versions. A global version of realism would assert realism about all or most subject matters of the mature sciences. I will not consider global versions of realism here. My concern is with realist claims made about entities specifically in cognitive science.&lt;/p&gt;
&lt;p&gt;Within this domain, a realist may make a range of claims. Realist/anti-realist disputes take on a different character depending on which claim is at issue. In this section, I highlight six possible varieties of realist claim: claims regarding existence of an entity, the nature of that entity, referential semantics for the discourse that purports to talk about that entity, truth or approximate truth of that discourse, evidence for truth of that discourse, and mind independence of the entity. A realist may assert or deny these claims in various combinations.&lt;/p&gt;
&lt;p&gt;First, &lt;em&gt;existence&lt;/em&gt;. On this view, realism about &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s commits one to the existence of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s. &lt;span class=&#34;citation&#34; data-cites=&#34;Fodor75&#34;&gt;Fodor (1975)&lt;/span&gt; is a realist in this sense about beliefs. The relevant kind of anti-realism would be eliminativism. &lt;span class=&#34;citation&#34; data-cites=&#34;Churchland81&#34;&gt;Churchland (1981)&lt;/span&gt; holds this position about beliefs.&lt;/p&gt;
&lt;p&gt;Second, &lt;em&gt;nature of the entities&lt;/em&gt;. Assuming that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s exist, what sort of things are &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s? This second variety of realism holds that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s are discrete individuals. &lt;span class=&#34;citation&#34; data-cites=&#34;Fodor75&#34;&gt;Fodor (1975)&lt;/span&gt; is a realist in this sense about beliefs. Beliefs are discrete individuals that occur and re-occur inside someone’s head. The relevant kind of anti-realism would take a deflationary view of the relevant entity. &lt;span class=&#34;citation&#34; data-cites=&#34;Dennett91a&#34;&gt;Dennett (1991b)&lt;/span&gt; argues that beliefs are not discrete individuals but rather amorphous and hard-to-count patterns in and around agents that observers may exploit for predictive or explanatory gain.&lt;/p&gt;
&lt;p&gt;Third, &lt;em&gt;referential semantics&lt;/em&gt;. If one is a realist about &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s, then the relevant part of the discourse that purports to talk about &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s should be understood as having a referential semantics. &lt;span class=&#34;citation&#34; data-cites=&#34;Fodor75&#34;&gt;Fodor (1975)&lt;/span&gt; is a realist in this sense too about beliefs. If we say, ‘Abby has the belief that beer is in the fridge’, we refer to some thing that Abby has. According to Fodor, that thing is a tokening of a sentence in the language of thought inside her head. The relevant form of anti-realism would be a non-referential semantics for the relevant discourse. &lt;span class=&#34;citation&#34; data-cites=&#34;Ryle49&#34;&gt;Ryle (1949)&lt;/span&gt; advocates this kind of anti-realism about beliefs. When we say, ‘Abby has the belief that beer is in the fridge’, we do not refer to any thing that Abby has. Instead, we intend to convey to our listeners a warrant to make inferences about, among other things, Abby’s behaviour.&lt;/p&gt;
&lt;p&gt;Fourth, &lt;em&gt;truth&lt;/em&gt;. If one is a realist about &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s then the relevant part of the discourse aims to tell the truth. &lt;span class=&#34;citation&#34; data-cites=&#34;Block07&#34;&gt;Block (2007)&lt;/span&gt; advocates this form of realism about phenomenal consciousness. Experiments to study phenomenal consciousness involve reports from human subjects about the occurrence of subjective phenomenal aspects of their experience (reporting, for example, that they experience red). We should, according to Block, understand these reports as aiming to tell the truth about those experiences. In contrast, &lt;span class=&#34;citation&#34; data-cites=&#34;Dennett91&#34;&gt;Dennett (1991a)&lt;/span&gt; argues that we should be fictionalists about phenomenal consciousness. Reports of experiencing red should be understood not as aiming to tell the truth about the occurrence of phenomenal aspects of experience but as a roundabout way for the subject to express that her cognitive system has detected a highly disjunctive physical property (such as redness). A realist holds that the discourse aims at telling the truth. An anti-realist denies the truth-seeking character of discourse but may maintain that the talk has other virtues (e.g. pragmatic virtues).&lt;/p&gt;
&lt;p&gt;Fifth, &lt;em&gt;evidence&lt;/em&gt;. If one is a realist about &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s, then one holds that we have justification for the truth (or approximate truth) of the relevant part of the discourse. &lt;span class=&#34;citation&#34; data-cites=&#34;Block07&#34;&gt;Block (2007)&lt;/span&gt; is a realist in this sense too about phenomenal consciousness. Subjects’ reports of conscious experience not only aim to tell the truth about instantiations of phenomenal character, we also (normally) have justification that they &lt;em&gt;are&lt;/em&gt; true. Significantly, the justification holds even under unusual presentational conditions such as when stimuli are flashed briefly to subjects in &lt;span class=&#34;citation&#34; data-cites=&#34;Sperling60&#34;&gt;Sperling (1960)&lt;/span&gt;’s experiments (a grid with characters is briefly presented followed by a visual mask). The relevant form of anti-realism would involve some degree of epistemic caution about the relevant claims. &lt;span class=&#34;citation&#34; data-cites=&#34;Irvine12&#34;&gt;Irvine (2012)&lt;/span&gt; claims that we lack justification for believing the reports of subjects about phenomenal aspects of their experiences in the context of Sperling’s experiments.&lt;/p&gt;
&lt;p&gt;Finally, &lt;em&gt;mind independence&lt;/em&gt;. Like the second claim, this concerns the nature of the entities. However, the question here is not about their nature as discrete individuals but about their degree of mind dependence: does that entity depend, for its existence or nature, on minds? All our knowledge of the world is mediated to some extent by our minds. We cannot see the world untouched by human conceptual, motivational, and other cognitive systems. We may attempt to counteract the effects of our cognitive makeup by taking into account its hypothesised nature. But seeing the world ‘as it is’, without some contribution from the human mind, is impossible. This invites a question: Which parts of our knowledge represent to entities and properties that are really out there and which are (partial) constructions of our minds? Some entities appear to exist and have the properties that we attribute to them independently of the way we think of them. Perhaps some fundamental particles in physics, e.g. electrons, are like this. If our minds were not to exist, or if they were to have a radically different nature, electrons would continue to exist and have unchanged properties. Other entities appear to be partial constructions of our minds. Beauty marks may be an example of these. Whether a specific skin colouration is a beauty mark depends on how that colouration strikes, or would strike, a mind like ours and ft with our visual preferences – whether that patch &lt;em&gt;looks beautiful to us&lt;/em&gt;. If human minds were not to exist, or if they were to have a different makeup, the distribution of beauty marks in the world would be different. One might ask the realism/anti-realism question about the entities of cognitive science. For example, among those entities are neural computations. Neural computations are invoked by cognitive science to explain human mental processes and mental capacities. specific mental processes – for example, specific kinds of decision making – are explained by saying that the brain of the subject concerned performs specific neural computations. Cognitive science invokes neural computations to explain mental life. Should one be a realist or an anti-realist about these neural computations?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;Fodor80&#34;&gt;Fodor (1980)&lt;/span&gt; is an example of someone who is a realist about these neural computations. Suppose that Abby’s brain performs a specific computation which realises her decision-making processes that determines, on a specific occasion, whether Abby goes to the fridge to get a beer. According to Fodor, whether Abby’s brain performs this computation, or any computation at all, has nothing to do with &lt;em&gt;how we view&lt;/em&gt; Abby. Whether Abby’s brain performs this computation is determined by facts about Abby and her brain. &lt;span class=&#34;citation&#34; data-cites=&#34;Burge86&#34;&gt;Burge (1986)&lt;/span&gt; is another realist about neural computation but he holds that the neural computation depends on a broader base of mind-independent facts: it depends not just on Abby’s brain but also her causal relationship to her environment. Despite their disagreement, both Fodor and Burge agree that neural computations are really out there, they are not a grouping that is dependent on how we human agents view Abby – a grouping that is somehow significant to us but not reflective of any objective distinction in the world. The aim of computational cognitive science is to discover and describe these objective distinctions in the world carved out by neural computations. One may get this description right or wrong, but one does so independently of how human agents conceive the world.&lt;/p&gt;
&lt;p&gt;In contrast, &lt;span class=&#34;citation&#34; data-cites=&#34;Putnam88&#34;&gt;Putnam (1988)&lt;/span&gt; and &lt;span class=&#34;citation&#34; data-cites=&#34;Searle92&#34;&gt;Searle (1992)&lt;/span&gt; argue for anti-realism about neural computation. According to them, neither Abby’s brain nor her brain plus her relation to her environment determine whether her brain performs a specific computation. Absent consideration of &lt;em&gt;how we view&lt;/em&gt; Abby, there is no fact about whether Abby’s brain performs one computation rather than another, or whether it performs any computation at all. Neural computations are observer relative. If human minds were not to exist, or if they were to have a different makeup, the distribution of neural computations would be different. Neural computations are more like beauty marks than electrons: they are a construction that reflects the specific way in which humans are disposed to conceive of the world, not objective features waiting ‘out there’ to be discovered.&lt;/p&gt;
&lt;p&gt;For electrons and beauty marks, the question about mind dependence can be posed in a relatively straightforward manner. The worry is that the same cannot be said for neural computations. Neural computations are hypothesised to be connected to mental life. They realise or otherwise constitute aspects of our mental life. This makes realism about neural computations hard to understand as a coherent possibility. If neural computations realise mental life, how can neural computations be mind independent? Fodor and Burge cannot believe that Abby’s neural computations are entirely mind independent. The entity in question – the neural computation that underlies Abby’s decision making about the beer – depends on at least one mind: Abby’s own. If Abby’s mind were not to exist or to have a different nature, that neural computation would differ. Similarly, Putnam and Searle cannot believe that Abby’s neural computations are &lt;em&gt;in some way or other&lt;/em&gt; mind dependent. That would be trivially true. No one think that her neural computations can exist, or have their nature, independently of how things go in Abby’s mental life. So both the realist and the anti-realist must agree that Abby’s neural computations are mind dependent &lt;em&gt;in some way or other&lt;/em&gt;. The realist/anti-realist dispute cannot therefore be about mind dependence &lt;em&gt;simpliciter&lt;/em&gt;. Something else must be going on. Identifying what this is – what is at stake in this realist/anti-realist dispute in cognitive science – is our puzzle.&lt;/p&gt;
&lt;p&gt;Earlier in this section we saw that a realist about &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; need not endorse a mind independence claim about &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. We saw five alternative ways to be a realist about cognitive science. This suggests a quick way out of our puzzle. If stating mind independence is a problem for cognitive science, why not simply abandon this form of realism and pursue some other form of realism? There are at least five other options to choose from. In the next section, I argue that while there is nothing wrong with these alternative other forms of realism about cognitive science, this strategy would have a significant cost. Cognitive science needs the mind-independence form of realism to fulfil one of its wider ambitions: the ambition to naturalise the mind.&lt;/p&gt;
&lt;h1 id=&#34;why-care-about-mind-independence&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Why care about mind independence?&lt;/h1&gt;
&lt;p&gt;The world contains at least two kinds of phenomenon: mental phenomena – involving entities like beliefs, sensations, ideas, concepts, thought processes, judgements, and so on – and physical phenomena – involving entities like bodies, brains, atoms, molecules, cells, and so on. The two appear to be related: changes in one correlate with changes in the other. But the exact nature of the relationship is unclear. In particular, it is unclear whether mental phenomena are &lt;em&gt;sui generis&lt;/em&gt; entities or whether they somehow ‘arise from’ the physical. Mental phenomena are puzzling not just because they are complex but because we do not know how they relate to the physical world.&lt;/p&gt;
&lt;p&gt;Some theories in cognitive science aim to bridge this gap. Those theories reductively pair specific mental phenomena with non-mental phenomena. The non-mental phenomena often have specific properties: the states &lt;em&gt;perform computations&lt;/em&gt;, &lt;em&gt;represent&lt;/em&gt;, &lt;em&gt;process information&lt;/em&gt;, &lt;em&gt;carry error signals&lt;/em&gt;, and so on. Certain instances of decision making, for example, are paired with certain neural computations &lt;span class=&#34;citation&#34; data-cites=&#34;SchultzDayan97 GoldShadlen01 GoldShadlen07 RangelCamerer08&#34;&gt;(Schultz, Dayan, and Read Monague 1997; Gold and Shadlen 2001, 2007; Rangel, Camerer, and Montague 2008)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Such theories propose a relationship between the mental and the non-mental that goes beyond that of mere correlation. The precise details differ between cases, but two general observations can be made. First, the association between the mental and non-mental has a &lt;em&gt;non-trivial modal extent&lt;/em&gt;. The mental and non-mental reliably correlate across a wide range of circumstances including conditions not experimentally tested. Precisely how far this modal dimension extends – across every possible world, across worlds with the same physical laws as ours, across worlds with the same natural laws as ours – is open to question, but we can be sure that the association has a non-trivial modal dimension. The second observation is that the non-mental member of the relationship could substitute for its mental counterpart without change in scientifically relevant effects. For example, the scientifically relevant effects associated with decision making include patterns in behaviour, patterns in error making, how uncertain evidence is weighed, reaction times, and characteristic downstream neural effects. A potential non-mental partner would not only need to co-occur with specific instances of decision making but also to produce those characteristic effects. The drift-diffusion model, for example, aims to provide not just a neural correlate of decision making but also to show that this correlate would produce the characteristic effects associated with decision making regarding reaction times, weighting of evidence, and susceptibility to errors &lt;span class=&#34;citation&#34; data-cites=&#34;GoldShadlen07&#34;&gt;(Gold and Shadlen 2007)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If a non-mental phenomenon co-occurs with a mental phenomenon across a wide range of modal circumstances and it also generates all the scientifically relevant effects associated with that mental phenomenon, then we are in a position to advance a reductive claim. Rather than hold that the mental phenomenon and non-mental phenomenon are two distinct entities that happen to co-occur, we may reduce one to the other. One might hypothesise that the mental and non-mental entities bear some reductive relation – perhaps &lt;em&gt;identity&lt;/em&gt;, &lt;em&gt;realisation&lt;/em&gt;, &lt;em&gt;constitution&lt;/em&gt;, &lt;em&gt;grounding&lt;/em&gt;, or another relation – to each other. For example, one might claim that decision making is a specific neural computation or that decision making is &lt;em&gt;realised by&lt;/em&gt; a neural computation or that decision making is &lt;em&gt;grounded by&lt;/em&gt; a neural computation.&lt;/p&gt;
&lt;p&gt;The theories in question identify some kind of reductive base for a mental phenomenon. The details of the reductive relation may differ (&lt;em&gt;identity&lt;/em&gt; vs. &lt;em&gt;realisation&lt;/em&gt; vs. &lt;em&gt;constitution&lt;/em&gt; vs. &lt;em&gt;grounding&lt;/em&gt;). But the general idea of finding some non-mental base that is sufficient for the scientifically relevant effects of the mental phenomenon is shared. One pairs a mental phenomenon with a non-mental phenomenon in such a way that the non-mental phenomenon is sufficient for, and somehow produces, the scientifically relevant properties of the mental phenomenon.&lt;/p&gt;
&lt;p&gt;Successful reductions of this sort appear to provide a road to naturalising the mind. By ‘naturalising’ I mean explaining scientifically relevant effects of mental phenomena in non-mental terms: in terms of a subject matter that does not already presuppose mental life. A naturalising explanation is one that takes as its &lt;em&gt;explanandum&lt;/em&gt; some scientifically relevant effect of a mental phenomenon (for example, some property of decision making) and gives as its &lt;em&gt;explanans&lt;/em&gt; an account that does not refer to or otherwise already presuppose mental life (for example, an &lt;em&gt;explanans&lt;/em&gt; exclusively in terms of neural computations, physical inputs, and physical outputs of the brain). Naturalising the mind therefore requires realism about the subject matter of the &lt;em&gt;explanans&lt;/em&gt;. One needs to be a realist – in the sense of asserting mind independence – about the entities cited by the &lt;em&gt;explanans&lt;/em&gt;. To see why consider the alternative. Suppose that anti-realism about neural computation is correct. Explaining decision making by appeal to neural computation would in this case not serve to naturalise that mental phenomenon. Explanation in terms of neural computations would not explain the phenomenon in non-mental terms. It would explain the phenomenon in terms of entities that depend on minds for their existence and nature. Explanation of mental phenomena in terms of neural computation would not be an explanation that does not refer to or already presuppose mental phenomena. It would not, in the sense of ‘naturalising’ above, be naturalising. One might of course still explain decision making in terms of neural computation. But one should not think that this provides a way to naturalise the mind: one has not shown how decision making arises from non-mental ingredients. Rather, one has offered a non-reductive explanation: an explanation of a mental phenomenon in terms, &lt;em&gt;inter alia&lt;/em&gt;, of other mental phenomena. Nothing wrong with this – &lt;em&gt;per se&lt;/em&gt;. But it does not serve an ambition to naturalise the mind. In order for the naturalising strategy described above to work we need to be realists – specifically, realists who assert mind independence – about the subject matter of our &lt;em&gt;explanans&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Realism about the subject matter of cognitive science is not a mere idle intellectual posture. Realism of the mind-independence variety is needed for explanations within cognitive science to serve the project of naturalising the mind. It is perfectly possible to pursue cognitive science without any naturalistic ambition. But giving up that ambition is not to be taken lightly. Consider what we would miss out on: understanding how the mind arises from non-physical ingredients. Rather than abandon this variety of realism, let us instead examine what the problem with it is and how to solve it.&lt;/p&gt;
&lt;h1 id=&#34;the-puzzle-about-mind-independence&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; The puzzle about mind independence&lt;/h1&gt;
&lt;p&gt;Reductive theories in cognitive science aim to pair mental phenomena with non-mental phenomena. A reduction of this kind appears to open the door to naturalising the mind. However, this can only work if one can be a realist – in the sense of asserting mind independence – about the non-mental side of the relation. The problem is that the preceding two claims – (i) mental phenomena reduce to non-mental phenomena; (ii) the non-mental side of the relation is mind independent – are incompatible. This is our problem. Let us examine it in more detail.&lt;/p&gt;
&lt;p&gt;Consider what happens if the reductive relation in question is &lt;em&gt;identity&lt;/em&gt;. Assume that some instance of human decision making &lt;em&gt;is&lt;/em&gt; a specific neural computation. In order to use this to offer a naturalistic explanation of decision making, we would need to be realists about this neural computation: we would need to assert that it is, in an appropriate sense, mind independent. But how could the neural computation be mind independent? If decision making &lt;em&gt;is&lt;/em&gt; a neural computation, then that neural computation must be mind dependent. Being identical to a mental phenomenon surely entails mind dependence. If human minds were not to exist, or if they were to have a different makeup, the existence and nature of the neural computation would be different. What stronger reason could there be for thinking that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is mind dependent than &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; being &lt;em&gt;identical to&lt;/em&gt; a mental phenomenon? But if the neural computation is mind dependent, then anti-realism is true and realism is false. The reduction of decision making to a specific neural computation seems to preclude realism about that neural computation.&lt;/p&gt;
&lt;p&gt;What if the reductive relation in question were &lt;em&gt;realisation&lt;/em&gt;? Identity is a symmetric relation: if &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is identical to &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is identical to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Perhaps it is the symmetry of this reductive relation that is the source of the problem. Realisation is asymmetric: if &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; realises &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; does not realise &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Would an asymmetric relation allow us to avoid mind dependence of one side of the relation infecting the other side? Unfortunately, no. The reason is that in spite of realisation being an asymmetric relation the reductive base still cannot occur &lt;em&gt;independently&lt;/em&gt; of its mental phenomenon, which is what the realist requires. Suppose that an instance of decision making is realised by a neural computation. If it is realised by that neural computation, then that neural computation is sufficient for that instance of decision making to occur.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; The occurrence of that neural computation is sufficient for the occurrence of that instance of decision making; otherwise, it would be unclear why what we found was a reductive base at all. So the following conditional holds: &lt;em&gt;If&lt;/em&gt; this neural computation were to occur, &lt;em&gt;then&lt;/em&gt; the relevant decision-making process would occur too. Moreover, this conditional holds over a non-trivial range of modal circumstances (the precise extent is determined by the realisation relation in question). The neural computation is tied to the mental process in a modally rich way such that the computation cannot occur without the relevant decision making also occurring. But then the neural computation is not mind independent. The neural computation cannot occur independently of minds. It cannot occur without the associated mental process also occurring. The reductive base is not mind independent. Let us put the same point schematically. Suppose that a reductive base, &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, realises some mental process, &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is tied in the modally rich way entailed by the realisation relation to &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; cannot occur (over some non-trivial range of modal circumstances) without &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; also occurring. But this means that B is not mind independent. &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; cannot occur without &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and hence &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; cannot occur without specific mental phenomena occurring. If human minds were not to exist, or if they were to have a different makeup (e.g. without &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;), then the facts about &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; would be different so the facts about &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; would be different. The realist’s claim that &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is mind independent is fat out incompatible with the claim that &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; realises &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Other reductive relations – grounding or constitution – suffer from the same problem. The reason is that for any reductive relation, the reductive base should, in some modally rich sense, be &lt;em&gt;sufficient&lt;/em&gt; for the mental phenomenon. It should ‘bring about’ that mental phenomenon. The specific content of ‘bringing about’ will be cashed out in different ways by different reductive relations. Irrespective of differences between reductive relations the reductive base must be sufficient for the mental phenomenon – otherwise, why think we have identified a reductive base at all? If the ‘base’ is not sufficient for the mental phenomenon, then we have identified only one ingredient among (possibly many) others associated with the occurrence of that mental phenomenon, and that is no reduction at all. If &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is a reductive base of a mental phenomenon, &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; cannot occur (over some non-trivial range of modal circumstances) without the associated mental phenomenon, &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;. But then &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; cannot (to the same modal extent) be mind independent. &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is tied to &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; via the web of associations stipulated by the reductive relation. If &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; were not to exist, or if it were to have a different nature, &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; would not exist or it would have a different nature. &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; cannot be both a reductive base of &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and be mind independent.&lt;/p&gt;
&lt;p&gt;The puzzle should not be confused with a similar puzzle about mind dependence. That puzzle arises from a worry about trivial &lt;em&gt;causal dependence&lt;/em&gt; on minds. Many entities causally depend on minds for their existence and nature: tables, chairs, cities, children. Is realism about those entities thereby undermined &lt;span class=&#34;citation&#34; data-cites=&#34;Devitt91 Miller12 Godfrey-Sm16&#34;&gt;(Devitt 1991; Miller 2012; Godfrey-Smith 2016)&lt;/span&gt;? &lt;span class=&#34;citation&#34; data-cites=&#34;Devitt91&#34;&gt;Devitt (1991)&lt;/span&gt; and &lt;span class=&#34;citation&#34; data-cites=&#34;Miller12&#34;&gt;Miller (2012)&lt;/span&gt; argue that it is not because a realist does not deny causal dependence on minds. Anti-realism, rather than asserting causal dependence, is defined by a ‘further (philosophically interesting)’ sense of dependence that goes beyond ‘mundane’ causal dependence on minds &lt;span class=&#34;citation&#34; data-cites=&#34;Miller12&#34;&gt;(Miller 2012)&lt;/span&gt;.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; This does not help us with our puzzle. The form of mind dependence at issue for us is not causal dependence. The proposals above do not say that reductive bases &lt;em&gt;cause&lt;/em&gt; mental phenomena. Removing causal dependence from the field would not help us here. It is the further, non-mundane, ‘constitutive’ mind dependence built into the reductive relation that renders the anti-realist’s claim about cognitive science trivially true.&lt;/p&gt;
&lt;p&gt;In response to the puzzle, should we then grant the anti-realist an easy ‘win’: concede that we should be anti-realists about the reductive base of mental phenomena in cognitive science, including neural computations? This is not an option we should contemplate. If we were to concede to the anti-realist here, anti-realism would spread to other entities outside cognitive science. Atoms and electrons – large collections of them – are among the (likely) reductive bases of human mental life. Collections of atoms and electrons realise (or constitute, ground, etc.) at least some human mental phenomena. The atoms and electrons occupying the space where you sit now are sufficient to produce (some aspects of) your mental life. If one were to replicate these atoms and electrons, one would replicate those mental phenomena. This conditional holds true over a non-trivial range of modal scenarios. But then the argument of this section can be applied to these collections of atoms and electrons. At a push, one might concede anti-realism about neural computation. But conceding anti-realism about atoms and electrons on the basis of the argument above seems madness.&lt;/p&gt;
&lt;p&gt;Let us see how to respond to the puzzle in a way that does not grant a win to the anti-realist.&lt;/p&gt;
&lt;h1 id=&#34;solutions-to-the-puzzle&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Solutions to the puzzle&lt;/h1&gt;
&lt;p&gt;Each of the proposals described in this section solve the puzzle by distinguishing between two types of mind dependence. Reductive theories in cognitive science involve one form of mind dependence whereas anti-realism about the subject matter involves another. The hard question is how to draw the distinction between a (trivial) reductive form of mind dependence and a (non-trivial) anti-realist form of mind dependence. In this section, I examine three ways to do this. The first two distinguish the two kinds of mind dependence based on dependence on the mind of the &lt;em&gt;subject&lt;/em&gt; versus dependence on the mind of an &lt;em&gt;enquirer&lt;/em&gt;. I argue that this approach is unlikely to succeed. My favoured proposal is based on attending to the structured nature of the reductive base in cognitive science. The two forms of mind dependence can be distinguished as dependence of the &lt;em&gt;component parts and relations&lt;/em&gt; of the reductive base on minds (non-trivial and the point of disagreement in realist/anti-realist disputes) versus dependence of the &lt;em&gt;whole reductive base&lt;/em&gt; on minds (trivial and entailed by reduction).&lt;/p&gt;
&lt;h2 id=&#34;dependence-on-the-enquirer-versus-the-subject&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Dependence on the enquirer versus the subject&lt;/h2&gt;
&lt;p&gt;The first way to distinguish the two forms of mind dependence is to ask on &lt;em&gt;whose&lt;/em&gt; mind the reductive base depends. When we described a neural computation that determines whether Abby goes to the fridge for a beer, we said that Abby’s neural computation trivially depends on &lt;em&gt;her&lt;/em&gt; mind but we did not comment on whether it depends on the mind of &lt;em&gt;anyone else&lt;/em&gt;. One might propose that anti-realism about neural computations is a claim about dependence on observers, not a claim about dependence on the subject being observed. Anti-realism in general is the claim that the world depends on how enquirers see or conceive of it. It does not depend on how the subject being studied sees it. Cognitive science appears to be special only in that the subject being observed has a mind. Whether the subject has a mind or not should be irrelevant to the anti-realist. Her concern is not to establish dependence of the subject on her own mind but to establish dependence of the subject on the mental life of others.&lt;/p&gt;
&lt;p&gt;Drawing the distinction this way also fits with the practice of cognitive science. Both the realist and the anti-realist can agree that the reductive base of some experimental subject’s mental life depends on that subject’s mind in the way described by the puzzle. If the experimental subject were not to have a mind, or if she were to have a radically different mind, the reductive base would be different. But the realist and the anti-realist can disagree about whether the reductive base depends on the minds of external enquirers. No justification for this flows from the reductive claim. We can state our distinction between two kinds of mind dependence as follows. &lt;em&gt;Reductive&lt;/em&gt; mind dependence is dependence on the subject’s own mental life. &lt;em&gt;Anti-realist&lt;/em&gt; mind dependence is dependence on the mental life of others, specifically the enquirers who study and ascribe properties to the reductive base.&lt;/p&gt;
&lt;p&gt;This way of drawing the distinction handles many cases, but not all. The problem is that there is no reason to believe that two separate persons are necessary to do cognitive science. An experimental subject could, in principle, perform experiments on herself. She could provide evidence and ascribe to her own brain specific neural representations. In this case, the proposal for distinguishing two kinds of mind dependence would fail. There would not be two separate minds (subject and enquirer), so there would not be two kinds of mind dependence. Both collapse to dependence on the subject’s own mind. The solution to the puzzle must lie elsewhere.&lt;/p&gt;
&lt;h2 id=&#34;dependence-on-second-order-mental-states&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Dependence on second-order mental states&lt;/h2&gt;
&lt;p&gt;One might try to finesse the previous proposal by looking for a difference &lt;em&gt;within&lt;/em&gt; a subject’s mental life between her &lt;em&gt;enquirer-like&lt;/em&gt; and &lt;em&gt;subject-like&lt;/em&gt; aspects. If these two aspects could be identified, we could map them onto our two kinds of mind dependence. But how to draw this distinction? One thought is that enquirer-like aspects of mental life are distinguished by &lt;em&gt;being about&lt;/em&gt; other aspects of mental life. A subject may have all sorts of mental states (beliefs, desires, and so on). What is special about her enquirer-like thoughts is that they are &lt;em&gt;about&lt;/em&gt; aspects of her mental life. Enquirer-like thoughts are second-order thoughts about the mental life of a subject. The second-order thoughts might occur within a separate person (an external enquirer) or within the same person (a subject who is her own enquirer). We therefore avoid the counterexample above of the subject who is her own enquirer. On this view, &lt;em&gt;reductive&lt;/em&gt; mind dependence would be dependence on a subject’s own mental life. &lt;em&gt;Anti-realist&lt;/em&gt; mind dependence would be dependence on second-order mental states, either of the subject or some other enquirer, which are about that subject’s mental life.&lt;/p&gt;
&lt;p&gt;The problem is that this proposal’s characterisation of anti-realism fails to ft many plausible forms of anti-realism. Consider &lt;span class=&#34;citation&#34; data-cites=&#34;Blackburn93&#34;&gt;Blackburn (1993)&lt;/span&gt;’s anti-realist reading of Hume’s view on causation. According to Blackburn, the existence and nature of causal relations depends on our cognitive apparatus – Hume is, in this sense, an anti-realist about causation. But Blackburn does not say that causation depends on our &lt;em&gt;representational&lt;/em&gt; mental states, such as our beliefs or desires about causation. Causation depends on a different feature of our mental life: our dispositions to make certain inferences. Whether &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; causes &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; depends on our disposition to readily infer the occurrence of &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; from the occurrence of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. We have here anti-realism but not dependence on representational mental states.&lt;/p&gt;
&lt;p&gt;Following this model, a form of anti-realism about cognitive science – for example, anti-realism about neural computation – need not say the relevant entities depend on anyone’s representational mental states. Indeed, an anti-realist need not say that we have mental representations at all. She might say that neural computations depend on non-representational aspects of our mental life (for example, our dispositions to make certain inferences). The distinction between first and second-order mental states only makes sense in the context of representational mental states. If anti-realism does not require representational states, the first-order/second-order distinction cannot be used to distinguish anti-realism from realism. Some other form of mind dependence must be at issue.&lt;/p&gt;
&lt;h2 id=&#34;dependence-of-the-parts-versus-the-whole-on-minds&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.3&lt;/span&gt; Dependence of the parts versus the whole on minds&lt;/h2&gt;
&lt;p&gt;The previous two proposals try to partition the mind into subject-like and enquirer-like parts. In certain cases, this may be feasible (for example, when subject and enquirer are in two different people). But in general, it is difficult to know what distinguishes an enquirer-like aspect of the mental world from a subject-like aspect of the mental world. The proposal in this section adopts a different strategy. Rather than try to partition the mental realm into subject-like and enquirer-like parts, let us instead attend to partitions already given to us by theories in cognitive science: partitions in the reductive base.&lt;/p&gt;
&lt;p&gt;The structured nature of the reductive base is important to cognitive science. Theories in cognitive science do not reduce a mental phenomenon to a single, undifferentiated entity. They reduce mental phenomena to a structured entity that consists of multiple individual parts and relations. Which parts and relations these are varies between theories: they might be computational steps, mechanisms, networks, dynamic relations, or causal sequences of events. For example, a theory that identifies an instance of decision making with a neural computation, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;, does not reduce decision making to a single, atomic individual, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;. Rather, the theory identifies decision making with a structured entity, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;, composed of multiple parts (perhaps including representations of environmental states, representations of utilities, and individual functional parts) and multiple relations (causal, syntactic, and other relations) that together are (or realise, constitute, ground) decision making.&lt;/p&gt;
&lt;p&gt;Observe that the puzzle described in the previous section only entails that the reductive base &lt;em&gt;as a whole&lt;/em&gt; is mind dependent. The reductive base cannot occur without its associated mental phenomenon. But nothing follows from this regarding the mind dependence of the &lt;em&gt;individual parts and relations&lt;/em&gt;. Mind dependence of the whole reductive base does not require mind dependence of the parts. The same argument as above cannot be run for the parts as there is no reason to suppose that any of the individual parts or relations would, by itself, be sufficient for a mental phenomenon. There is nothing contradictory in supposing that a part or relation of the reductive base can occur individually without any specific condition involving mental agents being met. For example, suppose that an instance of decision making is a specific neural computation. That entire neural computation is mind dependent: it cannot occur without the associated mental phenomenon. But this does not mean that the individual parts and relations that compose the computation are also mind dependent. It is possible that the individual parts and relations – the representations of environment states, the smaller functional units, the causal relations – could occur individually without any condition being met concerning mental agents. It is also possible that one or more of the parts and relations is mind dependent. Parts and relations may be mind dependent or fail to be mind dependent even if the whole reductive base is not mind dependent. There is scope for different anti-realist views by adopting the mind-dependence claim about different parts or relations: one might, for example, be an anti-realist about causal relations or about syntactic properties. By contrast, there is only one way to be a realist: hold that none of the constituent parts and relations is mind dependent. Each part or relation could occur individually without a further condition being met regarding mental agents. Hence, we can draw our distinction. &lt;em&gt;Reductive&lt;/em&gt; mind dependence is dependence of the whole reductive base on a mental phenomenon. &lt;em&gt;Anti-realist&lt;/em&gt; mind dependence is dependence of one or more of the constituent individual parts or relations that make up the reductive base. Reductive mind dependence is entailed by the reductive claim. Anti-realist mind dependence is not.&lt;/p&gt;
&lt;p&gt;How do we know this is the right way to draw our distinction? Recall that what is at stake is the ambition to naturalise the mind: the attempt to show how mental life arises from non-mental ingredients. A naturalising explanation explains properties of a mental phenomenon in terms of the individual parts and relations of the reductive base. Whether the form of explanation in question is functional explanation, mechanistic explanation, computational explanation, causal explanation, or some other form of explanation, it consists in citing the individual parts and how they are arranged by relations in the reductive base. The individual parts and their relations explain the scientifically relevant properties of the mental phenomenon. As we defined it above, an explanation is naturalising only if its &lt;em&gt;explanans&lt;/em&gt; does not refer to or otherwise presuppose mental phenomena. The relevant explanations in cognitive science appeal to the individual parts and relations of the reductive base. If we are realists about those parts and relations, we can appeal to them in our explanations without presupposing further conditions being met about mental phenomena. Conversely, if the naturalising project is to succeed, we must be able to be realists about the relevant parts and relations referred to by our explanations. In contrast, if one or more of the parts or relations that make up the reductive base are mind dependent, then an explanation that cites them will fail to naturalise the mental phenomenon. That the proposed distinction aligns with the fate of our naturalising ambitions indicates that we are on the right track here.&lt;/p&gt;
&lt;p&gt;Consider an analogy. You see a miniature castle in a shop window. You want to explain some of the castle’s properties: why it can bear so much weight or why it is resistant to attack by scrunched-up paper balls. You aim for your explanation to be ‘naturalistic’: to explain the castle’s properties in non-castle-involving terms. You do not want that explanation to make reference to or otherwise presuppose castles. Closer inspection reveals that the castle is built from Lego bricks. You make a reductive claim: the castle is (or is realised by, or is constituted by) this specific configuration of Lego bricks. Armed with this reductive claim, you can explain the effects first noted. The individual Lego bricks and their specific configuration explain the ability of the castle to bear weight. The individual Lego bricks and their configuration explain the resistance of the castle to attack by paper balls. Someone might object at this point that, according to your hypothesis, the castle is this configuration of Lego bricks. Hence, you have not explained the castle in non-castle-involving terms. You reply, rightly, that this kind of castle involvement does not matter to your naturalistic ambitions. The specific configuration of Lego bricks is a castle, but the individual bricks and their relations are not. Your &lt;em&gt;explanans&lt;/em&gt; cites those individual bricks and their relations not the configuration as an atomic whole. You have explained weight bearing and resistance to attack in terms of the powers of these parts and relations, neither of which are castles or are castle-dependent. That is all that is required to naturalistically explain the castle’s properties. Now suppose that one were to discover, to great surprise, that the individual Lego bricks do essentially depend on castles. Perhaps some Lego bricks contain tiny castles. The structured configuration of Lego bricks is now castle dependent in a new and more troublesome way. The original naturalising ambition – explaining the castle’s weight bearing and resistance to attack without making reference to or otherwise presupposing castles – would fail.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;I have argued that what matters to the realist/anti-realist dispute in cognitive science is not whose mind the reductive base depends on (subject versus enquirer). Rather, it is the mind dependence of the individual parts and relations versus the (trivial) mind dependence of the reductive base as a whole. The status of the individual nuts and bolts that realise cognition matters. Whether a specific configuration of nuts and bolts taken as a whole is mind dependent is irrelevant to realism and to the naturalising project. Perhaps surprisingly, the structured nature of the reductive base in cognitive science and cognitive science’s parallel emphasis on structured explanation (whether that be functional, mechanistic, computational, causal, or another form of explanation via appeal to a structure) turns out to be essential to articulating the realist/anti-realist dispute in this area. The relevant form of anti-realism targets one or more entities in that structure. If someone claims to be an anti-realist about cognitive science, the first question one should ask is: About which entities in the reductive base are you anti-realist? The next question should aim to discover whether those entities really do play an essential role in the reductive base of cognition.&lt;/p&gt;
&lt;h1 id=&#34;bibliography&#34; class=&#34;unnumbered&#34;&gt;Bibliography&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Blackburn93&#34;&gt;
&lt;p&gt;Blackburn, S. 1993. “Hume and Thick Connexions.” In &lt;em&gt;Essays in Quasi-Realism&lt;/em&gt;, 94–107. Oxford: Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Block07&#34;&gt;
&lt;p&gt;Block, N. 2007. “Consciousness, Accessibility, and the Mesh Between Psychology and Neuroscience.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 30:481–548.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Burge86&#34;&gt;
&lt;p&gt;Burge, T. 1986. “Individualism and Psychology.” &lt;em&gt;Philosophical Review&lt;/em&gt; 95:3–45.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Churchland81&#34;&gt;
&lt;p&gt;Churchland, P. M. 1981. “Eliminative Materialism and the Propositional Attitudes.” &lt;em&gt;The Journal of Philosophy&lt;/em&gt; 78:67–90.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Dennett91&#34;&gt;
&lt;p&gt;Dennett, D. C. 1991a. &lt;em&gt;Consciousness Explained&lt;/em&gt;. Boston, MA: Little, Brown &amp;amp; Company.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Dennett91a&#34;&gt;
&lt;p&gt;———. 1991b. “Real Patterns.” &lt;em&gt;The Journal of Philosophy&lt;/em&gt; 88:27–51.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Devitt91&#34;&gt;
&lt;p&gt;Devitt, M. 1991. &lt;em&gt;Realism and Truth&lt;/em&gt;. 2nd ed. Princeton, NJ: Princeton University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Fodor75&#34;&gt;
&lt;p&gt;Fodor, J. A. 1975. &lt;em&gt;The Language of Thought&lt;/em&gt;. Sussex: The Harvester Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Fodor80&#34;&gt;
&lt;p&gt;———. 1980. “Methodological Solipsism Considered as a Research Strategy in Cognitive Psychology.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 3:63–109.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Godfrey-Sm16&#34;&gt;
&lt;p&gt;Godfrey-Smith, P. 2016. “Dewey and the Question of Realism.” &lt;em&gt;Noûs&lt;/em&gt; 50:73–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-GoldShadlen01&#34;&gt;
&lt;p&gt;Gold, I. J., and M. N. Shadlen. 2001. “Neural Computations That Underlie Decisions About Sensory Stimuli.” &lt;em&gt;Trends in Cognitive Sciences&lt;/em&gt; 5:10–16.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-GoldShadlen07&#34;&gt;
&lt;p&gt;———. 2007. “The Neural Basis of Decision Making.” &lt;em&gt;Annual Review of Neuroscience&lt;/em&gt; 30:535–74.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Irvine12&#34;&gt;
&lt;p&gt;Irvine, E. 2012. &lt;em&gt;Consciousness as a Scientific Concept&lt;/em&gt;. Dordrecht: Springer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Miller12&#34;&gt;
&lt;p&gt;Miller, A. 2012. “Realism.” In &lt;em&gt;The Stanford Encyclopedia of Philosophy&lt;/em&gt;, edited by E. N. Zalta, Spring 2012. &lt;a href=&#34;http://plato.stanford.edu/archives/spr2012/entries/realism/&#34; class=&#34;uri&#34;&gt;http://plato.stanford.edu/archives/spr2012/entries/realism/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Putnam88&#34;&gt;
&lt;p&gt;Putnam, H. 1988. &lt;em&gt;Representation and Reality&lt;/em&gt;. Cambridge, MA: MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-RangelCamerer08&#34;&gt;
&lt;p&gt;Rangel, A., C. Camerer, and P. R. Montague. 2008. “A Framework for Studying the Neurbiology of Value-Based Decision Making.” &lt;em&gt;Nature Reviews Neuroscience&lt;/em&gt; 9 (545–556).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Ryle49&#34;&gt;
&lt;p&gt;Ryle, G. 1949. &lt;em&gt;The Concept of Mind&lt;/em&gt;. London: Hutchinson.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-SchultzDayan97&#34;&gt;
&lt;p&gt;Schultz, W., P. Dayan, and P. Read Monague. 1997. “A Neural Substrate of Prediction and Reward.” &lt;em&gt;Science&lt;/em&gt; 275:1593–9.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Searle92&#34;&gt;
&lt;p&gt;Searle, J. R. 1992. &lt;em&gt;The Rediscovery of the Mind&lt;/em&gt;. Cambridge, MA: MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Shoemaker07&#34;&gt;
&lt;p&gt;Shoemaker, S. 2007. &lt;em&gt;Physical Realization&lt;/em&gt;. Oxford: Clarendon Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Sperling60&#34;&gt;
&lt;p&gt;Sperling, G. 1960. “The Information Available in Brief Visual Presentations.” &lt;em&gt;Psychological Monographs: General and Applied&lt;/em&gt; 74:1–29.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Wilson01&#34;&gt;
&lt;p&gt;Wilson, R. A. 2001. “Two Views of Realization.” &lt;em&gt;Philosophical Studies&lt;/em&gt; 104:1–30.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;section class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I assume we are considering the total realiser here &lt;span class=&#34;citation&#34; data-cites=&#34;Shoemaker07&#34;&gt;(Shoemaker 2007)&lt;/span&gt;. Changing to talk about the core realiser would not improve matters as core realisers have further worries pertaining to their mind dependence &lt;span class=&#34;citation&#34; data-cites=&#34;Wilson01&#34;&gt;(Wilson 2001)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;Godfrey-Sm16&#34;&gt;Godfrey-Smith (2016)&lt;/span&gt; argues that this reply is wrong: causal dependence on minds is relevant to realism.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Philosophy of the psychological and cognitive sciences</title>
      <link>http://marksprevak.com/publications/philosophy-of-the-psychological-and-cognitive-sciences-2016/</link>
      <pubDate>Mon, 13 Nov 2017 00:00:00 UTC</pubDate>
      <author>Mark Sprevak</author>
      <guid>http://marksprevak.com/publications/philosophy-of-the-psychological-and-cognitive-sciences-2016/</guid>
      <description>&lt;div&gt;
&lt;h1 id=&#34;introduction&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;Philosophy of the psychological and cognitive sciences is a broad and heterogeneous domain. The psychological and cognitive sciences are rapidly evolving, fragmented, and often lacking in theories that are as precise as one might like. Consequently, philosophers of science have a challenging subject matter: fast moving, variegated, and sometimes slightly fuzzy. Nevertheless, the psychological and cognitive sciences are fertile ground for philosophers. Philosophers can bring their skills to bear with productive effect: in interpreting scientific theories, precisifying concepts, exploring relations of explanatory and logical coherence between theories, and in typing psychological states, processes, and capacities.&lt;/p&gt;
&lt;p&gt;In this chapter, I organize work in philosophy of the psychological and cognitive science by the kind of task that philosophers have taken on. The tasks on which I focus are:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;How should we interpret theories in cognitive science?&lt;/li&gt;
&lt;li&gt;How should we precisify theoretical concepts in cognitive science?&lt;/li&gt;
&lt;li&gt;How do theories or methodologies in cognitive science fit together?&lt;/li&gt;
&lt;li&gt;How should cognitive states, processes, and capacities be individuated?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;None of these tasks is distinctively philosophical: not in the sense of being of interest only to philosophers nor in the sense that philosophers would be the best people to solve them. All of the tasks engage a wide range of inquirers; all are carried out to the highest standards by a diverse group of individuals. What marks these tasks out as special is that they are among the problems that tend to interest philosophers and to whose solution philosophers tend to be well placed to contribute. Philosophy of the psychological and cognitive sciences is not defined by questions that set it apart from other forms of inquiry. It is characterized by questions, shared with other researchers, that tend to suit the skills, and attract the interest of, philosophers.&lt;/p&gt;
&lt;p&gt;Three qualifications before proceeding. First, this chapter does not attempt to cover non-cognitive research in the psychological sciences (e.g., differential or social psychology). For the purposes of this chapter, ‘psychological science’ and ‘cognitive science’ will be used interchangeably. Second, the term ‘theory’ will be used loosely, and looser than is normal in other areas of philosophy of science. ‘Theory’ may refer to a full-fledged theory or a model, description of a mechanism, sketch, or even a general claim. Finally, the tasks I discuss, and the particular examples I give, only sample work in philosophy of the psychological and cognitive sciences. I focus on a small number of cases that I hope are illustrative of wider, structurally similar projects. The examples are not intended to be a list of the best work in the field, but only examples by which to orientate oneself.&lt;/p&gt;
&lt;p&gt;Let us consider the four tasks in turn.&lt;/p&gt;
&lt;h1 id=&#34;interpreting&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Interpreting&lt;/h1&gt;
&lt;p&gt;A crude way to divide up scientific work is between &lt;em&gt;theory building&lt;/em&gt; and &lt;em&gt;theory interpretation&lt;/em&gt;. Theory building involves identifying empirical effects and coming up with a theory to predict, test, and explain those effects. Theory interpretation concerns how – granted the empirical prowess or otherwise of a theory – we should understand that theory. Interpretation involves more than assigning a semantics. The job of interpretation is to understand the import of the theory: What is the purpose of the descriptions and physical models used to express the theory? What are the criteria of success for the theory: truth, empirical adequacy, instrumental value, or something else? Which terms in the theory are referring terms? Which kinds of ontological commitments does a theory entail? Which aspects of a theory are essential?&lt;/p&gt;
&lt;p&gt;Philosophers have tools to help with theory interpretation. Analogues to the questions above have been pursued for ethical, normative, mathematical, and other scientific discourses. A range of options have been developed concerning theory interpretation, amongst which are versions of realism and instrumentalism. A theory interpretation set a combination of semantic, pragmatic, and ontological parameters regarding a theory. Our job is to see which setting results in the best interpretation of a psychological theory.&lt;/p&gt;
&lt;p&gt;These concerns have recently played out with Bayesian models of cognition. Bayesian models predict and explain an impressive array of human behavior. For example, Bayesian models provide a good, predictive model of human behavior in sensory cue integration tasks &lt;span class=&#34;citation&#34; data-cites=&#34;ErnstBanks02&#34;&gt;(Ernst and Banks 2002)&lt;/span&gt;. In these tasks, subjects are presented with a single stimulus in two different sensory modalities (say, touch and vision) and asked to make judgments about that stimulus by combining information from the two modalities. For example, subjects may be presented with two ridges and asked to decide, using touch and vision, which ridge is taller. Ernst and Banks found that subjects’ behavior could be predicted if we assume that the input to each sensory modality is represented by a probability density function and that these representations are combined using the Bayesian calculus to yield a single estimate. Probability density functions and Bayesian computational machinery do a good job of predicting human behavior in sensory cue integration. Bayesian models also appear to explain human behavior. This is because they tie human behavior to the optimal way in which to weigh evidence. Many aspects of human behavior have been modeled with this kind of Bayesian approach, including causal learning and reasoning, category learning and inference, motor control, and decision making &lt;span class=&#34;citation&#34; data-cites=&#34;PougetBeck13&#34;&gt;(Pouget et al. 2013)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;How should we interpret these Bayesian models?&lt;/p&gt;
&lt;p&gt;One option is &lt;em&gt;realism&lt;/em&gt;. This is known within psychology as the &lt;em&gt;Bayesian brain hypothesis&lt;/em&gt;. Here, the central terms of Bayesian models – the probability density functions and Bayesian computational methods – are interpreted as picking out real (and as yet unobserved) entities and processes in the human brain. The brain ‘represents information probabilistically, by coding and computing with probability density functions or approximations to probability density functions,’ and those probabilistic representations enter into Bayesian, or approximately Bayesian, inference via neural computation &lt;span class=&#34;citation&#34; data-cites=&#34;KnillPouget04&#34;&gt;(Knill and Pouget 2004, 713)&lt;/span&gt;. Bayesian models are &lt;em&gt;both&lt;/em&gt; a theory of human behavior &lt;em&gt;and&lt;/em&gt; of the neural and computational machinery that underpin the behavior. Realism about Bayesian models is usually qualified with the claim that current Bayesian models of cognition are only approximately true. What current Bayesian models get right is that the brain encodes information probabilistically and that it implements some form of approximate Bayesian inference. The precise format of the probabilistic representations and the precise method of approximate Bayesian inference is left open to future inquiry &lt;span class=&#34;citation&#34; data-cites=&#34;GriffithsChater12&#34;&gt;(Griffiths et al. 2012)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Another interpretation option for Bayesian models is &lt;em&gt;instrumentalism&lt;/em&gt; &lt;span class=&#34;citation&#34; data-cites=&#34;BowersDavis12 ColomboSeries12 Danks09 JonesLove11&#34;&gt;(Bowers and Davis 2012; Colombo and Seriès 2012; Danks 2008; Jones and Love 2011)&lt;/span&gt;. According to the instrumentalist, Bayesian models do not aim to describe the underlying neurocomputational mechanisms (other than providing general constraints on their inputs and outputs). The central terms of Bayesian models – probability density functions and Bayesian computational machinery – should be understood, not as referring to hidden neural entities and processes, but as formal devices that allow experimenters to describe human behavioral patterns concisely. The instrumentalist allows that the underlying neural mechanisms could be Bayesian. But this possibility should be distinguished from the content of current Bayesian models. The aim of those models is to predict behavior. The success of Bayesian models in predicting behavior is not evidence that the mechanisms that generate that behavior are Bayesian. One might be inspired by the success of Bayesian models in predicting behavior to entertain the Bayesian brain hypothesis. But inspiration is not evidential support. Bayesian models should be understood as aiming at behavioral adequacy. Their aim is to predict behavior and specify human behavioral competences, not to describe neural or computational mechanisms.&lt;/p&gt;
&lt;p&gt;Here we have two incompatible proposals about how to interpret Bayesian models of cognition. The difference between the two proposals matters. Do Bayesian models tell us how our cognitive processes work? According to realism, they do. According to instrumentalism, they do not (or, at least, they only provide constraints on inputs and outputs). How do we decide which interpretation option is correct?&lt;/p&gt;
&lt;p&gt;The primary rationale for instrumentalism is epistemic caution. Instrumentalism makes a strictly weaker claim than realism while remaining consistent with the data. There appear to be good reasons for epistemic caution too. First, one might worry about underdetermination. Many non-Bayesian mechanisms generate Bayesian behavior. A lookup table, in the limit, can generate the same behavior as a Bayesian mechanism. Why should we believe that the brain uses Bayesian methods given the vast number of behaviorally indistinguishable, non-Bayesian, alternatives? Second, one might worry about the underspecification of mechanisms in current Bayesian models. The Bayesian brain hypothesis is a claim about neurocomputational mechanisms. There are a huge number of ways in which a behaviorally-adequate Bayesian model could be implemented, both neurally and computationally. Current Bayesian models tend to be silent about their neural or computational implementation in actual brains. Absent specification of the neurocomputational implementation, we should most charitably interpret current Bayesian theories as simply not making a claim about neural or computational mechanisms at all.&lt;/p&gt;
&lt;p&gt;What reasons are there for realism? A common inductive inferential pattern in science is to go beyond an instrumentalist interpretation if a theory has a sufficiently impressive track record of prediction and explanation &lt;span class=&#34;citation&#34; data-cites=&#34;Putnam75e&#34;&gt;(Putnam 1975)&lt;/span&gt;. Arguably, Bayesian models do have such a track record. Therefore, our interpretation of Bayesian models of cognition should be realist. The burden on the instrumentalist to show otherwise: to show that the brain does &lt;em&gt;not&lt;/em&gt; use probabilistic representations or Bayesian inference. And current scientific evidence gives us no reason to think that the brain does not use Bayesian methods &lt;span class=&#34;citation&#34; data-cites=&#34;Rescorla15&#34;&gt;(Rescorla 2016)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The disagreement between the realist and the instrumentalist is not epiphenomenal to scientific practice. The choice one makes affects whether, and how, experimental results bear on Bayesian models. For example, if instrumentalism is correct, then no neural evidence could tell in favor (or against) a Bayesian model. The reason is straightforward: the models are not in the business of making any claim about neural implementation, so there is nothing in the model for neural evidence to contest. If realism about Bayesian models is correct, then neural evidence &lt;em&gt;is&lt;/em&gt; relevant to confirming the Bayesian models. If there is neural evidence that the Bayesian model’s probability distributions or methods occur in the brain, then that is evidence in favor of the model. If there is evidence against, that is evidence against the Bayesian model. Bayesian models are evaluated not only by their fit to behavioral data, but also by their neural plausibility.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;BowersDavis12&#34;&gt;Bowers and Davis (2012)&lt;/span&gt; object that Bayesian theories of cognition are trivial or lacking empirical content. Their objection is that almost any dataset could be modeled as the output of some or other Bayesian model. Specific Bayesian models could be confirmed or disconfirmed by empirical data, but the general Bayesian approach is bound to succeed no matter what. Bayesianism is no more confirmed by behavioral data than number theory is confirmed. If the instrumentalist is right, then Bowers and Davis’s objection has bite. Some or another Bayesian model will always fit a behavioral dataset. However, if the realist is right, then it is no longer clear that Bowers and Davis’s objection succeeds. Realism raises the stakes of Bayesianism. This opens up the possibility that Bayesianism could be subject to empirical test. Precisely how to test it is not yet obvious. The reason is that currently there are no agreed proposals about the neural implementation of Bayesian models’ theoretical posits (probability density functions and Bayesian computational processes). Nevertheless, realism at least opens the door to the possibility of testing Bayesianism. Suppose one were to have a theory of neural implementation in hand. If the brain’s measured neural representations and computations – identifiable via the implementation proposal – really do have the properties ascribed by Bayesianism, then the general Bayesian approach would be vindicated. If not – for example, if the brain turns out to employ non-probabilistic representations or to manipulate its representations via a lookup table – then Bayesianism about cognition would be found to be false. A theory of implementation plus neural evidence allows Bayesianism about cognition to be tested.&lt;/p&gt;
&lt;p&gt;Interpreting a theory requires making decisions about the theory’s goals (truth vs. instrumental accuracy) and how to interpret its theoretical terms (referring vs. formal devices). A final aspect of interpretation is the decision about &lt;em&gt;which&lt;/em&gt; claims a theory includes. Which parts of a theory are essential, and which are explication or trimming?&lt;/p&gt;
&lt;p&gt;Suppose that realism about Bayesian models is correct and therefore that the brain manipulates probabilistic representations. What does a Bayesian model require of these representations? Some things are clearly required: the representations must be probabilistic hypotheses, and they must encode information about the uncertainty of events as well as their truth conditions or accuracy conditions. But to &lt;em&gt;which&lt;/em&gt; entities and events do these probabilistic hypotheses refer? Do they refer to distal objects in the environment (e.g., tables and chairs) or to mathematical entities (e.g., numerical values of a parameter in an abstract graphical model). In other words, does the correct interpretation of a Bayesian model of cognition make reference to distal objects in the organism’s environment, or is the correct interpretation entirely formal and mathematical?&lt;/p&gt;
&lt;p&gt;On either view, entertaining a probabilistic hypothesis could enable an organism to succeed. On the first, ‘distal content’, option, this could be because the probabilistic hypotheses are the organism’s best guess about the state of its environment, and that appears to be useful information for an organism to consider when deciding how to act in its environment &lt;span class=&#34;citation&#34; data-cites=&#34;Rescorla15a&#34;&gt;(Rescorla 2015)&lt;/span&gt;. On this view, neural representations of distal environment stimuli would be an essential part of a Bayesian story, not an optional extra. If this is correct, it would mean that Bayesianism about cognition is incompatible with eliminativism or fictionalism about neural representations of distal objects &lt;span class=&#34;citation&#34; data-cites=&#34;Keijzer98 McDowell10 Sprevak13&#34;&gt;(Keijzer 1998; McDowell 2010; Sprevak 2013)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;On the second, ‘mathematical content’, option, the organism could reliably succeed because the probabilistic hypotheses describe a mathematical structure that is adaptive for the organism to consider in its environment. This need not be because the mathematical structure represents distal properties in the environment. There are other ways than representation in which inferences over a formal structure could reliably lead to successful action. Formal properties may track contingent nomological connections between the organism’s environment, body, and sensory system. Consider that it is adaptive for an organism to consider the result of applying a Fourier transform to the values of incoming signals from the auditory nerve, or for an organism to consider the results of applying &lt;span class=&#34;math inline&#34;&gt;\(\nabla^{2}G\)&lt;/span&gt; to the values of the incoming signal from its retina. These are useful transformations for an organism to consider even if neither is a representation of a distal environmental property. &lt;span class=&#34;citation&#34; data-cites=&#34;Egan10&#34;&gt;Egan (2010)&lt;/span&gt; argues for a ‘mathematical content’ interpretation of classical computational models in cognitive science. Her reasoning could be extended to Bayesian models. According to such a view, it is adaptive for an organism to consider the mathematical relations described by the Bayesian model even though the terms in that model do not represent distal properties or events in the environment. On this view, representations of distal objects would not be an essential part of a Bayesian story. Distal content could feature in a Bayesian model of cognition, but it is not an essential part of such a model.&lt;/p&gt;
&lt;p&gt;My intention here is not to argue for one interpretation rather than another. My intention is only to illustrate that each theory in cognitive science requires interpretation. There are entailments between theory interpretation options. Just as in other areas of science, some aspects of psychological theories should be understood as essential, literal, fact stating, ontologically committing, whereas other aspects play a different role. It takes care and sensitivity to hit on the correct interpretation, or even to narrow the interpretation options down. This cannot be achieved simply by appealing to the utterances of theory builders because those utterances themselves need interpretation. Theories do not wear their interpretation on their sleeves.&lt;/p&gt;
&lt;h1 id=&#34;precisifying&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Precisifying&lt;/h1&gt;
&lt;p&gt;Imprecision is bad, not only for its own sake, but also because it permits fallacious inference. If one did not know the difference between the two senses of ‘bank,’ one might wrongly infer that since a river has two banks, a river would be a good place to conduct a financial transaction. That no such confusion occurs reflects that the relevant precisification is known to every competent speaker of the language. Unfortunately, the same is not true of every term in psychology. The correct usage of many terms in psychology – ‘consciousness’, ‘concept’, ‘module’ – is murky, even for experts. One task to which philosophers have contributed is to clarify our theoretical terms so that those terms better support reliable, non-trivial, inductive inference. This may involve distinguishing between different things that fall under a term, redefining a term, or sometimes removing a term entirely. This is not mere semantic busy work. Concepts are the building blocks of scientific theories. Fashioning precise and inductively powerful concepts is essential to scientific progress (for more on this, see &lt;span class=&#34;citation&#34; data-cites=&#34;Machery15&#34;&gt;Machery (n.d.)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;I discuss here two examples of concepts that philosophers have helped to precisely: the concept of consciousness and the concept of a cognitive module.&lt;/p&gt;
&lt;p&gt;The term ‘consciousness’ has its origin in folk use. We might say, ‘she wasn’t conscious of the passing pedestrian,’ ‘he was knocked unconscious in the boxing ring,’ or speak of the ‘conscious’ experience of smelling a rose, making love, or hearing a symphony, making life worth living. Scientific and philosophical work on consciousness only started to make progress when it distinguished different things that fall under the folk term. Precisifying the concept of consciousness has enabled researchers to have a fighting chance to discover the purpose, functional mechanism, and neural basis of consciousness.&lt;/p&gt;
&lt;p&gt;A preliminary precisification of consciousness is &lt;em&gt;arousal&lt;/em&gt;. When we say that someone is conscious, we might mean that she is alert and awake; she is not asleep or incapacitated. When in dreamless sleep, or in a pharmacological coma, a person is unconscious. This sense of ‘consciousness’ is usually accompanied by the assumption that the predicate ‘… is conscious’ is a monadic predicate. Someone is simply conscious or unconscious; they need not be conscious &lt;em&gt;of&lt;/em&gt; something specific. Someone may also be conscious in the sense of being aroused without being capable of consciousness in the sense of being aware of particular stimuli. Patients in a vegetative state show sleep-wake cycles, hence arousal, but they are not aware of particular stimuli. The neural mechanisms that govern consciousness-as-arousal also appear distinct from those that govern consciousness-as-awareness. Arousal is regulated by neural systems in the brainstem, notably, the reticular activating system. In contrast, the neural basis of consciousness-as-awareness appears to be in higher cortical regions and their subcortical reciprocal connections. Consciousness-as-arousal and consciousness-as-awareness have different purposes, relate to different aspects of functional cognitive architecture, and have different neural implementations &lt;span class=&#34;citation&#34; data-cites=&#34;LaureysBoly09&#34;&gt;(Laureys et al. 2009)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Ned Block’s concept of access consciousness is one way to further precisify consciousness-as-awareness &lt;span class=&#34;citation&#34; data-cites=&#34;Block95a&#34;&gt;(Block 1995)&lt;/span&gt;. A mental representation is defined as access conscious if and only if ‘it is poised for free use in reasoning and for direct “rational” control of action and speech’ &lt;span class=&#34;citation&#34; data-cites=&#34;Block95a&#34;&gt;(Block 1995, 382)&lt;/span&gt;. One indicator of access consciousness is verbal reportability – whether the subject can say he or she is aware of a given mental episode. Reportability is, however, neither necessary nor sufficient for access consciousness. Block’s precisification of consciousness-as-awareness highlights a number of other properties of consciousness-as-awareness. First, access consciousness is attributed with a relational predicate: an individual is conscious &lt;em&gt;of&lt;/em&gt; something. Second, the object of that individual’s consciousness is determined by a representation encoded in their brain. Third, access consciousness requires that this representation be ‘broadcast widely’ in their brain: it should be available to central reasoning processes and able to cause a wide variety of behavior, including verbal report. Fourth, the representation need not have actual behavioral effects; it need only have the disposition to cause appropriate behavioral effects. This catalogue provides a partial functional specification of consciousness-as-awareness. Empirical work has focused on identifying which, if any, neural properties answer to this description &lt;span class=&#34;citation&#34; data-cites=&#34;Baars97 DehaeneChangeux04&#34;&gt;(Baars 1997; Dehaene and Changeux 2004)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Despite its virtues, there are idiosyncrasies in Block’s precisification of consciousness-as-awareness: Why is rational control necessary for access consciousness? What does it mean for a neural representation to be ‘poised’ to have effects but not actually have them? What does it mean for a representation to ‘directly’ control behavior given that all control of behavior is mediated by other neural systems? The best way to see Block’s account is as a stepping stone along the way to an appropriate concept of consciousness-as-awareness.&lt;/p&gt;
&lt;p&gt;Forging the right notion of consciousness-as-awareness is not a task for armchair reflection. Precisification does not proceed prior to, or independently of, empirical inquiry. Precisification involves a two-way interaction between empirical hypotheses and consideration of how changes to the concepts that make up the empirical hypotheses would better capture the patterns relevant to scientific psychology. Precisification of a concept must be informed by the utility of the resulting concept to scientific practice. A precisification of consciousness-as-awareness proves its worth by whether the way it groups phenomena pays off for achieving goals in scientific psychology. Groupings that yield reliable inductive inferences or explanatory unification are those that should be favored. For example, a precisification of consciousness-as-awareness should aim to pick out shared facts about purpose, cognitive functional architecture, and neural implementation of consciousness-as-awareness. Recent work suggests that consciousness-as-awareness should be split into smaller concepts as no one concept meets all three of these conditions &lt;span class=&#34;citation&#34; data-cites=&#34;DehaeneChangeux06 KochTsuchiya06&#34;&gt;(Dehaene et al. 2006; Koch and Tsuchiya 2006)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The concepts of consciousness-as-arousal and consciousness-as-awareness are distinct from the concept of phenomenal consciousness. The concept of phenomenal consciousness picks out the qualitative feel – ‘what it is like’ – associated with some mental episodes. It feels a certain way to taste chocolate; it feels a certain way to taste mint; and those two feelings are different. Phenomenal consciousness is characterized purely ostensively and from a subjective, first-person point of view. Consider your mental life, pay attention to the qualitative feelings that accompany certain episodes – those are phenomenally conscious feelings. Given that our concept of phenomenal consciousness is rooted in first-person reflection, it is not surprising that this concept has proven hard to place in relation to scientific concepts related to brain function. &lt;span class=&#34;citation&#34; data-cites=&#34;Dehaene14&#34;&gt;Dehaene (2014)&lt;/span&gt; suggests that the concept of phenomenal consciousness should for the moment be set aside in pursuing a science of consciousness; phenomenal consciousness is not currently amenable to scientific explanation.&lt;/p&gt;
&lt;p&gt;In his introduction to modularity, &lt;span class=&#34;citation&#34; data-cites=&#34;Fodor83&#34;&gt;Fodor (1983)&lt;/span&gt; lists nine features that characterize a cognitive module: domain specificity, mandatory operation, limited central accessibility, fast processing, informational encapsulation, shallow outputs, fixed neural architecture, characteristic and specific breakdown patterns, and characteristic ontogenetic pace and sequencing. Whether a mechanism counts as a module depends on whether it meets a weighted sum of these features to an ‘interesting’ extent &lt;span class=&#34;citation&#34; data-cites=&#34;Fodor83&#34;&gt;(Fodor 1983, 37)&lt;/span&gt;. What counts as interesting, and how different features from the list should be weighed, is left largely unspecified. As one might imagine, there is room for precisifying the concept of modularity in different ways. Fodor claimed that the nine listed properties typically co-occur. Subsequent work has shown that they do not. Moreover, even if they did co-occur, their co-occurrence would not necessarily be of interest to scientific psychology &lt;span class=&#34;citation&#34; data-cites=&#34;ElsabbaghKarmiloff06 Prinz06a&#34;&gt;(Elsabbagh and Karmiloff-Smith 2006; Prinz 2006)&lt;/span&gt;. The concept of a psychological module has since been precisified in different ways and for different purposes. This has been done by giving priority to different properties associated with modularity from Fodor’s list.&lt;/p&gt;
&lt;p&gt;Fodor himself gives highest priority to two properties from the list: domain specificity and informational encapsulation &lt;span class=&#34;citation&#34; data-cites=&#34;Fodor00&#34;&gt;(Fodor 2000)&lt;/span&gt;. These properties are distinct. Domain specificity is a restriction on the inputs that a mechanism may receive. A mechanism is domain specific if only certain representations turn the module on, or are processed by the module. For example, in the visual system, a domain-specific module might only process information about retinal disparity and ignore everything else. Informational encapsulation is different. A mechanism is informationally encapsulated if, once the mechanism is processing an input, the information that the mechanism may then draw on is less than the sum total of information in the cognitive system. For example, in the visual system an informationally encapsulated module that processes information about retinal disparity might not be able to draw on the system’s centrally held beliefs. Illusions like the Müller–Lyer illusion appear to show that the visual system is, to some extent, informationally encapsulated.&lt;/p&gt;
&lt;p&gt;This gets us in the right ball park for precisifying the concept of a cognitive module, but domain specificity and information encapsulation need to be more carefully characterized. Informational encapsulation requires that something like the following three further conditions be met &lt;span class=&#34;citation&#34; data-cites=&#34;Samuels05&#34;&gt;(Samuels 2005)&lt;/span&gt;. First, the informational encapsulation should not be short-lived; it should be a relatively enduring characteristic of the mechanism. Second, informational encapsulation should not be the product of performance factors, such as fatigue, lack of time, or lapses in attention. Third, informational encapsulation need not shield the mechanism from every external influence. Processing may, for example, be affected by attentional mechanisms &lt;span class=&#34;citation&#34; data-cites=&#34;Coltheart99&#34;&gt;(Coltheart 1999)&lt;/span&gt;. Informational encapsulation requires ‘cognitive impenetrability.’ Roughly, this means that although the mechanism may be modulated by certain informational factors (e.g., attention), it cannot be modulated by others (e.g., the high-level beliefs, goals, or similar representational states of the organism). Pinning this down more precisely requires work (see &lt;span class=&#34;citation&#34; data-cites=&#34;Machery15a&#34;&gt;Machery (2015)&lt;/span&gt;; &lt;span class=&#34;citation&#34; data-cites=&#34;FirestoneScholl15&#34;&gt;Firestone and Scholl (2015)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Both domain specificity and informational encapsulation admit of degrees. Not just any step down from complete informational promiscuity produces domain specificity or information encapsulation. Moreover, the step down is not merely numerical, but also a matter of kind. The input domain and the informational database should be, in some sense, unified. One should be able to characterize the mechanism as a module for &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is some task or process that makes sense as a single unit in the light of concerns about the purpose and cognitive architecture of the organism. Illuminating the precise nature of this constraint on modularity is non-trivial.&lt;/p&gt;
&lt;p&gt;The concepts of domain specificity and informational encapsulation offer scope for the development of a palette of distinct precisifications of the concept of a cognition module. If one turns attention to other properties associated with modularity in Fodor’s list, more scope for divergent precisifications emerges. One could think of Fodor’s criteria as conceptually separable parameters that are important in theorizing about the mind or brain &lt;span class=&#34;citation&#34; data-cites=&#34;ElsabbaghKarmiloff06&#34;&gt;(Elsabbagh and Karmiloff-Smith 2006)&lt;/span&gt;. Some criteria may be more important for capturing certain kinds of pattern – computational, anatomical, developmental, and so on – than others. Which kind of pattern one wishes to capture depends on the interests of the investigator. Different concepts of modularity may include various combinations of these criteria. How we should separate out and sharpen the parameters of Fodor’s characterization into one or more useful working concepts depends on the kinds of pay-off previously described. A precisification of modularity should help inquirers achieve their scientific goals. It should allow us to capture empirical patterns relevant to scientific psychology. Achieving this requires a two-way interaction between empirical inquiry and reflection on how changes to the concepts that make up the empirical hypotheses would allow us to better pick up on significant empirical patterns.&lt;/p&gt;
&lt;p&gt;Before moving on, it is worth noting that there is also benefit in having imprecise concepts. Lack of precision in one’s concepts may sometimes result in a fortuitous grouping of properties by a hypothesis. And it is not always bad for different research teams to be working with different understandings of theoretical concepts or for their inquiry to proceed with underspecified concepts. The promiscuous inferences that result may sometimes be helpful in generating discovery. Nevertheless, &lt;em&gt;pace&lt;/em&gt; heuristic benefits, at some stage we need to be aware of exactly which claims are being made, and to understand when a conflict between say, two research teams, is genuine or merely verbal. Eventually, it is in everyone’s interest to precisify.&lt;/p&gt;
&lt;h1 id=&#34;understanding-how-things-hang-together&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Understanding how things hang together&lt;/h1&gt;
&lt;p&gt;Sellars described the task of philosophy as ‘to understand how things in the broadest possible sense of the term hang together in the broadest possible sense of the term’ &lt;span class=&#34;citation&#34; data-cites=&#34;Sellars63&#34;&gt;(Sellars 1963, 1)&lt;/span&gt;. One way to do this is to understand the explanatory and logical relations between theories and the ways in which they offer, or fail to offer, each other epistemic support. In this section, we examine two ways in which this is done in philosophy of the psychological and cognitive sciences. First, we examine the relationship between computational and dynamical systems theories in the psychological sciences. Second, we examine the relationship between different levels of inquiry in the psychological sciences.&lt;/p&gt;
&lt;p&gt;Computational theories and dynamical systems theories both attempt to explain cognitive capacities. Both aim to explain how, in certain circumstances, we are able to do certain tasks. However, the two theories appear to explain this in different ways.&lt;/p&gt;
&lt;p&gt;According to the computational approach, a cognitive capacity should be explained by giving a computational model of that capacity. Pick a cognitive capacity – for example, the ability to infer the three-dimensional shape of an object from information about its two-dimensional shading. An advocate of the computational approach might offer a computation that is able to solve this problem and suggest that this computation, or something like it, is implemented in the brain and causally responsible for the capacity in question &lt;span class=&#34;citation&#34; data-cites=&#34;LehkySejnowski88&#34;&gt;(Lehky and Sejnowski 1988)&lt;/span&gt;. Computational explanations are characterized by appeal to subpersonal representations and formal transformations by mechanisms built from simple components.&lt;/p&gt;
&lt;p&gt;Dynamical systems theorists also aim to explain cognitive capacities, but their explanations appear to work differently. A dynamical systems theory involves differential equations relating variables that correspond to abstract parameters and time. The dynamical systems theorist claims that these parameters also correspond to some aspects of neural and bodily activity. Differential equations describe how these parameters interact over time to generate the behavior. Dynamical systems theory explains a cognitive capacity by narrowing in on a dynamical property of the brain or body that is causally responsible for the capacity &lt;span class=&#34;citation&#34; data-cites=&#34;Schoner08&#34;&gt;(Schöner 2008)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Both computational theories and dynamical systems theories have had various successes in explaining cognitive capacities. Aspects of decision-making, such as production of the A-not-B error in infants – the phenomenon of an infant persevering in reaching for box A even though she just saw the experimenter place a desired toy in box B – are well modeled by dynamical systems theory &lt;span class=&#34;citation&#34; data-cites=&#34;ThelenSchoner01&#34;&gt;(Thelen et al. 2001)&lt;/span&gt;. Other cognitive capacities, such as those involved in inferring three-dimensional shape from two-dimensional information recorded by the retina, are well modeled by computation &lt;span class=&#34;citation&#34; data-cites=&#34;Qian97&#34;&gt;(Qian 1997)&lt;/span&gt;. Our question is: How these two theories relate? If psychology employs both, how do the theories fit together?&lt;/p&gt;
&lt;p&gt;On the face of it, they appear to be rivals &lt;span class=&#34;citation&#34; data-cites=&#34;Gelder95&#34;&gt;(Gelder 1995)&lt;/span&gt;. Each seems to instantiate a rival bet about the nature of the mind: either a cognitive capacity is produced by a computation or it is produced by a dynamical causal relation.&lt;/p&gt;
&lt;p&gt;On reflection, this seems too strong. The computational and dynamical systems approach agree on a great deal. They agree that the brain is a dynamical system. It is no part of a computational theory to deny that cognitive capacities could, or should be, explained by a time-based evolution of physical parameters; indeed, the computational approach proposes one class of paths through the space of physical parameters. Computational theories might appear to differ from dynamical systems theories in that only computational theories employ subpersonal representations. However, when explaining a psychological capacity, there are often reasons – independent of the decision to use a computational or dynamical systems theory – to introduce subpersonal representations &lt;span class=&#34;citation&#34; data-cites=&#34;Gelder95 Bechtel98&#34;&gt;(Gelder 1995, 376; Bechtel 1998)&lt;/span&gt;. Some psychological capacities are ‘representation hungry’; for example, some capacities require the system to keep track of absent stimuli &lt;span class=&#34;citation&#34; data-cites=&#34;ClarkToribio94&#34;&gt;(Clark and Toribio 1994)&lt;/span&gt;. Explaining these capacities motivates the introduction of subpersonal representations, no matter whether one places those representations in a computational or dynamical systems context. Furthermore, it is unclear whether subpersonal representations are really a necessary commitment of a computational approach. Not every state in a computation needs to be representational. It is an open question how much, if any, of a mechanism need have representational properties in a computational approach &lt;span class=&#34;citation&#34; data-cites=&#34;Piccinini08&#34;&gt;(Piccinini 2008)&lt;/span&gt;. Appeal to subpersonal representations does not show that computational and dynamical systems theories are incompatible.&lt;/p&gt;
&lt;p&gt;Dynamical systems theory appears to differ from a computational approach in that dynamical models give a special role to time. Dynamical models offer descriptions in which time is continuous and the transitions between states are governed by explicitly time-based differential equations. Computational models – for example, Turing machines – use discrete time evolution and are governed by rules that do not mention time. However, not all computational models are like Turing machines. Some computational models involve continuous time evolution and have time-based differential equations as their transition rules. Within this class are connectionist models &lt;span class=&#34;citation&#34; data-cites=&#34;Eliasmith97&#34;&gt;(Eliasmith 1997)&lt;/span&gt; and more recent realistic computational models of neural function &lt;span class=&#34;citation&#34; data-cites=&#34;Eliasmith13&#34;&gt;(Eliasmith 2013)&lt;/span&gt;. These computational models assume continuous time evolution, contain parameters that map onto a subset of neural and bodily properties, and use, as their transition rules, differential equations that specify how the parameters evolve over time. These models have all the signature properties of dynamical systems theories.&lt;/p&gt;
&lt;p&gt;The distance between computational theories and dynamical systems theories is not as great as it may first appear. In some cases, the two theories converge on the same class of model. This is not to say that every dynamical systems theory is a computational theory or vice versa. It is rather that, in the context of explaining our cognitive capacities, a computational approach and a dynamical systems approach may converge: they may employ the same elements related in the same way – representations, abstract parameters, mapping from those parameters to neural and bodily properties, continuous time evolution, and time-based differential equations. The right way to see the relationship between a computational theory and a dynamical systems theory is not as two rivals, but as two possibly compatible alternatives.&lt;/p&gt;
&lt;p&gt;This does not explain how classical, discrete, symbolic-rule-governed computational models relate to the dynamical systems approach. One suggestion is that the latter reduce to the former as a limiting case (for an attempt to show this, see &lt;span class=&#34;citation&#34; data-cites=&#34;Feldman12&#34;&gt;Feldman (2012)&lt;/span&gt;). And to understand how things hang together generally across theories in the psychological sciences, one needs to understand the relationship between computational theories, dynamical systems theories, and a wide range of other theories: statistical, Bayesian, enactivist, and others. Under which conditions are two theories rivals, compatible alternatives, or do they reduce to one another? Under which conditions do they offer each other epistemic support?&lt;/p&gt;
&lt;p&gt;A second way in which to understand how things hang together in the psychological sciences is to understand how investigation at different levels of inquiry coheres. What is the relationship between inquiry at different levels in the psychological sciences?&lt;/p&gt;
&lt;p&gt;Here, we focus on only two kinds of level: levels of spatial organization inside mechanisms and Marr’s levels of computational explanation.&lt;/p&gt;
&lt;p&gt;First, let us consider levels of spatial organization inside mechanisms. Different mechanisms exist at different spatial scales. ‘Higher’ level mechanisms involve larger systems and components. ‘Lower’ level mechanisms involve smaller systems and components. We normally explain by describing mechanisms at several levels. For example, we explain the cognitive capacity that a mouse exhibits when navigating a maze to find a reward by appeal to mechanisms inside and around the mouse at multiple spatial scales. The top-level mechanism is the entire mouse engaged in a spatial navigation task. Within this mechanism is a mechanism that involves part of the mouse’s brain, the hippocampus, storing a map of locations and orientations inside the maze. Within this mechanism, inside the mouse’s hippocampus, is a smaller mechanism storing information by changing weights in the synapses between pyramidal cells. Within this mechanism is a mechanism that produces long-term changes at a synapse by modifying the synapse’s N-methyl-D-aspartate (NMDA) receptors. Those NMDA receptors undergo change because of yet smaller mechanisms governing their functioning. A mechanistic explanation of the mouse’s cognitive capacity involves appeal to multiple mechanisms at multiple spatial scales and showing how they work in concert to produce the cognitive capacity &lt;span class=&#34;citation&#34; data-cites=&#34;Craver07&#34;&gt;(Craver 2007)&lt;/span&gt;. The relationship between higher and lower levels goes beyond the mere mereological whole–part relation. Higher level mechanisms are not just part of, but also are &lt;em&gt;realized&lt;/em&gt; by lower level mechanisms. Lower level mechanisms form the component parts of higher level mechanisms. Cognitive capacities are explained by showing how mechanisms at different spatial scales, integrated by the mechanistic realization relation, produce the cognitive capacity in question.&lt;/p&gt;
&lt;p&gt;Descriptions of mechanisms at different levels of spatial scale have a degree of autonomy from each other. A description of a mechanism at one spatial level may be silent about how that mechanism’s component parts work, or about the larger system in which the mechanism is embedded. One might, for example, describe how cells in the mouse hippocampus store a map of locations while remaining silent about the lower level mechanism that produces synaptic change. One might also describe how cells in the hippocampus store a map of locations while remaining silent about how those cells are recruited by the entire mouse to solve the task.&lt;/p&gt;
&lt;p&gt;This partial autonomy between descriptions at different spatial levels is not full-blown independence. The autonomy arises because mechanistic realization allows for the (logical) possibility of multiple realization. It is possible for the component parts of a mechanism to be realized in multiple ways, within the constraints that the performance of the mechanism dictates. It is also possible for the mechanism to be embedded in multiple larger contexts, within the constraints that the context should support the operation of the mechanism. Description at a particular level of spatial scale places some constraints on higher or lower level descriptions, but leaves some degree of freedom. This degree of freedom allows different teams of inquirers to focus on discovering mechanisms at different spatial scales in a partially autonomous manner. It allows scientific inquiry in the psychological sciences to split into different fields: biochemistry, cellular biology, neurophysiology, cognitive neuroscience, and behavioral ecology.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;OppenheimPutnam58&#34;&gt;Oppenheim and Putnam (1958)&lt;/span&gt; claimed that scientific disciplines are structured into autonomous levels corresponding to the spatial scale of their subject matter. We are now in a position to see what is right about this idea. There is no necessity for scientific inquiry to be structured by levels of spatial scale. Nevertheless, structuring scientific inquiry in the psychological sciences by spatial scale is permissible. It is permissible because the ontologically layered structure generated by the mechanistic realization relation, and the partial autonomy between levels of spatial scale that this provides, allows scientific inquiry to proceed at different spatial scales along relatively separate tracks. The structuring of scientific disciplines by levels of spatial scale that Oppenheim and Putnam describe is a consequence of the ontological layering, and partial autonomy, generated by the mechanistic realization relation.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;Marr82&#34;&gt;Marr (1982)&lt;/span&gt; introduced a distinct kind of level of inquiry into the psychological sciences. Marr argued that the psychological and cognitive sciences should be divided into three levels of explanation.&lt;/p&gt;
&lt;p&gt;Marr’s first level is the ‘computational’ level. The aim of inquiry at the computational level is to describe &lt;em&gt;which&lt;/em&gt; task an organism solves in a particular circumstance and &lt;em&gt;why&lt;/em&gt; that task is important to the organism. A task should be understood as an extensional function: a pattern of input and output behavior. In order to discover which function an organism computes, we need to understand the ecological purpose of computing that function – why the organism computes this function and what computation of this function would allow the organism to achieve. Without a guess as to the ecological purpose of computing this function, there would be no way of picking out from the vast number of things that the organism does (its patterns of input–output behavior) which are relevant to cognition.&lt;/p&gt;
&lt;p&gt;Marr’s second level is the ‘algorithmic’ level. The aim of inquiry at the algorithmic level is to answer &lt;em&gt;how&lt;/em&gt; the organism solves its task. The answer should consist in an algorithm: a finite number of simple steps that take one from input to output. Many different algorithms compute the same extensional function. Therefore, even if we were to know which extensional function the organism computes, the algorithm would still be left open. In order to discover which algorithm an organism uses, researchers look for indirect clues about the information-processing strategies exploited by the organism, such as the organism’s reaction times and susceptibility to errors.&lt;/p&gt;
&lt;p&gt;Marr’s third level is the ‘implementation’ level. The aim of inquiry at the implementation level is to describe how steps in the algorithm map onto physical changes. Even if we were to know both the extensional function and the algorithm, that would still leave open how the algorithm is implemented in physical changes of the organism. The brain is a complex physical system. Without some guide as to which parts of the brain implement which parts of an algorithm, there would be no way to know how the brain enables the organism to solve its task. The implementation level identifies which physical parts are functionally significant: which parts are relevant, and in which ways, to the computation that the organism performs. In the case of an electronic PC, electrical changes inside the silicon chips are functionally significant; the color of the silicon chips or the noise the cooling fan makes are not. Researchers look for implementation level descriptions by using techniques such as magnetic resonance imaging, electroencephalograms, single-cell recording, and testing how performance is affected when physical resources are damaged (e.g., by stroke) or temporarily disabled (e.g., by drugs).&lt;/p&gt;
&lt;p&gt;Marr’s three levels are not the same as the levels of spatial organization in mechanisms described previously. The levels of spatial organization in mechanisms involve positing an ontological layering relation: psychological mechanisms are related smaller to larger by mechanistic realization; component parts are realized by increasingly smaller mechanisms. One consequence of this ontological layering is that scientific inquiry at different spatial scales can be structured into partially autonomous domains (from biochemistry to behavioral ecology). Structuring scientific inquiry into levels is a consequence, not the principal content, of the claim. In contrast, the principal content of Marr’s claim is that scientific inquiry, not ontology, should be structured. Marr divides work in the psychological sciences into three types of inquiry: computational, algorithmic, and implementational. There is no assumption that this division is accompanied by a division in the ontology. Marr’s claim is simply that the psychological sciences should pursue three types of question if they are to explain psychological capacities adequately.&lt;/p&gt;
&lt;p&gt;Computational, algorithmic, and implementational questions concern a single system at a single level of spatial scale. They also concern a single capacity: which extensional function that capacity instantiates, which algorithm computes that function, and how that algorithm is physically implemented. In contrast, each level of the mechanistic scale concerns a different physical system: the entire organism, the brain, brain regions, neural circuits, individual neurons, and subcellular mechanisms. Each level of scale concerns a different capacity: the capacity of the whole organism to navigate a maze, the capacity of the hippocampus to store spatial information, the capacity of pyramidal synapses to undergo long-term potentiation (LTP), and so on. At each level of spatial scale, and for each corresponding capacity and system, one can ask Marr’s questions: Which function does the physical system compute and why? Which algorithm does it use to compute this function? How is that algorithm implemented by physical changes in the system? Marr’s questions cut across those concerning mechanisms at different levels of spatial scale.&lt;/p&gt;
&lt;p&gt;Marr claimed a degree of autonomy, but not full independence, between his levels. The autonomy that exists between Marr’s levels derives from two properties of computation. First, the same extensional function can be computed by different algorithms. Second, the same algorithm can be implemented in different ways. The first property means that proposing a particular function at the computational level does not restrict algorithmic-level inquiry to a particular algorithm. The second property means that proposing a particular algorithm at the algorithmic level does not restrict implementation-level inquiry to a particular physical implementation. Similar degrees of freedom do not hold in reverse. If one were to propose a particular implementation – a mapping from physical activity in the system to steps of some algorithm – then the algorithm that the system employs would be thereby fixed. Similarly, if one were to propose that the system uses a particular algorithm, then the extensional function that the system computes would be thereby fixed. The autonomy between Marr’s levels is downwards only: lower levels are partially autonomous from upper levels, but not vice versa.&lt;/p&gt;
&lt;p&gt;Downwards autonomy in Marr’s scheme, like the autonomy between levels of inquiry about mechanisms at different spatial scales, is only present as a logical possibility. The degree of autonomy is likely to be attenuated in practice. Downwards autonomy for Marr derives from the two logical properties of computation described earlier. However, the psychological sciences are constrained by more than what is logically possible. Their concern is what is reasonable to think given all we know about the brain and agent. The numbers of permissible algorithms and implementations are likely to be significantly less than those that are logically possible when constraints are added about the resources that the brain can employ, the time the system can take to solve its task, assumptions made in other areas of the psychological sciences are taken into account.&lt;/p&gt;
&lt;p&gt;The autonomy between Marr’s levels of inquiry is different from that between levels of inquiry concerning mechanisms at different spatial scales. The autonomy between Marr’s levels of inquiry derives from two properties of computation: that many algorithms compute the same function and that there are many ways to implement the same algorithm. The autonomy between levels of inquiry concerning mechanisms at different spatial scales derives from two properties of the mechanistic realization relation: that it is possible for different mechanisms to produce the same causal power, and that the same mechanism could be embedded in different contexts. The latter two properties give rise to an in principle upward and downward autonomy between levels of spatial scale. Positing a mechanism at a particular level of spatial scale does not fix how its smaller component parts work, nor does it fix how that mechanism is embedded in a larger system. Autonomy between levels of inquiry concerning mechanisms at different spatial scales is bi-directional. This is formally distinct from the downwards-only autonomy associated with Marr’s levels of computational explanation.&lt;/p&gt;
&lt;h1 id=&#34;individuating&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Individuating&lt;/h1&gt;
&lt;p&gt;Disagreements within philosophy of the psychological sciences are sometimes not about the causal flow involved in cognition, but how to individuate that causal flow. Two philosophical camps may agree on the basic causal relations, but disagree about which of the elements in the causal structure are cognitive, representational, perceptual, sensory, doxastic, gustatory, olfactory, and so on. These disagreements may give outsiders the appearance of being merely verbal. But this is rarely the case. The competing sides agree on the meanings of their words. What they disagree about is how cognitive capacities, processes, and states should be individuated.&lt;/p&gt;
&lt;p&gt;In this section, I look at two examples of this kind of dispute. The first is the disagreement about how to individuate the senses. The second is the disagreement about whether human mental states and processes extend outside our brains and bodies.&lt;/p&gt;
&lt;p&gt;The senses are different ways of perceiving, such as seeing, hearing, touching, tasting, and smelling. What makes two senses different? How many senses are there? Under which conditions would an organism have a new sense? The psychological sciences provide data that appear to challenge folk ideas about the senses. Novel sense modalities seem to exist in non-human animals, including magnetic senses, electric senses, infrared senses, and echolocation. Humans seem to have senses for pressure, temperature, pain, balance, and their internal organs in addition to their traditional five senses. Neural processing of sensory information in humans is multimodal; visual areas in the brain are not exclusively visual and integrate information from sound and other stimuli. Blindsight patients appear to have vision without associated visual phenomenology or consciously accessible beliefs. Tactile-visual sensory substitution (TVSS)-equipped patients appear to see via touch. Based on this information, should we revise the folk view that humans have five senses? If so, how? In order to answer this question, we need a way to individuate the senses. Let us look at four contenders.&lt;/p&gt;
&lt;p&gt;The first is representation-based. Suppose that each sense has an object or property that is exclusively detected and represented by that sense – its ‘proper sensible’. The proper sensibles of hearing, tasting, smelling, and seeing are sound, flavor, odor, and color respectively. According to the representation-based view, the representations of these proper sensibles – representations that are not generated in any other way – individuate the senses. A sense is individuated by the characteristic representations that the sense produces. A challenge for the view is that it lands us with a new problem: How do we individuate the sensory representations? It is not clear that this is substantially easier than the original problem of how to individuate the senses.&lt;/p&gt;
&lt;p&gt;The second approach is experience-based. Hearing, tasting, smelling, seeing, and touch are each associated not only with distinct representations, but also with distinct subjective phenomenal experiences. The phenomenal experiences tend to be similar within sensory modalities and different between sensory modalities. According to the experience-based view, it is because sensory experiences are phenomenally similar to and different from each other that we have distinct senses. A sense is individuated by the types of phenomenal experience to which it gives rise. A challenge for the view is to say what are the relevant similarities and differences in phenomenal experience. Experiences within a sensory modality are not all alike and those between sensory modalities are not all dissimilar. Which phenomenal similarities and differences matter for individuating the senses, and why are they important?&lt;/p&gt;
&lt;p&gt;The third approach is stimulus-based. Different senses involve responding to proximal stimuli of different physical kinds. Seeing involves reacting to electromagnetic waves between 380 nm and 750 nm. Hearing involves reacting to air pressure waves in the ear canal. Smelling involves reacting to airborne chemicals in the nose. According to the stimulus-based view, the reason why the senses are distinct is because they involve responses to different physical types of proximal stimulus. A sense is individuated by type of proximal stimulus to which the organism reacts. A challenge for the view is that the same proximal stimulus could be associated with different senses. For example, the same pressure wave in the air may be processed by an organism for hearing and for echolocation.&lt;/p&gt;
&lt;p&gt;The final approach is organ-based. Different senses tend to be associated with different sense organs. Seeing involves the eyes, hearing involves the ears, smelling involves the nose. Each sense organ contains physiologically distinct receptor cells. According to the organ-based view, the reason why the senses are distinct is because they employ distinct sense organs. A sense is individuated by its associated sense organ. A challenge for the view is that the same sense organ (e.g., the ear) could be used for two different senses (e.g., hearing and echolocation).&lt;/p&gt;
&lt;p&gt;The four proposals prescribe different revisions to folk assumptions about the senses in light of scientific data. The task facing philosophers is to determine which, if any, of these views is correct. &lt;span class=&#34;citation&#34; data-cites=&#34;Nudds04&#34;&gt;Nudds (2004)&lt;/span&gt; argues that we should not endorse any of them. His claim is that individuation of the senses is context-dependent. No single account individuates the senses across all contexts. Different criteria apply in different contexts depending on our interests. &lt;span class=&#34;citation&#34; data-cites=&#34;Macpherson11&#34;&gt;Macpherson (2011)&lt;/span&gt; argues that the senses should be individuated context-independently. She claims that all of the criteria above matter. All contribute jointly to individuating the senses. The four proposals can be used as a multidimensional metric on which any possible sense can be judged. The clustering of an organism’s cognitive capacities across multiple dimensions, rather than on a single dimension, determines how its senses should be individuated.&lt;/p&gt;
&lt;p&gt;Let us turn to our second example of a dispute about individuation. The hypothesis of extended cognition (HEC) asserts that human mental life sometimes extends outside the brain and takes place partly inside objects in the environment, such as notebooks or iPhones &lt;span class=&#34;citation&#34; data-cites=&#34;ClarkChalmers98&#34;&gt;(Clark and Chalmers 1998)&lt;/span&gt;. Disagreements about whether HEC is true have taken the form of a disagreement about the individuation conditions of human mental states and processes.&lt;/p&gt;
&lt;p&gt;The best way to understand HEC is to start with the weaker claim known as &lt;em&gt;distributed cognition&lt;/em&gt; &lt;span class=&#34;citation&#34; data-cites=&#34;Hutchins95&#34;&gt;(Hutchins 1995)&lt;/span&gt;. An advocate of distributed cognition claims that human cognitive capacities do not always arise solely in, and should not always be explained exclusively in terms of, neural mechanisms. The mechanisms behind human cognitive capacities sometimes include bodily and environmental processes. The brain is not the sole mechanism responsible for our cognitive abilities, but is only part – albeit a large part – of a wider story. The brain recruits environmental and bodily resources to solve problems. Recruiting these resources allows humans to do more than they could otherwise, and to work more quickly and reliably. Brains off-load work onto the body and environment. Sometimes the off-loading is under conscious control: for example, when we consciously decide to use a pen, paper, or our fingers to solve a mathematical problem. Sometimes the off-loading is not under conscious control: for example, when we use our eye gaze to store information &lt;span class=&#34;citation&#34; data-cites=&#34;BallardHayhoe97 GrayFu04&#34;&gt;(Ballard et al. 1997; Gray and Fu 2004)&lt;/span&gt;. Distributed cognition is the claim that &lt;em&gt;distributed&lt;/em&gt; information-processing strategies figure in the best explanation of, and causal story behind, some human cognitive accomplishments.&lt;/p&gt;
&lt;p&gt;HEC is a stronger claim than distributed cognition. According to HEC, not only do brains recruit environmental resources to solve problems, but those non-neural resources, when they have been recruited, also &lt;em&gt;have mental properties&lt;/em&gt;. Parts of the environment and the body, when employed in distributed cognition strategies, have just as much claim to mental or cognitive status as any neural process. Against this, the hypothesis of embedded cognition (HEMC) accepts the distributed-cognition claim about the exploitation of bodily and environmental resources, but rejects HEC’s assertion about the body and environment having mental properties &lt;span class=&#34;citation&#34; data-cites=&#34;Rupert04 Rupert13&#34;&gt;(Rupert 2004, 2013)&lt;/span&gt;. According to HEMC, non-neural resources, despite figuring in the explanation and causal story behind some cognitive accomplishments, do not have mental properties. Only neural processes have mental or cognitive properties.&lt;/p&gt;
&lt;p&gt;How is this about individuation? HEC is, in essence, a claim about the individuation of mental kinds. HEC claims that the causal flow in human cognition should be individuated in such a way that the neural and non-neural parts instantiate a single kind – a mental kind. This is not to say that there are no differences relevant to psychology between the neural and non-neural parts. HEC’s claim is merely that the neural and non-neural parts jointly satisfy a condition sufficient for them to instantiate a single mental kind. In contrast, HEMC’s claim is that the causal flow in human cognition should be individuated so that the neural and non-neural parts fall under different kinds – mental and non-mental respectively. This is not to say that there are no kinds of interest to psychology that both instantiate. Rather, it is to say that whatever kinds they instantiate, they jointly fail to meet the condition required for them to instantiate a single mental kind. HEC and HEMC disagree, it cases of distributed cognition, about how to individuate mental properties across the causal flow.&lt;/p&gt;
&lt;p&gt;Which view is right: HEC or HEMC? To answer this, we need to agree on the minimal condition, mentioned earlier, for a physical process to instantiate a mental kind. There are two main proposals on this score. The first is functionalist. On this view, a physical state or process is mental provided it has the right functional profile. I have argued elsewhere that the functionalist proposal decisively favors HEC &lt;span class=&#34;citation&#34; data-cites=&#34;Sprevak08b&#34;&gt;(Sprevak 2009)&lt;/span&gt;. If one does not accept HEC granted functionalism, one concedes the chauvinism about the mind that functionalism was designed to avoid. The second proposal is based on explanatory pay-off to cognitive science. On this view, a physical state or process is mental just in case it fits with best (current or future) cognitive science to treat it as such. I have argued elsewhere that considerations of explanatory value regarding cognitive science are toothless to decide between HEC and HEMC. Cognitive science could continue to be conducted with little or no loss either way &lt;span class=&#34;citation&#34; data-cites=&#34;Sprevak09&#34;&gt;(Sprevak 2010)&lt;/span&gt;. The dispute between HEC and HEMC is a case in point for which a question about individuation of mental states and processes cannot be answered by a straightforward appeal to scientific practice. Work in philosophy of the psychological and cognitive sciences needs to draw on a wide range of considerations to settle such questions about individuation.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;We have surveyed four types of task in philosophy of the psychological and cognitive sciences: How should we interpret our scientific theories? How should we precisify our theoretical concepts? How do our theories or methodologies fit together? How should our cognitive states, processes, and capacities be individuated? We have focused on only some of current work in philosophy of the psychological and cognitive sciences. Work we have not covered includes proposals for psychological mechanisms and architectures &lt;span class=&#34;citation&#34; data-cites=&#34;Grush04 ApperlyButterfill09&#34;&gt;(for example, Grush 2004; Apperly and Butterfill 2009)&lt;/span&gt;; analysis of scientific methodology in the psychological sciences &lt;span class=&#34;citation&#34; data-cites=&#34;Glymour01 Machery12&#34;&gt;(Glymour 2001; Machery 2013)&lt;/span&gt;; examination of key experimental results in the psychological sciences &lt;span class=&#34;citation&#34; data-cites=&#34;Block07 SheaBayne10&#34;&gt;(Block 2007; Shea and Bayne 2010)&lt;/span&gt;; and analysis of folk psychological concepts &lt;span class=&#34;citation&#34; data-cites=&#34;GrayGray07 KnobePrinz08&#34;&gt;(Gray, Gray, and Wegner 2007; Knobe and Prinz 2008)&lt;/span&gt;.&lt;/p&gt;
&lt;h1 id=&#34;acknowledgements&#34; class=&#34;unnumbered&#34;&gt;Acknowledgements&lt;/h1&gt;
&lt;p&gt;I would like to thank faculty and graduate students at the University of Pittsburgh, as well as David Danks, for helpful suggestions on this entry.&lt;/p&gt;
&lt;h1 id=&#34;bibliography&#34; class=&#34;unnumbered&#34;&gt;Bibliography&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-ApperlyButterfill09&#34;&gt;
&lt;p&gt;Apperly, I. A., and S. A. Butterfill. 2009. “Do Humans Have Two Systems to Track Belief and Belief-Like States?” &lt;em&gt;Psychological Review&lt;/em&gt; 116:953–70.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Baars97&#34;&gt;
&lt;p&gt;Baars, B. 1997. &lt;em&gt;In the Theater of Consciousness&lt;/em&gt;. Oxford: Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-BallardHayhoe97&#34;&gt;
&lt;p&gt;Ballard, D. H., M. M. Hayhoe, P. Pook, and R. Rao. 1997. “Deictic Codes for the Embodiment of Cognition.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 20:723–67.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Bechtel98&#34;&gt;
&lt;p&gt;Bechtel, W. 1998. “Representations and Cognitive Explanations: Assessing the Dynamicist’s Challenge in Cognitive Science.” &lt;em&gt;Cognitive Science&lt;/em&gt; 22:295–318.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Block95a&#34;&gt;
&lt;p&gt;Block, N. 1995. “On a Confusion About a Function of Consciousness.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 18:227–47.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Block07&#34;&gt;
&lt;p&gt;———. 2007. “Consciousness, Accessibility, and the Mesh Between Psychology and Neuroscience.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 30:481–548.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-BowersDavis12&#34;&gt;
&lt;p&gt;Bowers, J. S., and C. J. Davis. 2012. “Bayesian Just-so Stories in Psychology and Neuroscience.” &lt;em&gt;Psychological Bulletin&lt;/em&gt; 128:389–414.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ClarkChalmers98&#34;&gt;
&lt;p&gt;Clark, A., and D. J. Chalmers. 1998. “The Extended Mind.” &lt;em&gt;Analysis&lt;/em&gt; 58:7–19.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ClarkToribio94&#34;&gt;
&lt;p&gt;Clark, A., and J. Toribio. 1994. “Doing Without Representing?” &lt;em&gt;Synthese&lt;/em&gt; 101:401–31.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ColomboSeries12&#34;&gt;
&lt;p&gt;Colombo, M., and P. Seriès. 2012. “Bayes on the Brain—on Bayesion Modelling in Neuroscience.” &lt;em&gt;The British Journal for the Philosophy of Science&lt;/em&gt; 63:697–723.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Coltheart99&#34;&gt;
&lt;p&gt;Coltheart, M. 1999. “Modularity and Cognition.” &lt;em&gt;Trends in Cognitive Sciences&lt;/em&gt; 3:115–20.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Craver07&#34;&gt;
&lt;p&gt;Craver, C. F. 2007. &lt;em&gt;Explaining the Brain&lt;/em&gt;. Oxford: Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Danks09&#34;&gt;
&lt;p&gt;Danks, D. 2008. “Rational Analyses, Instrumentalism, and Implementations.” In &lt;em&gt;The Probabilistic Mind: Prospects for Rational Models of Cognition&lt;/em&gt;, edited by N. Chater and M. Oaksford, 59–75. Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Dehaene14&#34;&gt;
&lt;p&gt;Dehaene, S. 2014. &lt;em&gt;Consciousness and the Brain: Deciphering How the Brain Codes Our Thoughts&lt;/em&gt;. London: Penguin Books.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-DehaeneChangeux04&#34;&gt;
&lt;p&gt;Dehaene, S., and J.-P. Changeux. 2004. “Neural Mechanisms for Access to Consciousness.” In &lt;em&gt;The Cognitive Neurosciences, III&lt;/em&gt;, edited by M. Gazzaniga, 1145–57. Cambridge, MA: MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-DehaeneChangeux06&#34;&gt;
&lt;p&gt;Dehaene, S., J.-P. Changeux, L. Naccache, J. Sackur, and C. Sergent. 2006. “Conscious, Preconscious, and Subliminal Processing: A Testable Taxonomy.” &lt;em&gt;Trends in Cognitive Sciences&lt;/em&gt; 10:204–11.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Egan10&#34;&gt;
&lt;p&gt;Egan, F. 2010. “Computational Models: A Modest Role for Content.” &lt;em&gt;Studies in History and Philosophy of Science&lt;/em&gt; 41:253–59.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Eliasmith97&#34;&gt;
&lt;p&gt;Eliasmith, C. 1997. “Computation and Dynamical Models of Mind.” &lt;em&gt;Minds and Machines&lt;/em&gt; 7:531–41.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Eliasmith13&#34;&gt;
&lt;p&gt;———. 2013. &lt;em&gt;How to Build a Brain: A Neural Architecture for Biological Cognition&lt;/em&gt;. Oxford: Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ElsabbaghKarmiloff06&#34;&gt;
&lt;p&gt;Elsabbagh, M., and A. Karmiloff-Smith. 2006. “Modularity of Mind and Language.” In &lt;em&gt;The Encyclopedia of Language and Linguistics&lt;/em&gt;, edited by K. Brown, 8:218–24. Oxford: Elsevier.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ErnstBanks02&#34;&gt;
&lt;p&gt;Ernst, M. O., and M. S. Banks. 2002. “Humans Integrate Visual and Haptic Information in a Statistically Optimal Fashion.” &lt;em&gt;Nature&lt;/em&gt; 415:429–33.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Feldman12&#34;&gt;
&lt;p&gt;Feldman, J. 2012. “Symbolic Representation of Probabilistic Worlds.” &lt;em&gt;Cognition&lt;/em&gt; 123:61–83.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-FirestoneScholl15&#34;&gt;
&lt;p&gt;Firestone, C., and B. Scholl. 2015. “Cognition Does Not Affect Perception: Evaluating the Evidence for ‘Top-down’ Effects.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; ?????&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Fodor83&#34;&gt;
&lt;p&gt;Fodor, J. A. 1983. &lt;em&gt;The Modularity of Mind&lt;/em&gt;. MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Fodor00&#34;&gt;
&lt;p&gt;———. 2000. &lt;em&gt;The Mind Doesn’t Work That Way&lt;/em&gt;. Cambridge, MA: MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Gelder95&#34;&gt;
&lt;p&gt;Gelder, T. van. 1995. “What Might Cognition Be, If Not Computation?” &lt;em&gt;The Journal of Philosophy&lt;/em&gt; 91:345–81.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Glymour01&#34;&gt;
&lt;p&gt;Glymour, C. 2001. &lt;em&gt;The Mind’s Arrows: Bayes Nets and Graphical Causal Models in Psychology&lt;/em&gt;. Cambridge, MA: MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-GrayGray07&#34;&gt;
&lt;p&gt;Gray, H. M., K. Gray, and D. M. Wegner. 2007. “Dimensions of Mind Perception.” &lt;em&gt;Science&lt;/em&gt; 315:619.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-GrayFu04&#34;&gt;
&lt;p&gt;Gray, W. D., and W.t.- Fu. 2004. “Soft Constraints in Interactive Behavior.” &lt;em&gt;Cognitive Science&lt;/em&gt; 28:359–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-GriffithsChater12&#34;&gt;
&lt;p&gt;Griffiths, T. L., N. Chater, D. Norris, and A. Pouget. 2012. “How the Bayesians Got Their Beliefs (and What Those Beliefs Actually Are): Comment on Bowers and Davis (2012).” &lt;em&gt;Psychological Bulletin&lt;/em&gt; 138:415–22.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Grush04&#34;&gt;
&lt;p&gt;Grush, R. 2004. “The Emulator Theory of Representation: Motor Control, Imagery, and Perception.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 27:377–442.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Hutchins95&#34;&gt;
&lt;p&gt;Hutchins, E. 1995. &lt;em&gt;Cognition in the Wild&lt;/em&gt;. Cambridge, MA: MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-JonesLove11&#34;&gt;
&lt;p&gt;Jones, M., and B. C. Love. 2011. “Bayesian Fundamentalism or Enlightenment? On the Explanatory Status and Theoretical Contributions of Bayesian Models of Cognition.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 34:169–231.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Keijzer98&#34;&gt;
&lt;p&gt;Keijzer, F. A. 1998. “Doing Without Representations Which Specify What to Do.” &lt;em&gt;Philosophical Psychology&lt;/em&gt; 11:269–302.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-KnillPouget04&#34;&gt;
&lt;p&gt;Knill, D. C., and A. Pouget. 2004. “The Bayesian Brain: The Role of Uncertainty in Neural Coding and Computation.” &lt;em&gt;Trends in Neurosciences&lt;/em&gt; 27:712–19.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-KnobePrinz08&#34;&gt;
&lt;p&gt;Knobe, J., and J. Prinz. 2008. “Intuitions About Consciousness: Experimental Studies.” &lt;em&gt;Phenomenology and Cognitive Science&lt;/em&gt; 7:67–85.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-KochTsuchiya06&#34;&gt;
&lt;p&gt;Koch, C., and N. Tsuchiya. 2006. “Attention and Consciousness: Two Distinct Brain Processes.” &lt;em&gt;Trends in Cognitive Sciences&lt;/em&gt; 11:16–22.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-LaureysBoly09&#34;&gt;
&lt;p&gt;Laureys, S., M. Boly, G. Moonen, and P. Maquet. 2009. “Coma.” &lt;em&gt;Encyclopedia of Neuroscience&lt;/em&gt; 2:1133–42.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-LehkySejnowski88&#34;&gt;
&lt;p&gt;Lehky, S. R., and T. J. Sejnowski. 1988. “Network Model of Shape-from-Shading: Neural Function Arises from Both Receptive and Projective Fields.” &lt;em&gt;Nature&lt;/em&gt; 333:452–54.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Machery12&#34;&gt;
&lt;p&gt;Machery, E. 2013. “In Defense of Reverse Inference.” &lt;em&gt;The British Journal for the Philosophy of Science&lt;/em&gt; 65:251–67.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Machery15a&#34;&gt;
&lt;p&gt;———. 2015. “Cognitive Penetrability: A No-Progress Report.” In &lt;em&gt;The Cognitive Penetrability of Perception&lt;/em&gt;, edited by J. Zeimbekis and A. Raftopoulos, 59–74. Oxford: Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Machery15&#34;&gt;
&lt;p&gt;———. n.d. “Philosophy Within Its Proper Bounds.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Macpherson11&#34;&gt;
&lt;p&gt;Macpherson, F. 2011. “Individuating the Senses.” In &lt;em&gt;The Senses: Classic and Comtemporary Philosophical Perspectives&lt;/em&gt;, edited by F. Macpherson, 3–43. Oxford: Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Marr82&#34;&gt;
&lt;p&gt;Marr, D. 1982. &lt;em&gt;Vision&lt;/em&gt;. San Francisco, CA: W. H. Freeman.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-McDowell10&#34;&gt;
&lt;p&gt;McDowell, J. 2010. “Tyler Burge on Disjunctivism.” &lt;em&gt;Philosophical Explorations&lt;/em&gt; 13:243–55.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Nudds04&#34;&gt;
&lt;p&gt;Nudds, M. 2004. “The Significance of the Senses.” &lt;em&gt;Proceedings of the Aristotelian Society&lt;/em&gt; 104:31–51.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-OppenheimPutnam58&#34;&gt;
&lt;p&gt;Oppenheim, P., and H. Putnam. 1958. “Unity of Science as a Working Hypothesis.” In &lt;em&gt;Concepts, Theories, and the Mind–Body Problem&lt;/em&gt;, edited by H. Feigl, M. Scriven, and G. Maxwell, II:3–36. Minnesota Studies in the Philosophy of Science. Minneapolis, MN: University of Minnesota Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Piccinini08&#34;&gt;
&lt;p&gt;Piccinini, G. 2008. “Computation Without Representation.” &lt;em&gt;Philosophical Studies&lt;/em&gt; 137:205–41.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-PougetBeck13&#34;&gt;
&lt;p&gt;Pouget, A., J. M. Beck, W. J. Ma, and P. E. Latham. 2013. “Probabilistic Brains: Knows and Unknowns.” &lt;em&gt;Nature Neuroscience&lt;/em&gt; 16:1170–8.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Prinz06a&#34;&gt;
&lt;p&gt;Prinz, J. 2006. “Is the Mind Really Modular?” In &lt;em&gt;Contemporary Debates in Cognitive Science&lt;/em&gt;, edited by R. Stainton, 22–36. Oxford: Blackwell.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Putnam75e&#34;&gt;
&lt;p&gt;Putnam, H. 1975. &lt;em&gt;Mathematics, Matter and Method, Philosophical Papers, Volume 1&lt;/em&gt;. Cambridge: Cambridge University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Qian97&#34;&gt;
&lt;p&gt;Qian, N. 1997. “Binocular Disparity and the Perception of Depth.” &lt;em&gt;Neuron&lt;/em&gt; 18:359–68.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Rescorla15a&#34;&gt;
&lt;p&gt;Rescorla, M. 2015. “Bayesian Perceptual Psychology.” In &lt;em&gt;The Oxford Handbook of Philosophy of Perception&lt;/em&gt;, edited by M. Matthen, 694–716. Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Rescorla15&#34;&gt;
&lt;p&gt;———. 2016. “Bayesian Sensorimotor Psychology.” &lt;em&gt;Mind and Language&lt;/em&gt; 31:3–36.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Rupert04&#34;&gt;
&lt;p&gt;Rupert, R. D. 2004. “Challenges to the Hypothesis of Extended Cognition.” &lt;em&gt;The Journal of Philosophy&lt;/em&gt; 101:389–428.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Rupert13&#34;&gt;
&lt;p&gt;———. 2013. “Memory, Natural Kinds, and Cognitive Extension; Or, Martians Don’t Remember, and Cognitive Science Is Not About Cognition.” &lt;em&gt;Review of Philosophy and Psychology&lt;/em&gt; 4:25–47.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Samuels05&#34;&gt;
&lt;p&gt;Samuels, R. 2005. “The Complexity of Cognition: Tractability Arguments for Massive Modularity.” In &lt;em&gt;The Innate Mind: Vol. I, Structure and Contents&lt;/em&gt;, edited by P. Carruthers, S. Laurence, and S. P. Stich, 107–21. Oxford: Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Schoner08&#34;&gt;
&lt;p&gt;Schöner, G. 2008. “Dynamical Systems Approaches to Cognition.” In &lt;em&gt;Cambridge Handbook of Computational Psychology&lt;/em&gt;, edited by R. Sun, 101–26. Cambridge: Cambridge University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Sellars63&#34;&gt;
&lt;p&gt;Sellars, W. 1963. &lt;em&gt;Science, Perception and Reality&lt;/em&gt;. London. Routledge &amp;amp; Kegan Paul.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-SheaBayne10&#34;&gt;
&lt;p&gt;Shea, N., and T. Bayne. 2010. “The Vegetative State and the Science of Consciousness.” &lt;em&gt;The British Journal for the Philosophy of Science&lt;/em&gt; 61:459–84.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Sprevak08b&#34;&gt;
&lt;p&gt;Sprevak, M. 2009. “Extended Cognition and Functionalism.” &lt;em&gt;The Journal of Philosophy&lt;/em&gt; 106:503–27.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Sprevak09&#34;&gt;
&lt;p&gt;———. 2010. “Inference to the Hypothesis of Extended Cognition.” &lt;em&gt;Studies in History and Philosophy of Science&lt;/em&gt; 41:353–62.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Sprevak13&#34;&gt;
&lt;p&gt;———. 2013. “Fictionalism About Neural Representations.” &lt;em&gt;The Monist&lt;/em&gt; 96:539–60.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ThelenSchoner01&#34;&gt;
&lt;p&gt;Thelen, E., G. Schöner, C. Scheier, and L. B. Smith. 2001. “The Dynamics of Embodiment: A Field Theory of Infant Perseverative Reaching.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 24:1–86.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Turing&#39;s model of the mind</title>
      <link>http://marksprevak.com/publications/turing-s-model-of-the-mind-2017/</link>
      <pubDate>Mon, 13 Nov 2017 00:00:00 UTC</pubDate>
      <author>Mark Sprevak</author>
      <guid>http://marksprevak.com/publications/turing-s-model-of-the-mind-2017/</guid>
      <description>&lt;div&gt;
&lt;p&gt;Alan Turing contributed to a revolutionary idea: that mental activity is computation. Turing’s work helped lay the foundation for what is now known as cognitive science. Today, computation is an essential element for explaining how the mind works. In this chapter, I return to Turing’s early attempts to understanding the mind using computation and examine the role that Turing played in the early days of cognitive science.&lt;/p&gt;
&lt;h1 id=&#34;engineering-versus-psychology&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Engineering versus psychology&lt;/h1&gt;
&lt;p&gt;Turing is famous as a founding figure in artificial intelligence (AI) but his contribution to cognitive science is less well known. The aim of AI is to create an intelligent machine. Turing was one of the first people to carry out research in AI, working on machine intelligence as early as 1941 and, as Chapters 29 and 30 explain, he was responsible for, or anticipated, many of the ideas that were later to shape AI.&lt;/p&gt;
&lt;p&gt;Unlike AI, cognitive science does not aim to create an intelligent machine. It aims instead to understand the mechanisms that are peculiar to human intelligence. On the face of it, human intelligence is miraculous. How do we reason, understand language, remember past events, come up with a joke? It is hard to know how even to begin to explain these phenomena. Yet, like a magic trick that looks like a miracle to the audience, but which is explained by revealing the pulleys and levers behind the stage, so human intelligence could be explained if we knew the mechanisms that lie behind its production.&lt;/p&gt;
&lt;p&gt;A first step in this direction is to examine a piece of machinery that is usually hidden from view: the human brain. A challenge is the astonishing complexity of the human brain: it is one of the most complex objects in the universe, containing 100 billion neurons and a web of around 100 trillion connections. Trying to uncover the mechanisms of human intelligence by looking at the brain is impossible unless one has an idea of what to look for. Which properties of the brain are relevant to intelligence? One of the guiding and most fruitful assumptions in cognitive science is that the relevant property of the brain for producing intelligence is the computation that the brain performs.&lt;/p&gt;
&lt;p&gt;Cognitive science and AI are related: both concern human intelligence and both use computation. It is important to see, however, that their two projects are distinct. AI aims to create an intelligent machine that may or may not use the same mechanisms for intelligence as humans. Cognitive science aims to uncover the mechanisms peculiar to human intelligence. These two projects could, in principle, be pursued independently.&lt;/p&gt;
&lt;p&gt;Consider that if one were to create an artificial hovering machine it is not also necessary to solve the problem of how birds and insects hover. Today, more than 100 years after the first helicopter flight, how birds and insects hover is still not understood. Similarly, if one were to create an intelligent machine, one need not also know how humans produce intelligent behaviour. One might be sanguine about AI but pessimistic about cognitive science. One might think that engineering an intelligent machine is possible, but that the mechanisms of human intelligence are too messy and complex to understand. Alternatively, one might think that human intelligence can be explained, but that the engineering challenge of building an intelligent machine is outside our reach.&lt;/p&gt;
&lt;p&gt;In Turing’s day, optimism reigned for AI and the cognitive-science project took a back seat. Fortunes have now reversed. Few AI researchers aim to create the kind of general, human-like, intelligence that Turing envisioned. In contrast, cognitive science is regarded as a highly promising research project.&lt;/p&gt;
&lt;p&gt;Cognitive science and AI divide roughly along the lines of psychology versus engineering. Cognitive science aims to understand human intelligence; AI aims to engineer an intelligent machine. Turing’s contribution to the AI project is well known. What did Turing contribute to the cognitive-science project? Did he intend his computational models as psychological models as well as engineering blueprints?&lt;/p&gt;
&lt;h1 id=&#34;building-brainy-computers&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Building brainy computers&lt;/h1&gt;
&lt;p&gt;Turing rarely discussed psychology directly in his work. There is good evidence, however, that he saw computational models as shedding some light on human psychology.&lt;/p&gt;
&lt;p&gt;Turing was fascinated by the idea of building a &lt;em&gt;brain-like&lt;/em&gt; computer. His B-machines were inspired by his attempt to reproduce the action of the brain, as described in Chapter 29. Turing talked about his desire to build a machine to ‘imitate a brain’, to ‘mimic the behaviour of the human computer’, ‘to take a man … and to try to replace … parts of him by machinery … [with] some sort of “electronic brain”’, he claimed that ‘it is not altogether unreasonable to describe digital computers as brains’, and that ‘our main problem [is] how to programme a machine to imitate a brain.’&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Evidently, Turing thought that the tasks in AI engineering and psychology were somehow related. What did he think was the nature of their relationship? We should distinguish three different things that he might have meant:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Psychology sets standards for engineering success&lt;/em&gt;. Human behaviour is where our grasp on intelligence starts. Intelligent behaviour is, in the first instance, known to us as something that humans do. One thing that psychology provides is a specification of human behaviour. This description can then be used in the service of AI by providing a benchmark for the behaviour of intelligent machines. Whether a machine counts as intelligent depends on how well it meets an appropriately idealized version of standards set by psychology. Psychology is relevant to AI because it specifies what is meant by intelligent behaviour. This connection seems peculiar to intelligent behaviour. One could, for instance, understand perfectly well what hovering is without knowledge of birds or insects.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Psychology as a source of inspiration for engineering&lt;/em&gt;. We know that the human brain produces intelligent behaviour. One way to tackle the AI engineering problem is to examine the human brain and take inspiration from it. Note, however, that the ‘being inspired by’ relation is a relatively weak one. Someone may be inspired by a design without understanding much about how that design works. Someone impressed by how birds hover may add wings to an artificial hovering machine. But even if this were successful, it would not mean the engineer knows how a bird’s wings enable it to hover. Indeed, the way in which wings allows a bird to hover may not be the same as the way in which wings allow the engineer’s artificial machine to hover – flapping may be an essential part in one case but not the other. An AI engineer might take inspiration from brains without knowing how brains work&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Psychology should explain human intelligence in terms of the brain’s computational mechanisms&lt;/em&gt;. Unlike the two previous claims, this involves the idea that the mechanisms of human thought are computational. The first two claims are compatible with this idea but they do not entail it. Indeed, the first two claims are silent about what psychology should, or should not, do. They describe a one-sided interaction between psychology and engineering with the influence going all from psychology to engineering: psychology sets the standards of engineering success or psychology inspires engineering. This claim is different: it recommends that psychology should adopt the computational framework of the AI engineering project. The way in which we explain human intelligence, and not just attempts to simulate it artificially, should be computational.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Did Turing make the third (cognitive-science) claim? Turing certainly gets close to it and, as we shall see in the final section, his work has been used by others in the service of that claim.&lt;/p&gt;
&lt;p&gt;In the quotations above, Turing describes one possible strategy for AI: imitating the brain’s mechanisms in an electronic computer. In order for such this strategy to work, one has to know which are the relevant properties of the brain to imitate. Turing says that the important features are not that ‘the brain has the consistency of cold porridge’ or any specific electrical property of nerves.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; Rather, among the relevant features he cites the brain’s ability ‘to transmit information from place to place, and also to store it’:&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;brains very nearly fall into [the class of electronic computers], and there seems to be every reason to believe that they could have been made to fall genuinely into it without any change in their essential properties.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;On the face of it, this still has the flavour of a one-way interaction between AI engineering and psychology: which features of the brain &lt;em&gt;are relevant to&lt;/em&gt; AI engineering? But unlike the claims above, this one-way interaction presupposes a specific view about how the human brain works: that the brain produces intelligent behaviour via (perhaps among other things) its computational properties. This is very close to the cognitive-science claim. Turing appears to be committed to something like to the third claim above (the cognitive-science claim) via his engineering strategy.&lt;/p&gt;
&lt;p&gt;However, there is a problem with this reading of Turing. The key terms that Turing uses – ‘reproduce’, ‘imitate’, ‘mimic’, ‘simulate’ – have a special meaning in his work that is incompatible with the reading above. Those terms can be read as either ‘strong’ or ‘weak’. On a strong reading, ‘reproducing’, ‘imitating’, ‘mimicking’, or ‘simulating’ means &lt;em&gt;copying that system’s inner workings&lt;/em&gt; – copying the equivalent of the levers and pulleys by which the system achieves its behaviour. On a weak reading, ‘reproducing’, ‘imitating’, ‘mimicking’, or ‘simulating’ means &lt;em&gt;copying the system’s overall input-output behaviour&lt;/em&gt; – reproducing the behaviour of the system, but not necessarily the system’s method for doing so. The strong reading requires that an ‘imitation’ of a brain work in the same way as a real brain. The weak reading requires only that an imitation of a brain produce the same overall behaviour.&lt;/p&gt;
&lt;p&gt;We assumed the strong reading above. In Turing’s work, however, he tended to use the weak reading. Use of the weak reading is important to prove the computational results for which Turing is most famous (see Chapter 7). If the weak reading is the correct one, then the interpretation of Turing’s words above is not correct. Imitating a brain does not require knowing how brains work – only knowing the overall behaviour brains produce. This falls squarely under the first relationship between psychology and engineering: psychology sets standards for engineering success. Imitating a brain – in the (weak) sense of reproducing the brain’s overall behaviour – requires only that psychology specify the overall behaviour that AI should aim to reproduce. It does not require that psychology also adopt a computational theory about human psychology.&lt;/p&gt;
&lt;p&gt;Is there evidence that Turing favoured the strong over the weak reading? Turing wrote to the psychologist W. Ross Ashby that:&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In working on the ACE I am more interested in the possibility of producing models of the action of the brain than in practical applications to computing. … Thus, although the brain may in fact operate by changing its neuron circuits by the growth of axons and dendrites, we could nevertheless make a model, within the ACE, in which this possibility was allowed for, but in which the actual construction of the ACE did not alter, but only the remembered data, describing the mode of behaviour applicable at any time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This appears to show that Turing endorsed something like the cognitive-science claim: he believed that the computational properties of the brain are the relevant ones to capture in a simulation of a brain. Unfortunately, it is also dogged by the same problem we saw previously. ‘Producing a computational model of the action of the brain’ can be given either a strong or a weak reading. It could mean &lt;em&gt;producing a model that works in the same way as the brain&lt;/em&gt; (strong), or &lt;em&gt;producing a model that produces the same overall behaviour&lt;/em&gt; (weak). Both kinds of computational model interested Turing and Ashby. Only the former would tell in favour of the cognitive-science claim.&lt;/p&gt;
&lt;p&gt;Tantalisingly, Turing finished his 1951 BBC radio broadcast with:&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The whole thinking process is still rather mysterious to us, but I believe that the attempt to make a thinking machine will help us greatly in finding out how we think ourselves.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The difficulty is that ‘helping’, like ‘being inspired by’, is not specific enough to pin the cognitive- science claim to Turing. There are many ways that the attempt to make a thinking machine might help psychology: the machines created might do useful number crunching, building the machines may teach us high-level principles that apply to all intelligent systems, building the machines may motivate psychology to give a specification of human competences. None of these would commit Turing to the cognitive-science claim.&lt;/p&gt;
&lt;p&gt;Turing’s writings are consistent with the cognitive-science claim but they do not offer unambiguous support for it. In the next section, we will see a clearer, but different, type of influence that Turing has had on modern-day cognitive science. In the final section, we will see how his computational models have been taken up and used by others as psychological models.&lt;/p&gt;
&lt;h1 id=&#34;from-mathematics-to-psychology&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; From mathematics to psychology&lt;/h1&gt;
&lt;p&gt;Turing proposed several computational models that have influenced psychology. Here I focus on only one: the Turing machine. Ostensibly, the purpose of the Turing machine was to settle questions about mathematics – in particular, the question of which mathematical statements can and cannot be proven by mechanical means. We will see that Turing’s model is good for another purpose: it can be used as a model of human thought. This spin-off benefit has been extremely influential.&lt;/p&gt;
&lt;p&gt;A Turing machine is an abstract mathematical model of a human clerk. Imagine that a human being works by himself, mechanically, without undue intelligence or insight, to solve a mathematical problem. Turing asks us to compare this ‘to a machine that is only capable of a finite number of conditions.’&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; That machine, a Turing machine, has a finite number of internal states in its head and an unlimited length of blank tape divided into squares on which it can write and erase symbols. At any moment, the machine can read a symbol from its tape, write a symbol, erase a symbol, move to neighbouring square, or change its internal state. Its behaviour is fixed by a finite set of instructions (a transition table) that specifies what it should do next in every circumstance (read, write, erase symbol, change state, move head).&lt;/p&gt;
&lt;p&gt;Turing wanted to know which mathematical tasks could and could not be performed by a human clerk. Could a human clerk, given enough time and paper, calculate any number? Could a clerk tell us which mathematical statements are provable and which are not? Turing’s brilliance was to see that these seemingly impossible questions about human clerks can be answered if we reformulate them to be about Turing machines. If one could show that the problems that can be solved by Turing machines are the same as the problems that can be solved by a human clerk, then any result about which problems a Turing machine can solve would carry over to a result about which problems a human clerk can solve. Turing machines can be proxies for human clerks in our reasoning.&lt;/p&gt;
&lt;p&gt;It is easy to prove that the problems that a Turing machine can solve can also be solved by a human clerk. The clerk could simply step through the operations of the Turing machine by hand. Proving the converse claim – that the problems that a human clerk can solve could also be solved by a Turing machine – is harder. Turing offered a powerful informal argument for this second claim. Significantly, his argument depended on &lt;em&gt;psychological reasoning&lt;/em&gt; about the human clerk:&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The behaviour of the [clerk] at any moment is determined by the symbols which he is observing, and his ‘state of mind’ at that moment. We may suppose that there is a bound &lt;em&gt;B&lt;/em&gt; to the number of symbols or squares that the [clerk] can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite. The reasons for this are of the same character as those which restrict the number of symbols. If we admitted an infinity of states of mind, some of them will be ‘arbitrarily close’ and will be confused.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Turing’s strategy is to argue that the clerk cannot bring any more internal resources to bear in solving a problem than a Turing machine. Therefore, the class of problems that a clerk can solve is no larger than those of a Turing machine. In conjunction with the first claim above, this establishes the crucial claim that the problems that can be solved by Turing machines are exactly the same as those that can be solved by a human clerk.&lt;/p&gt;
&lt;p&gt;Turing’s argument is an exercise in weak modelling. His aim is to show that Turing machines and human clerks solve the same class of problems: they are capable of producing the same pattern of behaviour. His argument requires him to show that a Turing machine can copy the behaviour of the clerk and vice versa (weak modelling). It does not require him to show that the Turing machine reproduces that clerk’s internal psychological mechanisms for generating his behaviour (strong modelling). Strong modelling goes beyond what was required by Turing’s work on the &lt;em&gt;Entscheidungsproblem&lt;/em&gt; but it is what we need for cognitive science.&lt;/p&gt;
&lt;p&gt;One might conclude that there is nothing of further interest here for psychology. Yet, Turing’s argument should give one pause for thought. Turing’s argument requires that human clerks and Turing machines share at least &lt;em&gt;some&lt;/em&gt; similarity in their inner working. They must have similar kinds of internal resources; otherwise, Turing’s argument that the clerk’s resources do not differ in kind from those of a Turing machine would not work. This suggests that a Turing machine is more than just a weak model of a human clerk. A Turing machine also provides a description, albeit rather high level and abstract, of the clerk’s inner workings. In addition to capturing the clerk’s outward behaviour, Turing machines also give some information about the levers and pulleys behind the clerk’s behaviour.&lt;/p&gt;
&lt;h1 id=&#34;your-brains-inner-turing-machine&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Your brain’s inner Turing machine&lt;/h1&gt;
&lt;p&gt;Does a Turing machine provide a psychologically realistic model of the mechanisms of the human mind? Turing never seriously pursued this question in print, but it has been taken up by others. The philosopher Hilary Putnam argued that a Turing machine is a good psychological model. Putnam claimed that a Turing machine is not only a good model of a clerk’s mind while he is solving a mathematical task, it is a good model of other aspects of mental life.&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; According to Putnam, all human mental states (beliefs, desires, thoughts, imaginings, feelings, pains) should be understood as states of a Turing machine and its tape. All human mental processes (reasoning, association, remembering) should be understood as computational steps of some Turing machine. Psychological explanation should be explanation in terms of the nature and operation of an inner Turing machine. Only when one sees the brain as implementing a Turing machine can one correctly see the contribution that the brain makes to our mental life. Putnam’s proposal falls neatly under the cognitive-science claim identified above.&lt;/p&gt;
&lt;p&gt;Putnam and others quickly became dissatisfied with the Turing machine as a psychological model.&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; It is not hard to see why. The human brain lacks any clear ‘tape’ or ‘head’, human mental states are not atomic states that change in a step-wise way over time, human psychology is not serial: it involves parallel mechanisms that cooperate or compete with each other. If the mind is a computer, it is unlikely to be a Turing machine.&lt;/p&gt;
&lt;p&gt;The past fifty years have seen an explosion in the number and variety of computational models in psychology. State-of-the-art computational models of the mind look and work nothing like Turing machines. Among the most popular models are hierarchical recurrent connectionist networks that make probabilistic predictions and implement Bayesian inference.&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt; The mechanisms of these computational models bear little resemblance to Turing machines. Yet, one might wonder, is there still something essentially right, albeit high level and abstract, about Turing machines as psychological models? And even if Turing machines do not model all aspects of our mental life, perhaps they provide a good model of some parts of our mental life.&lt;/p&gt;
&lt;p&gt;Turing machines provide a good psychological model of at least one part our mental life: deliberate, serial, rule-governed inference – the capacity at work inside the head of the human clerk when he is solving his mathematical problems. In some situations humans deliberately arrange their mental processes to work in a rule-governed, serial way. They attempt to follow rules without using initiative, insight, or ingenuity, and without being disturbed by their other mental processes. In these situations, it seems that our psychological mechanisms approximate those of a Turing machine: our mental states appear step-wise, as atomic entities, and change in a serial fashion.&lt;/p&gt;
&lt;p&gt;At a finer level of detail – and moving closer to the workings the brain – there is of course a more complex story to tell. Yet, as a ‘high-level’ computational model, the Turing machine is not bad as a piece of psychology. In certain situations, and at a high, abstract, level of description, our brains implement a Turing machine.&lt;/p&gt;
&lt;p&gt;Modern computational models of the mind are massively parallel, exhibit complex and delicate dynamics, and operate with probability distributions rather than discrete symbols. How can one square them with Turing machines? One way to integrate the two models is to use the idea that a Turing machine runs as a &lt;em&gt;virtual machine&lt;/em&gt; on these models.&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; The idea is that a Turing machine arises, as an emergent phenomenon, out of some lower-level computational processes.&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt; This idea should be familiar from electronic PCs: a high-level computation (in C# or Java) can arise out of lower-level computation (in assembler or microcode). High-level and low-level computational descriptions are both important when we explain how an electronic PC works. Similarly, we should expect that high-level and low-level descriptions will be important to explain how the human brain produces intelligence.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;Turing has had a huge influence on cognitive science but, as we have seen, tracing the precise course of his influence is complex. In this chapter, we looked at two possible sources: Turing’s discussion of how AI should be proceed, and the way in which Turing’s computational models have influenced others. On the first score, we saw that Turing rarely talked about how AI should influence psychology, and that it is not easy to attribute to Turing the modern-day claim that human psychology should be computational. On the second, a clearer picture emerges. Turing’s 1936 paper on the &lt;em&gt;Entscheidungsproblem&lt;/em&gt; suggests that Turing machines are more than weak models of human psychology. Putnam and others took up this idea and proposed that Turing machines are strong models of human psychology. This idea remains influential today. Despite the wide range of exotic computational models in cognitive science, Turing machines still appear to capture a fundamental, albeit high-level truth about the workings of the human mind.&lt;/p&gt;
&lt;h1 id=&#34;bibliography&#34; class=&#34;unnumbered&#34;&gt;Bibliography&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Clark13&#34;&gt;
&lt;p&gt;Clark, A. 2013. “Whatever Next? Predictive Brains, Situated Agents, and the Future of Cognitive Science.” &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt; 36:181–253.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Dennett91&#34;&gt;
&lt;p&gt;Dennett, D. C. 1991. &lt;em&gt;Consciousness Explained&lt;/em&gt;. Boston, MA: Little, Brown &amp;amp; Company.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Feldman12&#34;&gt;
&lt;p&gt;Feldman, J. 2012. “Symbolic Representation of Probabilistic Worlds.” &lt;em&gt;Cognition&lt;/em&gt; 123:61–83.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Putnam75g&#34;&gt;
&lt;p&gt;Putnam, H. 1975a. “Minds and Machines.” In &lt;em&gt;Mind, Language and Reality, Philosophical Papers, Volume 2&lt;/em&gt;, 362–87. Cambridge: Cambridge University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Putnam75a&#34;&gt;
&lt;p&gt;———. 1975b. “Philosophy and Our Mental Life.” In &lt;em&gt;Mind, Language and Reality, Philosophical Papers, Vol. 2&lt;/em&gt;, 291–303. Cambridge: Cambridge University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Putnam75f&#34;&gt;
&lt;p&gt;———. 1975c. “The Mental Life of Some Machines.” In &lt;em&gt;Mind, Language and Reality, Philosophical Papers, Volume 2&lt;/em&gt;, 408–28. Cambridge: Cambridge University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Turing04c&#34;&gt;
&lt;p&gt;Turing, A. M. 2004a. “Can Automatic Calculating Machines Be Said to Think?” In &lt;em&gt;The Essential Turing&lt;/em&gt;, edited by B. J. Copeland, 487–506. Oxford: Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Turing04b&#34;&gt;
&lt;p&gt;———. 2004b. “Can Digital Computers Think?” In &lt;em&gt;The Essential Turing&lt;/em&gt;, edited by B. J. Copeland, 476–86. Oxford: Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Turing04a&#34;&gt;
&lt;p&gt;———. 2004c. “Computing Machinery and Intelligence.” In &lt;em&gt;The Essential Turing&lt;/em&gt;, edited by B. J. Copeland, 441–64. Oxford: Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Turing04&#34;&gt;
&lt;p&gt;———. 2004d. “Intelligent Machinery.” In &lt;em&gt;The Essential Turing&lt;/em&gt;, edited by B. J. Copeland, 395–432. Oxford: Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Turing04f&#34;&gt;
&lt;p&gt;———. 2004e. “On Computable Numbers, with an Application to the &lt;em&gt;Entscheidungsproblem&lt;/em&gt;.” In &lt;em&gt;The Essential Turing&lt;/em&gt;, edited by B. J. Copeland, 58–90. Oxford: Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ZylberbergDehaene11&#34;&gt;
&lt;p&gt;Zylberberg, A., S. Dehaene, P. R. Roelfsema, and M. Sigman. 2011. “The Human Turing Machine: A Neural Framework for Mental Programs.” &lt;em&gt;Trends in Cognitive Sciences&lt;/em&gt; 15:293–300.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;section class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;Turing04b&#34;&gt;Turing (2004b)&lt;/span&gt;, p. 484; &lt;span class=&#34;citation&#34; data-cites=&#34;Turing04a&#34;&gt;Turing (2004c)&lt;/span&gt;, p. 445; &lt;span class=&#34;citation&#34; data-cites=&#34;Turing04&#34;&gt;Turing (2004d)&lt;/span&gt;, p. 420; &lt;span class=&#34;citation&#34; data-cites=&#34;Turing04b&#34;&gt;Turing (2004b)&lt;/span&gt;, p. 482; &lt;span class=&#34;citation&#34; data-cites=&#34;Turing04&#34;&gt;Turing (2004d)&lt;/span&gt;, p. 472.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;Turing04c&#34;&gt;Turing (2004a)&lt;/span&gt;, p. 495; &lt;span class=&#34;citation&#34; data-cites=&#34;Turing04&#34;&gt;Turing (2004d)&lt;/span&gt;, p. 420.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;Turing04&#34;&gt;Turing (2004d)&lt;/span&gt;, p. 420.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Letter from Turing to W. Ross Ashby, no date (Woodger papers (catalogue reference M11/99); a digital facsimile is in the Turing Archive for the History of Computing [&lt;a href=&#34;http://www.alanturing.net/turing_ashby&#34;&gt;www.alanturing.net/turing_ashby&lt;/a&gt;].&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;Turing04b&#34;&gt;Turing (2004b)&lt;/span&gt;, p. 486.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;Turing04f&#34;&gt;Turing (2004e)&lt;/span&gt;, p. 59.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;Turing04f&#34;&gt;Turing (2004e)&lt;/span&gt;, pp. 75–76.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;Putnam75g&#34;&gt;Putnam (1975a)&lt;/span&gt;; &lt;span class=&#34;citation&#34; data-cites=&#34;Putnam75f&#34;&gt;Putnam (1975c)&lt;/span&gt;.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;Putnam75a&#34;&gt;Putnam (1975b)&lt;/span&gt;.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;Clark13&#34;&gt;Clark (2013)&lt;/span&gt;.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;See &lt;span class=&#34;citation&#34; data-cites=&#34;Dennett91&#34;&gt;Dennett (1991)&lt;/span&gt;.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;&lt;span class=&#34;citation&#34; data-cites=&#34;ZylberbergDehaene11&#34;&gt;Zylberberg et al. (2011)&lt;/span&gt;; &lt;span class=&#34;citation&#34; data-cites=&#34;Feldman12&#34;&gt;Feldman (2012)&lt;/span&gt;.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Eliminativism about consciousness</title>
      <link>http://marksprevak.com/publications/eliminativism-about-consciousness/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 UTC</pubDate>
      <author>Mark Sprevak</author>
      <guid>http://marksprevak.com/publications/eliminativism-about-consciousness/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Two kinds of information processing in cognition</title>
      <link>http://marksprevak.com/publications/two-kinds-of-information-processing-in-cognition/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 UTC</pubDate>
      <author>Mark Sprevak</author>
      <guid>http://marksprevak.com/publications/two-kinds-of-information-processing-in-cognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Zuse&#39;s Thesis, Gandy&#39;s Thesis, and Penrose&#39;s Thesis</title>
      <link>http://marksprevak.com/publications/zuse-s-thesis-gandy-s-thesis-and-penrose-s-thesis/</link>
      <pubDate>Mon, 22 May 2017 00:00:00 UTC</pubDate>
      <author>Mark Sprevak</author>
      <guid>http://marksprevak.com/publications/zuse-s-thesis-gandy-s-thesis-and-penrose-s-thesis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Distributed Cognition from Victorian Culture to Modernism</title>
      <link>http://marksprevak.com/publications/distributed-cognition-from-victorian-culture-to-modernism/</link>
      <pubDate>Wed, 28 Sep 2016 00:00:00 UTC</pubDate>
      <author>Mark Sprevak</author>
      <guid>http://marksprevak.com/publications/distributed-cognition-from-victorian-culture-to-modernism/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Distributed Cognition in Classical Antiquity</title>
      <link>http://marksprevak.com/publications/distributed-cognition-in-classical-antiquity/</link>
      <pubDate>Wed, 28 Sep 2016 00:00:00 UTC</pubDate>
      <author>Mark Sprevak</author>
      <guid>http://marksprevak.com/publications/distributed-cognition-in-classical-antiquity/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The computable universe</title>
      <link>http://marksprevak.com/publications/the-computable-universe-2017/</link>
      <pubDate>Tue, 23 Aug 2016 00:00:00 UTC</pubDate>
      <author>Mark Sprevak</author>
      <guid>http://marksprevak.com/publications/the-computable-universe-2017/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
