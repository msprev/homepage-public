<!DOCTYPE html>
<html lang="en-GB">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Mark Sprevak">
  <meta name="dcterms.date" content="2018-03-29">
  <title>Who lives and who drives?</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<!-- -------------------------------------- -->
<section id="title-slide" data-background-image="img/highway.jpg">
<!-- -------------------------------------- -->
  <h1 class="title">Who lives and who drives?</h1>
  <p class="author">Mark Sprevak</p>
  <p class="date">29 March 2018</p>
</section>

<section id="self-driving-cars" class="slide level1" data-background-image="img/self-driving.jpg">
<h1>Self-driving cars</h1>
<div>
<ul>
<li class="fragment">Self-driving cars are good</li>
<li class="fragment">But we need to do it right</li>
</ul>
</div>
<aside class="notes">
<p>[TT] I’m going to start by saying that self-driving cars are a good thing. We should support the development of self-driving cars.</p>
<p>[TT] But we need to do it right. We need to go into this technology with our eyes wide open. We should be aware of the decisions we are making <em>and</em> the compromises. Otherwise, those decisions and compromises will happen anyway, it is just that they will be done without us being aware of it and without adequate scrutiny.</p>
</aside>
</section>
<section class="slide level1">

<h2 id="ethical-issues">Ethical issues?</h2>
<div>
<ul>
<li class="fragment">Ownership issues</li>
<li class="fragment">Privacy issues</li>
<li class="fragment">Economic issues</li>
<li class="fragment"><span style="color:red">What happens in a crash?</span></li>
</ul>
</div>
<aside class="notes">
<p>What are the decision we need to watch out for? For example, what are the important ethical issues are raised by self-driving cars? There are quite a few.</p>
<ul>
<li><p><strong>Ownership</strong>: Does it matter to a self-driving car who owns it? Should a publicly owned bus or fire engine behave differently to a private car? Should you expect your car “owes” you an allegiance and protect you in preference to publicly owned vehicles?</p></li>
<li><p><strong>Privacy</strong>: Who has access to your car’s data? Who can feed into your car’s decision-making? What’s the role of 3rd parties in this – because there are going to be 3rd parties? Could an advertiser influence your car’s route selection, e.g., to steer your car past their business?</p></li>
<li><p><strong>Economic</strong>: What work should we offer people who currently drive cars &amp; trucks for a living? What will happen to the car insurance industry? Will self-driving cars reinforce or help to break down existing social inequalities?</p></li>
</ul>
<p>[TT] In addition to all of these, there are <em>special</em> ethical issues which arise for self-driving cars in the case of a crash. And that is what I want to focus on.</p>
</aside>
</section>
<section id="crashes-are-going-to-happen" class="slide level1" data-background-image="img/crash.jpg">
<h1>Crashes are going to happen</h1>
<aside class="notes">
<p>Crashes are inevitable. However good the car is, the environment is inherently unpredictable and there will be crashes.</p>
</aside>
</section>
<section class="slide level1">

<h3 id="not-all-crashes-raise-interesting-ethical-questions">Not all crashes raise interesting ethical questions</h3>
<div>
<ul>
<li class="fragment">For many, there is an ‘obvious’ correct answer</li>
<li class="fragment"><span style="color:red">A few</span> raise hard ethical questions</li>
</ul>
</div>
<aside class="notes">
<p>[TT] For probably the majority of crashes, there is an ‘obvious’ correct thing that the car should have done. The car may have failed to do this – for example, its sensors may not have worked or it may have processed information in the wrong way – but it is obvious to us what the car <em>should</em> have done.</p>
<p>The first fatality of a self-driving car in 2016 was like this. A tractor trailer was crossing the road perpendicularly, and the car tried to drive under trailer – it just shouldn’t have done this.</p>
<p>[TT] But, not all crashes are like that. A few types of crash raise hard ethical questions. And this is because we don’t know what the ‘right’ behaviour would be for a car in those circumstances. These cases involve <em>moral dilemmas</em>. We are unsure what is the <em>right</em> thing for the car to do.</p>
</aside>
</section>
<section id="jackknifed-lorry" class="slide level1" data-background-image="img/lorry.jpg">
<h1>Jackknifed lorry</h1>
<aside class="notes">
<p>There are lots of complexity involved in planning for a crash: you have to weigh lots of risks, bring in background knowledge, and respond to the particularity of the exact circumstances. But just to get us into the ball-park of discussing ethical dilemmas, here is a massively simplified case.</p>
<p>Imagine there are 5 passengers in your car as it approaches a jackknifed lorry at high speed. All 5 passengers will be killed unless the car swerves to avoid the lorry, but swerving would kill a pedestrian. What should the car do? Serve and kill the pedestrian <em>or</em> kill its 5 passengers?</p>
<p>Alternatively, suppose you’re the only person in the car and that 5 pedestrians would be killed if the car swerves to protect you. What should the car do? Should it kill 5 people to save its 1 passenger? What if the 5 people are children, what if they are criminals?</p>
<p>We should recognise that these are intrinsically hard questions to answer. The difficulty isn’t so much a technical or engineering difficulty. It is an ethical one: we <em>do not know</em> what the car <em>should</em> do in these circumstances.</p>
<p>Unfortunately the car <em>will</em> do something. So we <em>have</em> to make a decision about it. Not answering the ethical questions will still be answering them as some course of action is going to occur. We can’t just ‘duck’ the ethical questions in these cases. So how should we answer them?</p>
</aside>
</section>
<section class="slide level1">

<h2 id="what-ethics-do-we-program-into-the-car">What ‘ethics’ do we program into the car?</h2>
<aside class="notes">
<p>You could rephrase this question as: What ‘ethics’ – what system of rules – we should put into the car to guide it these tricky cases.</p>
</aside>
</section>
<section id="section" class="slide level1" data-background-image="img/philosophers.jpg">
<h1></h1>
<h2 id="normative-ethics">Normative ethics</h2>
<div class="fragment">
<p>Tells you what you <em>should</em> do</p>
<aside class="notes">
<p>The good news is that philosophers seem to have our backs. For thousands of years, they have developed <em>normative</em> ethical theories.</p>
<p>[TT] These are theories that tell us what the morally ‘right’ course of action is in any circumstance. Perfect.</p>
</aside>
</div>
</section>
<section id="section-1" class="slide level1" data-background-image="img/philosophers.jpg">
<h1></h1>
<h2 id="normative-ethics-1">Normative ethics</h2>
<ol type="1">
<li>Utilitarianism</li>
<li>Kantian ethics</li>
<li>Virtue ethics</li>
<li>Contractualism</li>
</ol>
<aside class="notes">
<p>If you open a philosophy textbook, you will find there are 4 main types of normative ethical theory.</p>
<ul>
<li><p><strong>Utilitarian</strong>: A good action is one that maximises well-being and minimises suffering for the maximum number of people</p></li>
<li><p><strong>Kantian</strong>: Roughly, ‘Do onto others as you would have them do unto you’. Act always on moral principles that you would also will to be universal laws.</p></li>
<li><p><strong>Virtue ethics</strong>: A good action comes from a virtuous agent – an agent with virtues like compassion, courage, generosity, and honesty.</p></li>
<li><p><strong>Contractualism</strong>: Good actions are those that rational agents would agree, on purely self-interested grounds, if they were to face the problem of having to live together.</p></li>
</ul>
<p>Millions of pages have been written to flesh out each one of these ethical theories.</p>
</aside>
</section>
<section class="slide level1">

<h2 id="problems">Problems</h2>
<div>
<ul>
<li class="fragment">Not implementable in code</li>
<li class="fragment">Don’t provide a single answer</li>
</ul>
</div>
<aside class="notes">
<p>Without going into details, one can flag problems with putting these theories in self-driving cars.</p>
<p>[TT] None of these theories implementable in computer code. The theories are simply not designed to used as algorithms that could be run on a computer. What the theories do is they tell us, from a God’s eye point of view – from the point of view of a human-like agent who knows everything and can compute all consequences – what is the right course of action. But we aren’t in that God-like position, and nor are cars.</p>
<p>[TT] We have got multiple ethical theories. And the theories often give <em>different</em> answers about what to do in our tricky moral dilemmas. A utilitarian may say that your car <em>should</em> swerve to kill a pedestrian to save 5 passengers – since that maximises total well-being and minimises total suffering. However, a Kantian may say that the car <em>shouldn’t</em> swerve, no matter how many lives it would save, because that involve treating the pedestrian’s life as merely means to an end – not allowed on a Kantian view.</p>
<!-- Different ethical theories give different answers not because the theories are 'bad' but because we face *genuine* ethical dilemmas. There are good moral reasons to go either way in these hard cases. The different theories capture the motivation for different responses to a dilemma. -->
<p>The car needs to do just one thing. We need to know what it should do. Having multiple contradictory answers is going to help.</p>
</aside>
</section>
<section id="section-2" class="slide level1" data-background-image="img/making-decisions.jpg">
<h1></h1>
<h3 id="what-about-empirical-ethics">What about ‘empirical’ ethics?</h3>
<div class="fragment">
<p>Program the ‘ethics’ that humans have</p>
<aside class="notes">
<p>Normative ethical theories tell us what we <em>should</em> do even if humans in fact don’t do it.</p>
<p>But maybe trying to achieve moral perfection is the wrong thing for a self-driving car. Instead, why not look at what humans <em>actually</em> do when they are confronted with an ethical dilemma.</p>
<p>In theory, one could train a car to make the same decisions as a human driver would. If a car is just as moral as a human in an ethical dilemma, that should be enough to put it on the road.</p>
<p>[TT] In other words, instead of aiming at a normative ethical theory, we could try to program the car with the ‘ethics’ that humans actually have in their heads.</p>
</aside>
</div>
</section>
<section class="slide level1">

<h2 id="problems-1">Problems</h2>
<div>
<ul>
<li class="fragment">People appear to be inconsistent</li>
</ul>
</div>
<div class="fragment">
<p><img data-src="img/dual-process.jpg" title="opt title" style="width:40%; border:0px;" /></p>
<aside class="notes">
<p>[TT] People are inconsistent. Even a single person is inconsistent. A now-famous study published in 2016 in <em>Science</em> magazine shows this. People in the study said they want two conflicting things from a self-driving car. They said that they want a self-driving car to minimise harm to everyone. But they also said they want their self-driving car to prioritise them over any other road users in an accident.</p>
<p>Now, they can’t have it both ways. Either the car prioritises its passengers or it aims to minimise harm to everyone. The two options correspond to two different actions in the jackknifed lorry case – either swerve to save the car’s passenger or don’t swerve to save the 5 pedestrians. So, asking people what one should do in these ethical dilemmas doesn’t seem to put us further forward – people give contradictory answers.</p>
<p>[TT] Incidentally, Joshua Greene’s lab in Harvard has done very interesting work to show that people don’t have a single set of ethical rules in their heads. Instead, we have several competing ethical systems – a bit like warring tribes in our heads. One of these ethical systems wins out over others in any given circumstance and gets to drive our action. But that winner gets selected in a very idiosyncratic way: it depends on the particular individual, time pressure, how well they’ve slept, their cultural background, their current emotional state, and so on. Distilling a consistent single set of ethical rules from actual human behaviour is just not going to happen.</p>
</aside>
</div>
</section>
<section id="section-3" class="slide level1" data-background-image="img/road.jpg">
<h1></h1>
<h2 id="what-next">What next?</h2>
<aside class="notes">
<p>Where do we go from here? Normative ethical theories don’t solve the problem, reading the answer off actual human behaviour doesn’t solve the problem. Where should we look to find out what a car should do in these rare but important cases?</p>
<p>I’m going to hand over to Tyron. I believe that Tyron is going to suggest that key to solving it is to return to the simplifying assumption I made at the start. If one moves away from extremely simple cases like the jackknifed lorry, and starts looking at the complexities of a real-like situation, perhaps ethical solutions might be unlocked.</p>
</aside>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: false,
        // Display a presentation progress bar
        progress: false,
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'none', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
